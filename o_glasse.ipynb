{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "5ikCmYN4Ba4U",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "2e5928da-be10-4917-f60f-075a530eb211",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCopyright (c) 2017-2022 Yuhei Otsubo\\nReleased under the MIT license\\nhttp://opensource.org/licenses/mit-license.php\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Copyright (c) 2017-2022 Yuhei Otsubo\n",
    "Released under the MIT license\n",
    "http://opensource.org/licenses/mit-license.php\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aB_DTssWWS39",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Experimental mode = 0(Ex.1) ~ 4(Ex.5)\n",
    "args_ex_mode = 0\n",
    "#Learning method: 0(UTL),1(DTL),2(UFT),3(DFT),4(2LF)\n",
    "args_lm = 5\n",
    "\n",
    "cuda_device = \"cuda:1\"\n",
    "\n",
    "# for saving trained model\n",
    "args_output_model = '20230104_01compiler'\n",
    "args_input_model = ''#'20201005_01compiler_old'\n",
    "\n",
    "\n",
    "#block size\n",
    "op_num = 235\n",
    "block_size = op_num\n",
    "args_s_limit = 20000\n",
    "# K-split cross-validation\n",
    "args_k = 4\n",
    "# num of epochs\n",
    "args_epoch = 200\n",
    "args_batch_size = 64\n",
    "args_arch_num = [9,51,3,3+9,3,3+3][args_ex_mode]\n",
    "args_max_norm = 2e-1#0.25 #勾配クリッピング\n",
    "args_gn_rate = 1.0 #Layer-wised Gradient Normalization\n",
    "# Automatic download of traind model when it is created (for Google Colab)\n",
    "F_download = False#True\n",
    "# Visualizing the output of the Weak Learner by T-SNE\n",
    "F_arch_check = True\n",
    "# Use multiple GPUs\n",
    "F_multi_GPU = False#True\n",
    "\n",
    "\n",
    "args_lr = 0.025 #Initial learning rate\n",
    "args_momentum = 0.9 #Momentum\n",
    "args_weight_decay = 1e-4 #Weight decay factor\n",
    "args_smoothing = 0.1 #Label smoothing\n",
    "args_dar = 0.15\n",
    "args_dar2 = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex.1 DFT+\n"
     ]
    }
   ],
   "source": [
    "ex_mode = ['Ex.1','Ex.2','Ex.3','Ex.4','Ex.5','Ex.6'][args_ex_mode]\n",
    "learning_mode = ['UTL','DTL','UFT','DFT','2LF','DFT+','Simple'][args_lm]\n",
    "\n",
    "if args_ex_mode >= 4:\n",
    "    path = './dataset/compiler_old/'\n",
    "else:\n",
    "    path = './dataset/20200424compiler/'\n",
    "\n",
    "f_pretrain_subnet = [True,False,True,False,False,False,False][args_lm]\n",
    "f_pretrain_mainnet = [False,True,False,True,False,True,False][args_lm]\n",
    "\n",
    "print(ex_mode,learning_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_8eecSi3xs3L",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "d1b0b93a-d68e-4312-ee5a-ba8206aa40c0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization done\n"
     ]
    }
   ],
   "source": [
    "#for Google Colab\n",
    "F_colab = False\n",
    "if F_colab and 'Init_Flag' not in locals().keys():\n",
    "  print('Initialization')\n",
    "  #mount google drive\n",
    "  from google.colab import drive\n",
    "  from google.colab import files\n",
    "  drive.mount('/content/drive')\n",
    "  #download dataset\n",
    "  !cp 'drive/My Drive/00_dataset/compiler/20200424compiler.zip' './dataset.zip'\n",
    "  #!cp 'drive/My Drive/00_dataset/compiler/20200608compiler_old.zip' './dataset.zip'\n",
    "  !unzip -o -qq dataset.zip\n",
    "\n",
    "  Init_Flag = True\n",
    "else:\n",
    "  print('Initialization done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ebwe7UvgW-IP",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from timm.loss import LabelSmoothingCrossEntropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "#import secrets\n",
    "from matplotlib.colors import Normalize\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "SEED = 2020\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ERPuJ3AR4IOI",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "args_output = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PpGY-k_YW6KM",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fild_all_files(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        yield root\n",
    "        for file in files:\n",
    "            path = os.path.join(root, file)\n",
    "            if os.path.islink(path):\n",
    "                continue\n",
    "            yield path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "sPFcq-ozxKvq",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def entropy(data):\n",
    "    result = []\n",
    "    s = len(data)\n",
    "    for x in range(256):\n",
    "        n = 0\n",
    "        for i in data:\n",
    "            if i == x:\n",
    "                n+=1\n",
    "        p_i = float(n)/s\n",
    "        if p_i != 0:\n",
    "                result.append(p_i * np.log2(p_i))\n",
    "    r = 0.0\n",
    "    for i in result:\n",
    "        if i == i:\n",
    "            #not NaN\n",
    "            r += i\n",
    "    return (-r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aOZDjwEKWdo7",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "43f10d57-445c-4fcc-d56a-64bfef6ea07e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/20200424compiler/\n"
     ]
    }
   ],
   "source": [
    "print(path)\n",
    "#Get file list\n",
    "\n",
    "files_file = [f for f in fild_all_files(path) if os.path.isfile(os.path.join(f))]\n",
    "files_file.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rO2oXCoHXQ2j",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "39020864-085e-42fe-806f-b2c48ab51b0c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 01_1_VC2003_32bit_none\n",
      "1 01_2_VC2017_32bit_none\n",
      "2 02_1_VC2003_32bit_max\n",
      "3 02_2_VC2017_32bit_max\n",
      "4 03_1_gcc6.3.0_x86_none\n",
      "5 03_2_x86-O0-gcc7.5.0\n",
      "6 04_1_gcc6.3.0_x86_O3\n",
      "7 04_2_x86-O3-gcc7.5.0\n",
      "8 05_1_clang5.0.2_32_none\n",
      "9 05_2_x86-O0-Clang10.0.0\n",
      "10 06_1_clang5.0.2_32_O3\n",
      "11 06_2_x86-O3-Clang10.0.0\n",
      "12 07_intel_32_none\n",
      "13 08_intel_32bit_max\n",
      "14 11_VC2017_64bit_none\n",
      "15 12_VC2017_64bit_max\n",
      "16 13_1_gcc6.3.0_64bit_none\n",
      "17 13_2_x86_64-O0-gcc7.5.0\n",
      "18 14_1_gcc6.3.0_64bit_max\n",
      "19 14_2_x86_64-O3-gcc7.5.0\n",
      "20 15_1_clang5.0.2_64bit_none\n",
      "21 15_2_x86_64-O0-Clang10.0.0\n",
      "22 16_1_clang5.0.2_64bit_max\n",
      "23 16_2_x86_64-O3-Clang10.0.0\n",
      "24 17_intel_64_none\n",
      "25 18_intel_64bit_max\n",
      "26 21_arm-O0-gcc\n",
      "27 22_arm-O3-gcc\n",
      "28 23_arm-O0-Clang\n",
      "29 24_arm-O3-Clang\n",
      "30 31_arm64-O0-gcc\n",
      "31 32_arm64-O3-gcc\n",
      "32 33_arm64-O0-Clang\n",
      "33 34_arm64-O3-Clang\n",
      "34 41_mips-O0-gcc\n",
      "35 42_mips-O3-gcc\n",
      "36 43_mips-O0-Clang\n",
      "37 44_mips-O3-Clang\n",
      "38 51_mips64-O0-gcc\n",
      "39 52_mips64-O3-gcc\n",
      "40 53_mips64-O0-Clang\n",
      "41 54_mips64-O3-Clang\n",
      "42 61_powerpc-O0-gcc\n",
      "43 62_powerpc-O3-gcc\n",
      "44 63_powerpc-O0-Clang\n",
      "45 64_powerpc-O3-Clang\n",
      "46 71_powerpc64-O0-gcc\n",
      "47 72_powerpc64-O3-gcc\n",
      "48 73_powerpc64-O0-Clang\n",
      "49 74_powerpc64-O3-Clang\n",
      "50 80_document\n"
     ]
    }
   ],
   "source": [
    "file_types = {}\n",
    "file_types_ = []\n",
    "num_of_file_types = {}\n",
    "num_of_types = 0\n",
    "for f in files_file:\n",
    "  #File type classification by directory name\n",
    "  file_type = f.replace(path,\"\").replace(os.path.basename(f),\"\").split(\"/\",1)[0]\n",
    "  if file_type in file_types:\n",
    "    num_of_file_types[file_type] += 1\n",
    "  else:\n",
    "    file_types[file_type]=num_of_types\n",
    "    file_types_.append(file_type)\n",
    "    num_of_file_types[file_type] = 1\n",
    "    print(num_of_types,file_type)\n",
    "    num_of_types+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#label置き換え用\n",
    "if args_ex_mode == 0:\n",
    "    new_label_m = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50]\n",
    "    new_label_s = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8]\n",
    "elif args_ex_mode == 1:\n",
    "    new_label_m = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8]\n",
    "    new_label_s = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50]\n",
    "elif args_ex_mode == 2:   \n",
    "    new_label_m = [0,1,0,1,2,3,2,3,4,5,4,5,6,6,7,7,8,9,8,9,10,11,10,11,12,12,13,13,14,14,15,15,16,16,17,17,18,18,19,19,20,20,21,21,22,22,23,23,24,24,25]\n",
    "    new_label_s = [0,0,1,1,0,0,1,1,0,0,1,1,0,1,0,1,0,0,1,1,0,0,1,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,2]\n",
    "elif args_ex_mode == 3:\n",
    "    new_label_m = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50]\n",
    "    new_label_s0 = [0,0,1,1,0,0,1,1,0,0,1,1,0,1,0,1,0,0,1,1,0,0,1,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,2]\n",
    "    new_label_s = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6,6,6,7,7,7,7,8]    \n",
    "elif args_ex_mode == 4:\n",
    "    new_label_m = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50]\n",
    "    new_label_s = [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,2]\n",
    "elif args_ex_mode == 5:\n",
    "    new_label_m = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50]\n",
    "    new_label_s0 = [0,0,1,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,2]\n",
    "    new_label_s = [0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,2]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ZOwfZkYMxKwG",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for calc entropy\n",
    "num_of_byte = [[0.0 for x in range(256)] for y in range(num_of_types)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "I-M3mdLN5DU8",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def onehotlist(n):\n",
    "  return [[0 if i!=j else 1 for i in range(n)] for j in range(n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g6EOi8TgX4sV",
    "outputId": "c54df216-ab51-4d40-fbe2-472f01b690b9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make dataset\n",
      "1 /10\n",
      "2 /10\n",
      "3 /10\n",
      "4 /10\n",
      "5 /10\n",
      "6 /10\n",
      "7 /10\n",
      "8 /10\n",
      "9 /10\n",
      "10 /10\n"
     ]
    }
   ],
   "source": [
    "#Dataset preparation\n",
    "print('make dataset')\n",
    "BitArray = [[int(x) for x in format(y,'08b')] for y in range(256)]\n",
    "BitArray.append([-1 for x in range(8)])\n",
    "num_of_dataset = {}\n",
    "total_samples = 0\n",
    "master_dataset_X = np.empty((num_of_types*args_s_limit,block_size*8+args_arch_num),dtype=np.int8)\n",
    "master_dataset_Y = np.empty(num_of_types*args_s_limit,dtype=np.int8)\n",
    "# master_dataset_b = []\n",
    "# order_l = [[0 for i in range(32)] for j in range(num_of_types)]\n",
    "random.shuffle(files_file)\n",
    "if args_ex_mode == 3:\n",
    "    I0 = onehotlist(3)\n",
    "    I = onehotlist(9)\n",
    "elif args_ex_mode == 5:\n",
    "    I0 = onehotlist(3)\n",
    "    I = onehotlist(3)\n",
    "else:\n",
    "    I = onehotlist(args_arch_num)\n",
    "n = len(files_file)\n",
    "i = 0\n",
    "for f in files_file:\n",
    "  i = i + 1\n",
    "  if i % int(n/10) == 0:\n",
    "    print(int(i/int(n/10)),'/10')\n",
    "  ft = f.replace(path,\"\").replace(os.path.basename(f),\"\").split(\"/\",1)[0]\n",
    "  if ft not in num_of_dataset:\n",
    "    num_of_dataset[ft] = 0\n",
    "  if args_s_limit > 0 and num_of_dataset[ft] >= args_s_limit:\n",
    "    continue\n",
    "  ftype = np.int8(file_types[ft])\n",
    "  fin = open(f,\"rb\")\n",
    "  bdata = fin.read()\n",
    "  fsize = len(bdata)\n",
    "  if fsize < block_size:\n",
    "    continue\n",
    "\n",
    "  for c in range(0,fsize-block_size,block_size):\n",
    "    if args_s_limit > 0 and num_of_dataset[ft] >= args_s_limit:\n",
    "      break\n",
    "    offset = c*1.0/fsize\n",
    "    block = bdata[c:c+block_size]\n",
    "    train = []\n",
    "    #1 Byte to 8 bit-array\n",
    "    for x in block:\n",
    "      num_of_byte[ftype][x]+=1\n",
    "    train = list(map(lambda x:BitArray[x],block))\n",
    "    if args_ex_mode == 3:\n",
    "        train.append(I0[new_label_s0[ftype]])\n",
    "    elif args_ex_mode == 5:\n",
    "        train.append(I0[new_label_s0[ftype]])\n",
    "    train.append(I[new_label_s[ftype]])\n",
    "    \n",
    "    train = list(itertools.chain.from_iterable(train))\n",
    " \n",
    "    train = np.asarray(train,dtype=np.int8)\n",
    "    master_dataset_X[total_samples]=train\n",
    "    master_dataset_Y[total_samples]=new_label_m[ftype]\n",
    "    total_samples+=1\n",
    "    num_of_dataset[ft]+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DPMj3KEManUW",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "a6d774ef-47ca-4df9-99de-dd777787044a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label File Code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\n",
      "01_1_VC2003_32bit_none 1350 20000 5.497218434100005 \n",
      "01_2_VC2017_32bit_none 1170 20000 5.357332913558505 \n",
      "02_1_VC2003_32bit_max 1306 20000 6.051776864801621 \n",
      "02_2_VC2017_32bit_max 1147 20000 5.873730771880074 \n",
      "03_1_gcc6.3.0_x86_none 2111 20000 5.708839188017366 \n",
      "03_2_x86-O0-gcc7.5.0 3006 20000 5.601603488119756 \n",
      "04_1_gcc6.3.0_x86_O3 1844 20000 6.117739130113774 \n",
      "04_2_x86-O3-gcc7.5.0 2756 20000 6.137322464651781 \n",
      "05_1_clang5.0.2_32_none 1205 20000 5.776051052833051 \n",
      "05_2_x86-O0-Clang10.0.0 3258 20000 5.473631820242736 \n",
      "06_1_clang5.0.2_32_O3 1196 20000 6.2622558422432775 \n",
      "06_2_x86-O3-Clang10.0.0 3278 20000 5.9776225651704795 \n",
      "07_intel_32_none 1761 20000 5.225804946995282 \n",
      "08_intel_32bit_max 1724 20000 6.112311167284559 \n",
      "11_VC2017_64bit_none 1456 20000 4.858022796891719 \n",
      "12_VC2017_64bit_max 1242 20000 5.920874953768457 \n",
      "13_1_gcc6.3.0_64bit_none 1582 20000 5.3858367234951485 \n",
      "13_2_x86_64-O0-gcc7.5.0 3058 20000 5.308941514545626 \n",
      "14_1_gcc6.3.0_64bit_max 1580 20000 6.085241050665823 \n",
      "14_2_x86_64-O3-gcc7.5.0 2804 20000 5.855112221565452 \n",
      "15_1_clang5.0.2_64bit_none 1892 20000 5.343581547305419 \n",
      "15_2_x86_64-O0-Clang10.0.0 3063 20000 5.182212522919054 \n",
      "16_1_clang5.0.2_64bit_max 1883 20000 6.106264033906786 \n",
      "16_2_x86_64-O3-Clang10.0.0 3062 20000 5.881024606845084 \n",
      "17_intel_64_none 1796 20000 4.943592256062177 \n",
      "18_intel_64bit_max 1728 20000 5.912320800754686 \n",
      "21_arm-O0-gcc 2493 20000 6.407722902557761 \n",
      "22_arm-O3-gcc 2375 20000 6.990287405780609 \n",
      "23_arm-O0-Clang 3249 20000 5.183598860874865 \n",
      "24_arm-O3-Clang 3249 20000 5.721986005961764 \n",
      "31_arm64-O0-gcc 2501 20000 5.25242163982573 \n",
      "32_arm64-O3-gcc 2382 20000 6.145096918522342 \n",
      "33_arm64-O0-Clang 3257 20000 5.878199342148142 \n",
      "34_arm64-O3-Clang 3257 20000 6.216400185393169 \n",
      "41_mips-O0-gcc 2494 20000 4.497354890903974 \n",
      "42_mips-O3-gcc 2376 20000 5.502052730693702 \n",
      "43_mips-O0-Clang 3243 20000 4.476266278740027 \n",
      "44_mips-O3-Clang 3243 20000 5.071412147153023 \n",
      "51_mips64-O0-gcc 2451 20000 4.472452080264617 \n",
      "52_mips64-O3-gcc 2333 20000 5.642743501742682 \n",
      "53_mips64-O0-Clang 3203 20000 4.583286416827601 \n",
      "54_mips64-O3-Clang 3203 20000 5.271222853132237 \n",
      "61_powerpc-O0-gcc 2491 20000 5.500952679395219 \n",
      "62_powerpc-O3-gcc 2373 20000 6.140708928282943 \n",
      "63_powerpc-O0-Clang 3246 20000 5.035140966705927 \n",
      "64_powerpc-O3-Clang 3246 20000 5.726032886052999 \n",
      "71_powerpc64-O0-gcc 2450 20000 5.315111622592085 \n",
      "72_powerpc64-O3-gcc 2332 20000 5.818507407426359 \n",
      "73_powerpc64-O0-Clang 3203 20000 5.023700640825878 \n",
      "74_powerpc64-O3-Clang 3203 20000 5.689836511028959 \n",
      "80_document 101 20000 7.49070153918922 \n",
      "total types 51\n",
      "total files 120212\n",
      "total samples 1020000\n"
     ]
    }
   ],
   "source": [
    "#Display information about the dataset\n",
    "total_samples = 0\n",
    "total_files = 0\n",
    "total_types = 0\n",
    "t_r = 0.0\n",
    "print ('label','File','Code','1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16')\n",
    "for t in file_types_:\n",
    "  print (t, end=' ')\n",
    "  print (num_of_file_types[t], end=' ')\n",
    "  print (num_of_dataset[t], end=' ')\n",
    "  total_types+=1\n",
    "  total_files+=num_of_file_types[t]\n",
    "  total_samples+=num_of_dataset[t]\n",
    "  #calc entropy\n",
    "  r = 0.0\n",
    "  for x in range(256):\n",
    "    p_i = float(num_of_byte[file_types[t]][x])/(num_of_dataset[t]*block_size)\n",
    "    if p_i != 0:\n",
    "      r += p_i * np.log2(p_i)\n",
    "  print ((-r), end=' ')\n",
    "  t_r += (-r)\n",
    "  print()\n",
    "        \n",
    "print ('total types', total_types)\n",
    "print ('total files', total_files)\n",
    "print ('total samples', total_samples)#,t_r/total_types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# master_dataset_X =torch.FloatTensor(master_dataset_X[:total_samples])\n",
    "master_dataset_Y =torch.LongTensor(master_dataset_Y[:total_samples])\n",
    "# master_dataset = TensorDataset(master_dataset_X, master_dataset_Y)\n",
    "# print(master_dataset_X[0],master_dataset_Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IS66SK9lFb1t",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "7683e3d4-ff35-479c-8520-748e2216295c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0+cu102\n"
     ]
    }
   ],
   "source": [
    "# PyTorch version\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fEl-Hz5MER4A",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "e684b415-bfd8-425a-c56a-81210b0c6276",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(37) [0 0 0 ... 0 0 0] (1889,)\n"
     ]
    }
   ],
   "source": [
    "#Dataset Class\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data = data\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        \n",
    "        self.len = len(labels)\n",
    "        self.idxs = [x for x in range(self.len)]\n",
    "        random.shuffle(self.idxs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def set_mask(mask):\n",
    "        self.mask = mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #idx = self.idxs[idx]\n",
    "        #block = copy.copy(self.data[idx])\n",
    "        #block = list(map(int,self.data[idx]))\n",
    "        out_label = self.labels[idx]\n",
    "        real_data = np.copy(self.data[idx])\n",
    "        \n",
    "\n",
    "        return real_data, out_label\n",
    "# master_dataset = MyDataset(master_dataset_X, master_dataset_Y,True)\n",
    "master_dataset = MyDataset(master_dataset_X, master_dataset_Y)\n",
    "x,y = master_dataset[0]\n",
    "print(y,x,x.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySubset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, indices, mask=None):\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "        self.mask = mask\n",
    "        self.mask_len = int(block_size*args_dar)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        real_data, label = self.dataset[self.indices[idx]]\n",
    "        if self.mask:\n",
    "            if random.random() < args_dar2:\n",
    "                da_list = random.sample(range(block_size), self.mask_len)\n",
    "                for pos in da_list:\n",
    "\n",
    "                    rnd = random.random()\n",
    "                    if rnd < 0.8:\n",
    "                        real_data[pos*8:pos*8+8] = BitArray[random.randrange(256)]\n",
    "                    # elif rnd < 0.9:\n",
    "                    #     continue\n",
    "                    else:\n",
    "                        # real_data[pos*8:pos*8+8] = BitArray[256]\n",
    "                        continue\n",
    "            \n",
    "\n",
    "        return torch.FloatTensor(real_data), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "55GH8e9tO3Lp",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "  def __init__(self, length, depth):\n",
    "    super(Attention,self).__init__()\n",
    "    self.l_q = nn.Conv1d(depth, depth*4, 1)\n",
    "    self.l_k = nn.Conv1d(depth, depth*4, 1)\n",
    "    self.l_v = nn.Conv1d(depth, depth*4, 1)\n",
    "    self.l_q2 = nn.Conv1d(depth*4, depth, 1)\n",
    "    self.l_k2 = nn.Conv1d(depth*4, depth, 1)\n",
    "    self.l_v2 = nn.Conv1d(depth*4, depth, 1)\n",
    "    self.depth = depth\n",
    "    self.length = length\n",
    "\n",
    "  def forward(self,x):\n",
    "    value = self.l_v(x)\n",
    "    value = F.relu(value)\n",
    "    value = self.l_v2(value)\n",
    "    value = F.relu(value)\n",
    "\n",
    "    attention_weight = self.get_aw(x)\n",
    "\n",
    "    attention_output = torch.bmm(value,attention_weight)\n",
    "    net_out = attention_output\n",
    "        \n",
    "    return net_out\n",
    "  def get_aw(self, x):\n",
    "    Memory = x\n",
    "    query = self.l_q(x)\n",
    "    key = self.l_k(x)\n",
    "\n",
    "    query = F.relu(query)\n",
    "    key = F.relu(key)\n",
    "\n",
    "    query = self.l_q2(query)\n",
    "    key = self.l_k2(key)\n",
    "\n",
    "    logit = torch.bmm(key.permute(0,2,1),query)\n",
    "    attention_weight = F.softmax(logit,dim=2)#.permute(0,2,1)\n",
    "\n",
    "    return attention_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "7LlGeWRlHDSt",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "  def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    pe = pe.unsqueeze(0).transpose(1, 2)\n",
    "    self.register_buffer('pe', pe)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.pe[:,:,:x.size(2)]\n",
    "    return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class o_glassesX(nn.Module):\n",
    "    def __init__(self, depth, length, n_out, input_depth = None):\n",
    "        super(o_glassesX,self).__init__()\n",
    "        if input_depth == None:\n",
    "            input_depth = depth\n",
    "        self.linear = nn.Conv1d(input_depth, depth, 1, 1)\n",
    "        self.bn = nn.BatchNorm1d(depth)\n",
    "        self.pe = PositionalEncoding(depth, 0.0)\n",
    "        self.att = Attention(length, depth)\n",
    "        self.fc = nn.Linear(depth*length, n_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mask = x > -10\n",
    "        mask = torch.all(mask,dim=1,keepdim=True)\n",
    "        # print(mask.shape)\n",
    "        # print(b)\n",
    "        #Embedding\n",
    "        x = self.linear(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn(x)\n",
    "        x = x * mask[:,:x.shape[1],:]\n",
    "        # Positional Encoding\n",
    "        x = self.pe(x)\n",
    "        # Attention\n",
    "        x = self.att(x)\n",
    "        # x = x * mask[:,:x.shape[1],:]\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class o_glassesX2(nn.Module):\n",
    "    def __init__(self, depth, length, n_out, n_out2,input_depth):\n",
    "        super(o_glassesX2,self).__init__()\n",
    "        self.sub1 = o_glassesX(depth, length, n_out, input_depth)\n",
    "        self.sub2 = o_glassesX(depth, length, n_out2, input_depth)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.sub1(x)\n",
    "        x1 = F.softmax(x1,dim=1)\n",
    "        x2 = self.sub2(x)\n",
    "        x2 = F.softmax(x2,dim=1)\n",
    "        \n",
    "        x = torch.cat([x1,x2], dim=1)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "  def __init__(self, op_len, n_out):\n",
    "    super(Model,self).__init__()\n",
    "    self.depth = 32\n",
    "    self.com_len = 4\n",
    "    self.arch_num = args_arch_num\n",
    "    op_len = op_len - self.com_len + 1\n",
    "    if args_ex_mode == 3:\n",
    "        self.sub = o_glassesX2(self.depth, op_len, 3, 9, self.depth)\n",
    "    elif args_ex_mode == 5:\n",
    "        self.sub = o_glassesX2(self.depth, op_len, 3, 3, self.depth)\n",
    "    else:\n",
    "        self.sub = o_glassesX(self.depth, op_len, self.arch_num, self.depth)\n",
    "    self.main = o_glassesX(self.depth, op_len, n_out, self.depth + self.arch_num)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def im2col(self,x):\n",
    "    x = torch.split(x,8,dim=1)\n",
    "    x = torch.stack(x,dim=2)\n",
    "    x = torch.cat([x[:,:,0:-3],x[:,:,1:-2],x[:,:,2:-1],x[:,:,3:]],dim=1)\n",
    "    return x\n",
    "    \n",
    "  def forward(self, x, f_arch = True):\n",
    "\n",
    "    a = x[:,-self.arch_num:]\n",
    "    x = x[:,:-self.arch_num]\n",
    "\n",
    "    # im2col\n",
    "    x = self.im2col(x)\n",
    "    if f_arch:\n",
    "        a = self.get_arch(x)\n",
    "        if args_ex_mode != 3:\n",
    "            a = F.softmax(a,dim=1)\n",
    "\n",
    "    # broadcast concatenate\n",
    "    a = a[:,:,np.newaxis]\n",
    "    a = a.expand(x.size(0),self.arch_num,x.size(2))\n",
    "    x = torch.cat([x,a], dim=1)\n",
    "        \n",
    "    x = self.main(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "  def get_arch(self,x):\n",
    "    x = self.sub(x)\n",
    "    # x = F.softmax(x,dim=1)\n",
    "    \n",
    "    return x\n",
    "\n",
    "  def forward2(self, x):\n",
    "    a = x[:,-self.arch_num:]\n",
    "    x = x[:,:-self.arch_num]\n",
    "\n",
    "    # im2col\n",
    "    x = self.im2col(x)\n",
    "    a = self.get_arch(x)\n",
    "    if args_ex_mode != 3:\n",
    "        a_ = F.softmax(a,dim=1)\n",
    "    else:\n",
    "        a_ = a.clone()\n",
    "\n",
    "    # broadcast concatenate\n",
    "    a_ = a_[:,:,np.newaxis]\n",
    "    a_ = a_.expand(x.size(0),self.arch_num,x.size(2))\n",
    "    x = torch.cat([x,a_], dim=1)\n",
    "        \n",
    "    x = self.main(x)\n",
    "\n",
    "    return x,a\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "d37Pk4u_XGrw",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "51ba7f58-2c72-4f09-d332-4bef43ff5322",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00:01:40'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def time2str(elapsed_time):\n",
    "    elapsed_time = int(elapsed_time)\n",
    "    \n",
    "    hour = elapsed_time // 3600\n",
    "    minute = (elapsed_time % 3600) // 60\n",
    "    second = (elapsed_time % 3600 % 60)\n",
    "    \n",
    "    return str(hour).zfill(2) + \":\" + str(minute).zfill(2) + \":\" + str(second).zfill(2)\n",
    "\n",
    "time2str(100.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "-UT1Hn2bXzP7",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model(model,file_name = args_output_model):\n",
    "    # 保存\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to('cpu')\n",
    "    torch.save(model.state_dict(),file_name+'.pth')\n",
    "    model.to(device)\n",
    "    #if F_download:\n",
    "    #    files.download(file_name+'.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "PqqsaNkdXgaO",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(file_name=args_input_model):\n",
    "    # 3. ネットワーク構築\n",
    "    # model = nn.Sequential()\n",
    "    # model.add_module('l1', Model(block_size, num_of_types))\n",
    "    #model = Model(block_size, num_of_types)\n",
    "    # 学習済みモデル読み込み\n",
    "    model.load_state_dict(torch.load(file_name + '.pth'))\n",
    "    model.to(device)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import log10 , floor\n",
    "def round_it(x, sig):\n",
    "    return round(x, sig-int(floor(log10(abs(x))))-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clip_grad_norm_(main,sub, rate: float, norm_type: float = 2.0) -> torch.Tensor:\n",
    "    rate = float(rate)\n",
    "    m_params = main.parameters()\n",
    "    s_params = sub.parameters()\n",
    "    if isinstance(m_params, torch.Tensor):\n",
    "        m_params = [m_params]\n",
    "    if isinstance(s_params, torch.Tensor):\n",
    "        s_params = [s_params]\n",
    "    m_params = [p for p in m_params if p.grad is not None]\n",
    "    s_params = [p for p in s_params if p.grad is not None]\n",
    "    norm_type = float(norm_type)\n",
    "\n",
    "    device = m_params[0].grad.device\n",
    "\n",
    "    total_norm_s = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in s_params]), norm_type)\n",
    "    total_norm_m = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in m_params]), norm_type)\n",
    "\n",
    "    clip_coef = total_norm_m / max([total_norm_s,1e-16])\n",
    "    if clip_coef > rate:\n",
    "        clip_coef_clamped = rate * total_norm_s / max([total_norm_m,1e-16])\n",
    "        for p in m_params:\n",
    "            p.grad.detach().mul_(clip_coef_clamped.to(p.grad.device))\n",
    "    return clip_coef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def GradNorm(parameters, rate: float = 1.0, norm_type: float = 2.0):\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p.grad is not None]\n",
    "    rate = float(rate)\n",
    "    norm_type = float(norm_type)\n",
    "    device = parameters[0].grad.device\n",
    "    \n",
    "    rate = min([torch.mean(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters])),rate])\n",
    "        \n",
    "    for p in parameters:\n",
    "        total_norm = torch.norm(p.grad.detach(), norm_type).to(device)\n",
    "        clip_coef = rate / (total_norm  + 1e-16)\n",
    "        p.grad.detach().mul_(clip_coef.to(p.grad.device))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ラベル置き換え用\n",
    "num_of_types = [51,9,26,51,19,19][args_ex_mode]\n",
    "file_types_=[file_types_[t] for t in range(num_of_types)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "TFYZcaKPxGEJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kfold_test(k):\n",
    "  global model, device\n",
    "  global args_lr\n",
    "  kf = StratifiedKFold(n_splits=k,shuffle=True,random_state=0)\n",
    "  mtp = [0 for j in range(num_of_types)]\n",
    "  mfp = [0 for j in range(num_of_types)]\n",
    "  mfn = [0 for j in range(num_of_types)]\n",
    "  mtn = [0 for j in range(num_of_types)]\n",
    "  mftn = [0 for j in range(num_of_types)]\n",
    "  mrs = [[0 for i in range(num_of_types)] for j in range(num_of_types)]\n",
    "  for fold_idx, (train_idx, test_idx) in enumerate(kf.split(master_dataset_X, master_dataset_Y)):\n",
    "    print(fold_idx+1)\n",
    "    arch_num = args_arch_num\n",
    "\n",
    "    ds_train = MySubset(master_dataset, train_idx,True)\n",
    "    ds_test = MySubset(master_dataset, test_idx)\n",
    "\n",
    "    loader_train = DataLoader(ds_train, batch_size=args_batch_size, shuffle=True)\n",
    "    loader_test = DataLoader(ds_test, batch_size=args_batch_size, shuffle=False)\n",
    "\n",
    "    model = Model(op_num, num_of_types)\n",
    "    model.to(device)\n",
    "    \n",
    "    \n",
    "    # multi GPU\n",
    "    if device == 'cuda' and F_multi_GPU:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    def main_train():\n",
    "      train_loss = 0\n",
    "      model.train()\n",
    "      #MainNet Training\n",
    "      for data, targets in loader_train:\n",
    "        if torch.cuda.is_available():\n",
    "          data, targets = data.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data,False)\n",
    "            \n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        # #Layer-wise Gradient Normalization\n",
    "        # GradNorm(model.parameters(),args_gn_rate)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "      scheduler.step()\n",
    "      return train_loss / len(loader_train.dataset)\n",
    "        \n",
    "    def sub_train():\n",
    "      train_loss = 0\n",
    "      model.train()\n",
    "      for data, targets in loader_train:\n",
    "        if torch.cuda.is_available():\n",
    "          data, targets = data.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model.get_arch(model.im2col(data[:,:-arch_num]))\n",
    "        _, targets = torch.max(data[:,-arch_num:], 1)\n",
    "            \n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        # #Layer-wise Gradient Normalization\n",
    "        # GradNorm(model.parameters(),args_gn_rate)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "      scheduler.step()\n",
    "      return train_loss / len(loader_train.dataset)\n",
    "        \n",
    "    def train():\n",
    "      train_loss = 0\n",
    "      model.train()\n",
    "      for data, targets in loader_train:\n",
    "        if torch.cuda.is_available():\n",
    "          data, targets = data.to(device), targets.to(device)\n",
    "        if False:#for LTL\n",
    "          #MainNet Training\n",
    "          for p in model.main.parameters():\n",
    "              p.requires_grad = True\n",
    "          for p in model.sub.parameters():\n",
    "              p.requires_grad = False\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(data,False)\n",
    "            \n",
    "          loss = loss_fn(outputs, targets)\n",
    "          loss.backward()\n",
    "          # #Layer-wise Gradient Normalization\n",
    "          # GradNorm(model.parameters(),args_gn_rate)\n",
    "          optimizer.step()\n",
    "          #Leaky Transfer Learning\n",
    "          for p in model.main.parameters():\n",
    "              p.requires_grad = True\n",
    "          for p in model.sub.parameters():\n",
    "              p.requires_grad = True\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(data,True)\n",
    "          loss = loss_fn(outputs, targets)\n",
    "          loss.backward()\n",
    "          # GradNorm(model.parameters(),args_gn_rate)\n",
    "          #勾配クリッピング\n",
    "          clip_grad_norm_(model.main,model.sub, args_max_norm)\n",
    "        elif args_lm == 4:\n",
    "          #Embedded in loss functions\n",
    "          optimizer.zero_grad()\n",
    "          outputs, outputs2 = model.forward2(data)\n",
    "          _, targets2 = torch.max(data[:,-arch_num:], 1)\n",
    "            \n",
    "          loss = loss_fn(outputs, targets) + loss_fn(outputs2, targets2)\n",
    "          loss.backward()\n",
    "        elif args_lm < 4:#for TL,FT\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(data,True)\n",
    "            \n",
    "          loss = loss_fn(outputs, targets)\n",
    "          loss.backward()\n",
    "          #勾配クリッピング\n",
    "          # clip_grad_norm_(model.main,model.sub, args_max_norm)\n",
    "        elif args_lm ==5:#for DFT+\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(data,True)\n",
    "            \n",
    "          loss = loss_fn(outputs, targets)\n",
    "          loss.backward()\n",
    "        elif args_lm ==6:#for Simple\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(data,True)\n",
    "            \n",
    "          loss = loss_fn(outputs, targets)\n",
    "          loss.backward()\n",
    "              \n",
    "            \n",
    "\n",
    "        # GradNorm(model.parameters(),args_gn_rate)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "      scheduler.step()\n",
    "      return train_loss / len(loader_train.dataset)\n",
    "    \n",
    "    def test(epoch=0):\n",
    "      model.eval()\n",
    "      if False:\n",
    "        correct = 0\n",
    "        correct_a = 0\n",
    "        correct_a2 = 0\n",
    "        train_loss = 0\n",
    "        with torch.no_grad():\n",
    "          for data, targets in loader_train:\n",
    "            if torch.cuda.is_available():\n",
    "              data, targets = data.to(device), targets.to(device)\n",
    "            outputs, outputs2 = model.forward2(data)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += predicted.eq(targets.data.view_as(predicted)).sum()\n",
    "\n",
    "            if args_ex_mode == 1:\n",
    "                _, predicted = torch.max(outputs2.data, 1)\n",
    "                _, targets2 = torch.max(data[:,-arch_num:], 1)\n",
    "                correct_a += predicted.eq(targets2.data.view_as(predicted)).sum()\n",
    "                \n",
    "                predicted = list(map(lambda x:new_label_m[int(x)],predicted.cpu()))\n",
    "                predicted =torch.LongTensor(predicted).to(device)\n",
    "                correct_a2 += predicted.eq(targets.data.view_as(predicted)).sum()\n",
    "            elif args_ex_mode == 3:\n",
    "                _, predicted = torch.max(outputs2[:,:3].data, 1)\n",
    "                _, targets = torch.max(data[:,-arch_num:-arch_num+3], 1)\n",
    "                correct_a += predicted.eq(targets.data.view_as(predicted)).sum()\n",
    "                _, predicted = torch.max(outputs2[:,3:].data, 1)\n",
    "                _, targets = torch.max(data[:,-arch_num+3:], 1)\n",
    "                correct_a2 += predicted.eq(targets.data.view_as(predicted)).sum()\n",
    "            elif args_ex_mode == 5:\n",
    "                _, predicted = torch.max(outputs2[:,:3].data, 1)\n",
    "                _, targets = torch.max(data[:,-arch_num:-arch_num+3], 1)\n",
    "                correct_a += predicted.eq(targets.data.view_as(predicted)).sum()\n",
    "                _, predicted = torch.max(outputs2[:,3:].data, 1)\n",
    "                _, targets = torch.max(data[:,-arch_num+3:], 1)\n",
    "                correct_a2 += predicted.eq(targets.data.view_as(predicted)).sum()\n",
    "            else:\n",
    "                _, predicted = torch.max(outputs2.data, 1)\n",
    "                _, targets = torch.max(data[:,-arch_num:], 1)\n",
    "                correct_a += predicted.eq(targets.data.view_as(predicted)).sum()\n",
    "\n",
    "        data_num = len(loader_train.dataset)\n",
    "        print('Train Acc.：{}/{}({:.2f}%),{}/{}({:.2f}%)'.format(correct, data_num, 100. * correct /data_num, correct_a, data_num, 100. * correct_a /data_num), end=\" \")\n",
    "        if args_ex_mode == 1 or args_ex_mode == 3 or args_ex_mode == 5:\n",
    "            print(',{}/{}({:.2f}%)'.format(correct_a2, data_num, 100. * correct_a2 /data_num), end=\" \")\n",
    "        print(round_it(train_loss/data_num,5),end=\" \")\n",
    "\n",
    "      correct = 0\n",
    "      correct_a = 0\n",
    "      correct_a2 = 0\n",
    "      test_loss = 0\n",
    "      with torch.no_grad():\n",
    "        for data, targets in loader_test:\n",
    "          if torch.cuda.is_available():\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "          outputs, outputs2 = model.forward2(data)\n",
    "\n",
    "          loss = loss_fn(outputs, targets)\n",
    "          test_loss += loss.item()\n",
    "\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          correct += predicted.eq(targets.data.view_as(predicted)).sum()\n",
    "\n",
    "          if args_ex_mode == 1:\n",
    "              _, predicted = torch.max(outputs2.data, 1)\n",
    "              _, targets2 = torch.max(data[:,-arch_num:], 1)\n",
    "              correct_a += predicted.eq(targets2.data.view_as(predicted)).sum()\n",
    "            \n",
    "              predicted = list(map(lambda x:new_label_m[int(x)],predicted.cpu()))\n",
    "              predicted =torch.LongTensor(predicted).to(device)\n",
    "              correct_a2 += predicted.eq(targets.data.view_as(predicted)).sum()\n",
    "          elif args_ex_mode == 3:\n",
    "              _, predicted = torch.max(outputs2[:,:3].data, 1)\n",
    "              _, targets = torch.max(data[:,-arch_num:-arch_num+3], 1)\n",
    "              correct_a += predicted.eq(targets.data.view_as(predicted)).sum()\n",
    "              _, predicted = torch.max(outputs2[:,3:].data, 1)\n",
    "              _, targets = torch.max(data[:,-arch_num+3:], 1)\n",
    "              correct_a2 += predicted.eq(targets.data.view_as(predicted)).sum()\n",
    "          elif args_ex_mode == 5:\n",
    "              _, predicted = torch.max(outputs2[:,:3].data, 1)\n",
    "              _, targets = torch.max(data[:,-arch_num:-arch_num+3], 1)\n",
    "              correct_a += predicted.eq(targets.data.view_as(predicted)).sum()\n",
    "              _, predicted = torch.max(outputs2[:,3:].data, 1)\n",
    "              _, targets = torch.max(data[:,-arch_num+3:], 1)\n",
    "              correct_a2 += predicted.eq(targets.data.view_as(predicted)).sum()\n",
    "          else:\n",
    "              _, predicted = torch.max(outputs2.data, 1)\n",
    "              _, targets = torch.max(data[:,-arch_num:], 1)\n",
    "              correct_a += predicted.eq(targets.data.view_as(predicted)).sum()\n",
    "\n",
    "      data_num = len(loader_test.dataset)\n",
    "      print('Test Acc.：{}/{}({:.2f}%),{}/{}({:.2f}%)'.format(correct, data_num, 100. * correct /data_num, correct_a, data_num, 100. * correct_a /data_num), end=\" \")\n",
    "      if args_ex_mode == 1 or args_ex_mode == 3 or args_ex_mode == 5:\n",
    "          print(',{}/{}({:.2f}%)'.format(correct_a2, data_num, 100. * correct_a2 /data_num), end=\" \")\n",
    "      print(round_it(test_loss/data_num,5),end=\" \")\n",
    "\n",
    "      return test_loss\n",
    "\n",
    "    def test_d(epoch=0):\n",
    "      model.eval()\n",
    "      tp = [0 for j in range(num_of_types)]\n",
    "      fp = [0 for j in range(num_of_types)]\n",
    "      fn = [0 for j in range(num_of_types)]\n",
    "      tn = [0 for j in range(num_of_types)]\n",
    "      ftn = [0 for j in range(num_of_types)]\n",
    "      rs = [[0 for j2 in range(num_of_types)] for j in range(num_of_types)]\n",
    "      with torch.no_grad():\n",
    "        for data, targets in loader_test:\n",
    "          if torch.cuda.is_available():\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "          outputs = model(data)\n",
    "\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          for i in range(len(targets)):\n",
    "            ft = targets[i]\n",
    "            result = predicted[i]\n",
    "            if ft == result:\n",
    "              tp[ft] += 1\n",
    "              tn[result] += 1\n",
    "              mtp[ft] += 1\n",
    "              mtn[result] += 1\n",
    "            else:\n",
    "              fp[ft] += 1\n",
    "              fn[result] += 1\n",
    "              mfp[ft] += 1\n",
    "              mfn[result] += 1\n",
    "            ftn[ft] += 1\n",
    "            rs[ft][result]+=1\n",
    "            mftn[ft] += 1\n",
    "            mrs[ft][result]+=1\n",
    "      for t in file_types_:\n",
    "        print(t,end=\" \")\n",
    "      print()\n",
    "      for t in file_types_:\n",
    "        print(t,end=\" \")\n",
    "        for j in range(num_of_types):\n",
    "          print(rs[file_types[t]][j],end=\" \")\n",
    "        print()\n",
    "      print(\"no label Num TP FP FN TN R P F1 Acc.\")\n",
    "      for t in file_types_:\n",
    "        ft = file_types[t]\n",
    "        print(ft,end=\" \")\n",
    "        print(t,end=\" \")\n",
    "        print(ftn[ft],end=\" \")\n",
    "        print(tp[ft],fp[ft],fn[ft],tn[ft],end=\" \")\n",
    "        if tp[ft]+fn[ft] != 0:\n",
    "          r = float(tp[ft])/(tp[ft]+fn[ft])\n",
    "        else:\n",
    "          r = 0.0\n",
    "        print(r,end=\" \")\n",
    "        if tp[ft]+fp[ft] != 0:\n",
    "          p = float(tp[ft])/(tp[ft]+fp[ft])\n",
    "        else:\n",
    "          p = 0.0                        \n",
    "        print(p,end=\" \")\n",
    "        if r+p != 0:\n",
    "          f1 = 2*r*p/(r+p)\n",
    "        else:\n",
    "          f1 = 0.0\n",
    "        print(f1,end=\" \")\n",
    "        acc = float(tp[ft]+tn[ft])/(tp[ft]+fp[ft]+fn[ft]+tn[ft])\n",
    "        print(acc)\n",
    "      sum_ftn = sum(ftn)\n",
    "      sum_tp = sum(tp)\n",
    "      sum_fp = sum(fp)\n",
    "      sum_fn = sum(fn)\n",
    "      sum_tn = sum(tn)\n",
    "      print('','',sum_ftn,sum_tp,sum_fp,sum_fn,sum_tn,end=\" \")\n",
    "      if sum_tp+sum_fn != 0:\n",
    "        r = float(sum_tp)/(sum_tp+sum_fn)\n",
    "      else:\n",
    "        r = 0.0\n",
    "      print(r,end=\" \")\n",
    "      if sum_tp+sum_fp != 0:\n",
    "        p = float(sum_tp)/(sum_tp+sum_fp)\n",
    "      else:\n",
    "        p = 0.0                        \n",
    "      print(p,end=\" \")\n",
    "      if r+p != 0:\n",
    "        f1 = 2*r*p/(r+p)\n",
    "      else:\n",
    "        f1 = 0.0\n",
    "      print(f1,end=\" \")\n",
    "      acc = float(sum_tp+sum_tn)/(sum_tp+sum_fp+sum_fn+sum_tn)\n",
    "      print(acc)\n",
    "\n",
    "\n",
    "    model_s0 = model.sub.to('cpu').state_dict()\n",
    "    model.sub.to(device)\n",
    "\n",
    "    if f_pretrain_mainnet:#for TL2,FT2\n",
    "      #PreTrain Mainnet\n",
    "      # Loss Functions\n",
    "      if args_smoothing:\n",
    "        loss_fn = LabelSmoothingCrossEntropy(smoothing=args_smoothing)\n",
    "      else:\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "      # Minimizer\n",
    "      optimizer = torch.optim.SGD(model.main.parameters(), lr=args_lr, momentum=args_momentum, weight_decay=args_weight_decay)\n",
    "      # Learning Rate Scheduler\n",
    "      scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args_epoch)\n",
    "\n",
    "      start_time = time.time()\n",
    "      print(\"Start MainNet PreTrain\")\n",
    "      for epoch in range(args_epoch):\n",
    "        print('epoch:',epoch+1,end=' ')\n",
    "        train_loss = main_train()\n",
    "        print('train_loss:', round_it(train_loss,5), end = ' ')\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(time2str(elapsed_time), end=\" \")\n",
    "        print(time2str(elapsed_time/(epoch+1)*args_epoch-elapsed_time),\"left. \")\n",
    "      # main_state = model.main.state_dict()\n",
    "    if f_pretrain_subnet:#for TL1,FT1\n",
    "      #PreTrain Subnet\n",
    "      # Loss Functions\n",
    "      if args_smoothing:\n",
    "        loss_fn = LabelSmoothingCrossEntropy(smoothing=args_smoothing)\n",
    "      else:\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "      # Minimizer\n",
    "      optimizer = torch.optim.SGD(model.sub.parameters(), lr=args_lr, momentum=args_momentum, weight_decay=args_weight_decay)\n",
    "      # Learning Rate Scheduler\n",
    "      scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args_epoch)\n",
    "\n",
    "      start_time = time.time()\n",
    "      print(\"Start SubNet PreTrain\")\n",
    "      for epoch in range(args_epoch):\n",
    "        print('epoch:',epoch+1,end=' ')\n",
    "        train_loss = sub_train()\n",
    "        print('train_loss:', round_it(train_loss,5), end = ' ')\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(time2str(elapsed_time), end=\" \")\n",
    "        print(time2str(elapsed_time/(epoch+1)*args_epoch-elapsed_time),\"left. \")\n",
    "      # sub_state = model.sub.state_dict()\n",
    "        \n",
    "    # if args_lm == 5 or args_lm==1:#for DFT+,DTL2\n",
    "    if args_lm == 5:#for DFT+\n",
    "      #warmup Subnet\n",
    "      # Loss Functions\n",
    "      # args_lr=0.0001\n",
    "      if args_smoothing:\n",
    "        loss_fn = LabelSmoothingCrossEntropy(smoothing=args_smoothing)\n",
    "      else:\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "      # Minimizer\n",
    "      optimizer = torch.optim.SGD(model.sub.parameters(), lr=args_lr, momentum=args_momentum, weight_decay=args_weight_decay)\n",
    "      # Learning Rate Scheduler\n",
    "      scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args_epoch)\n",
    "\n",
    "      start_time = time.time()\n",
    "      print(\"Start SubNet warmup\")\n",
    "      for epoch in range(args_epoch):\n",
    "        print('epoch:',epoch+1,end=' ')\n",
    "        train_loss = train()\n",
    "        print('train_loss:', round_it(train_loss,5), end = ' ')\n",
    "        test_loss = test(epoch+1)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(time2str(elapsed_time), end=\" \")\n",
    "        print(time2str(elapsed_time/(epoch+1)*args_epoch-elapsed_time),\"left. \")\n",
    "      # sub_state = model.sub.state_dict()\n",
    "      args_lr=0.025\n",
    "\n",
    "    # Loss Functions\n",
    "    if args_smoothing:\n",
    "      loss_fn = LabelSmoothingCrossEntropy(smoothing=args_smoothing)\n",
    "    else:\n",
    "      loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    # Minimizer\n",
    "    if args_lm >=2:#fot LTL,FT,2loss,Simple\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=args_lr, momentum=args_momentum, weight_decay=args_weight_decay)\n",
    "    elif args_lm == 0:#for TL1\n",
    "        optimizer = torch.optim.SGD(model.main.parameters(), lr=args_lr, momentum=args_momentum, weight_decay=args_weight_decay)\n",
    "    elif args_lm == 1:#for TL2\n",
    "        optimizer = torch.optim.SGD(model.sub.parameters(), lr=args_lr, momentum=args_momentum, weight_decay=args_weight_decay)\n",
    "        \n",
    "    # Learning Rate Scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args_epoch)\n",
    "\n",
    "    mini_loss = test()\n",
    "    print()\n",
    "    save_model(model, args_output_model)\n",
    "    opt_state = optimizer.state_dict()\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(args_epoch):\n",
    "      print('epoch:',epoch+1,end=' ')\n",
    "      train_loss = train()\n",
    "      print('train_loss:', round_it(train_loss,5), end = ' ')\n",
    "      # if epoch == 0:\n",
    "      #   mini_loss = train_loss\n",
    "      #   save_model(model, args_output_model)\n",
    "      if False:#train_loss > 1.1 * mini_loss:\n",
    "        model = load_model(args_output_model)\n",
    "        print('rerload model',epoch+1, end = ' ')\n",
    "      else:\n",
    "        test_loss = test(epoch+1)\n",
    "        if mini_loss > train_loss:\n",
    "          mini_loss = train_loss\n",
    "          opt_state = optimizer.state_dict()\n",
    "          save_model(model, args_output_model)\n",
    "          print(\"Save model\",end=' ')\n",
    "        # elif mini_loss*1.1 < train_loss:\n",
    "        #   # model = load_model(args_output_model)\n",
    "        #   load_model(args_output_model)\n",
    "        #   optimizer.load_state_dict(opt_state)\n",
    "        #   print(\"Load model\",end=' ')\n",
    "      elapsed_time = time.time() - start_time\n",
    "      print(time2str(elapsed_time), end=\" \")\n",
    "      print(time2str(elapsed_time/(epoch+1)*args_epoch-elapsed_time),\"left. \")\n",
    "\n",
    "    #subnetの貢献計算用\n",
    "    # load_model(args_output_model)\n",
    "    model.sub.load_state_dict(model_s0)\n",
    "    model.sub.to(device)\n",
    "    test(0)\n",
    "    print('')\n",
    "    \n",
    "    model = load_model(args_output_model)\n",
    "\n",
    "    test_d()\n",
    "    if args_output_model:\n",
    "        # Saving the trained model\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.to('cpu')\n",
    "        torch.save(model.state_dict(),args_output_model+'.pth')\n",
    "        if F_download:\n",
    "          files.download(args_output_model+'.pth')\n",
    "        # return\n",
    "    # return\n",
    "  print('All results')\n",
    "  for t in file_types_:\n",
    "    print(t,end=\" \")\n",
    "  print()\n",
    "  for t in file_types_:\n",
    "    print(t,end=\" \")\n",
    "    for j in range(num_of_types):\n",
    "      print(mrs[file_types[t]][j],end=\" \")\n",
    "    print()\n",
    "  print(\"no label Num TP FP FN TN R P F1 Acc.\")\n",
    "  for t in file_types_:\n",
    "    ft = file_types[t]\n",
    "    print(ft,end=\" \")\n",
    "    print(t,end=\" \")\n",
    "    print(mftn[ft],end=\" \")\n",
    "    print(mtp[ft],mfp[ft],mfn[ft],mtn[ft],end=\" \")\n",
    "    if mtp[ft]+mfn[ft] != 0:\n",
    "      r = float(mtp[ft])/(mtp[ft]+mfn[ft])\n",
    "    else:\n",
    "      r = 0.0\n",
    "    print(r,end=\" \")\n",
    "    if mtp[ft]+mfp[ft] != 0:\n",
    "      p = float(mtp[ft])/(mtp[ft]+mfp[ft])\n",
    "    else:\n",
    "      p = 0.0                        \n",
    "    print(p,end=\" \")\n",
    "    if r+p != 0:\n",
    "      f1 = 2*r*p/(r+p)\n",
    "    else:\n",
    "      f1 = 0.0\n",
    "    print(f1,end=\" \")\n",
    "    acc = float(mtp[ft]+mtn[ft])/(mtp[ft]+mfp[ft]+mfn[ft]+mtn[ft])\n",
    "    print(acc)\n",
    "  sum_mftn = sum(mftn)\n",
    "  sum_mtp = sum(mtp)\n",
    "  sum_mfp = sum(mfp)\n",
    "  sum_mfn = sum(mfn)\n",
    "  sum_mtn = sum(mtn)\n",
    "  print('','',sum_mftn,sum_mtp,sum_mfp,sum_mfn,sum_mtn,end=\" \")\n",
    "  if sum_mtp+sum_mfn != 0:\n",
    "    r = float(sum_mtp)/(sum_mtp+sum_mfn)\n",
    "  else:\n",
    "    r = 0.0\n",
    "  print(r,end=\" \")\n",
    "  if sum_mtp+sum_mfp != 0:\n",
    "    p = float(sum_mtp)/(sum_mtp+sum_mfp)\n",
    "  else:\n",
    "    p = 0.0                        \n",
    "  print(p,end=\" \")\n",
    "  if r+p != 0:\n",
    "    f1 = 2*r*p/(r+p)\n",
    "  else:\n",
    "    f1 = 0.0\n",
    "  print(f1,end=\" \")\n",
    "  acc = float(sum_mtp+sum_mtn)/(sum_mtp+sum_mfp+sum_mfn+sum_mtn)\n",
    "  print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ijzpx_G0xKxY",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def detect(bb):\n",
    "    x = bb\n",
    "    y = model(x)\n",
    "    \n",
    "    _, predicted = torch.max(y,1)\n",
    "    print(predicted)\n",
    "    \n",
    "    aw = model.get_aw(x)\n",
    "    \n",
    "    l2 = torch.norm(aw, 2, dim=2)\n",
    "    aw2 = [0.0 for i in range(op_num)]\n",
    "    for i in range(len(l2[0])):\n",
    "      aw2[i]+=float(l2[0][i])\n",
    "    return predicted,aw2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yRvAe4qlgN70",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "a4bfca81-32ca-4d9a-9d92-da8383ef8287",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(cuda_device if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O2XMBhUezz6u",
    "outputId": "8a50f80c-7cc6-4c2a-fecd-4ab695376972",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Start MainNet PreTrain\n",
      "epoch: 1 train_loss: 0.018046 00:01:47 05:57:58 left. \n",
      "epoch: 2 train_loss: 0.015272 00:03:36 05:57:58 left. \n",
      "epoch: 3 train_loss: 0.014791 00:05:29 06:00:29 left. \n",
      "epoch: 4 train_loss: 0.014547 00:07:28 06:06:06 left. \n",
      "epoch: 5 train_loss: 0.01438 00:09:28 06:09:48 left. \n",
      "epoch: 6 train_loss: 0.014256 00:11:27 06:10:27 left. \n",
      "epoch: 7 train_loss: 0.014163 00:13:26 06:10:35 left. \n",
      "epoch: 8 train_loss: 0.014098 00:15:25 06:10:04 left. \n",
      "epoch: 9 train_loss: 0.014062 00:17:24 06:09:24 left. \n",
      "epoch: 10 train_loss: 0.014007 00:19:25 06:09:02 left. \n",
      "epoch: 11 train_loss: 0.013972 00:21:25 06:08:05 left. \n",
      "epoch: 12 train_loss: 0.013944 00:23:24 06:06:45 left. \n",
      "epoch: 13 train_loss: 0.013911 00:25:23 06:05:16 left. \n",
      "epoch: 14 train_loss: 0.013898 00:27:23 06:03:55 left. \n",
      "epoch: 15 train_loss: 0.013866 00:29:22 06:02:17 left. \n",
      "epoch: 16 train_loss: 0.013845 00:31:23 06:00:59 left. \n",
      "epoch: 17 train_loss: 0.013823 00:33:22 05:59:20 left. \n",
      "epoch: 18 train_loss: 0.013834 00:35:21 05:57:34 left. \n",
      "epoch: 19 train_loss: 0.013798 00:37:20 05:55:45 left. \n",
      "epoch: 20 train_loss: 0.013788 00:39:20 05:54:05 left. \n",
      "epoch: 21 train_loss: 0.013772 00:41:19 05:52:13 left. \n",
      "epoch: 22 train_loss: 0.01376 00:43:18 05:50:21 left. \n",
      "epoch: 23 train_loss: 0.013749 00:45:16 05:48:27 left. \n",
      "epoch: 24 train_loss: 0.013744 00:47:15 05:46:36 left. \n",
      "epoch: 25 train_loss: 0.013733 00:49:14 05:44:42 left. \n",
      "epoch: 26 train_loss: 0.01373 00:51:14 05:42:52 left. \n",
      "epoch: 27 train_loss: 0.013715 00:53:14 05:41:06 left. \n",
      "epoch: 28 train_loss: 0.013702 00:55:09 05:38:49 left. \n",
      "epoch: 29 train_loss: 0.013708 00:57:06 05:36:47 left. \n",
      "epoch: 30 train_loss: 0.01369 00:59:08 05:35:08 left. \n",
      "epoch: 31 train_loss: 0.013679 01:01:07 05:33:12 left. \n",
      "epoch: 32 train_loss: 0.013672 01:03:06 05:31:20 left. \n",
      "epoch: 33 train_loss: 0.013666 01:05:08 05:29:37 left. \n",
      "epoch: 34 train_loss: 0.013653 01:07:08 05:27:47 left. \n",
      "epoch: 35 train_loss: 0.013644 01:09:05 05:25:45 left. \n",
      "epoch: 36 train_loss: 0.013639 01:11:04 05:23:47 left. \n",
      "epoch: 37 train_loss: 0.013632 01:13:03 05:21:50 left. \n",
      "epoch: 38 train_loss: 0.01363 01:15:01 05:19:50 left. \n",
      "epoch: 39 train_loss: 0.013622 01:17:01 05:18:00 left. \n",
      "epoch: 40 train_loss: 0.013608 01:19:02 05:16:09 left. \n",
      "epoch: 41 train_loss: 0.013607 01:21:00 05:14:08 left. \n",
      "epoch: 42 train_loss: 0.013596 01:22:58 05:12:07 left. \n",
      "epoch: 43 train_loss: 0.013588 01:24:57 05:10:12 left. \n",
      "epoch: 44 train_loss: 0.013589 01:26:56 05:08:15 left. \n",
      "epoch: 45 train_loss: 0.013577 01:28:55 05:06:19 left. \n",
      "epoch: 46 train_loss: 0.01357 01:30:51 05:04:09 left. \n",
      "epoch: 47 train_loss: 0.013564 01:32:47 05:02:05 left. \n",
      "epoch: 48 train_loss: 0.013562 01:34:45 05:00:05 left. \n",
      "epoch: 49 train_loss: 0.013556 01:36:44 04:58:06 left. \n",
      "epoch: 50 train_loss: 0.013554 01:38:43 04:56:09 left. \n",
      "epoch: 51 train_loss: 0.013553 01:40:38 04:54:02 left. \n",
      "epoch: 52 train_loss: 0.013539 01:42:36 04:52:03 left. \n",
      "epoch: 53 train_loss: 0.013531 01:44:37 04:50:11 left. \n",
      "epoch: 54 train_loss: 0.013527 01:46:36 04:48:13 left. \n",
      "epoch: 55 train_loss: 0.013513 01:48:32 04:46:08 left. \n",
      "epoch: 56 train_loss: 0.013523 01:50:32 04:44:13 left. \n",
      "epoch: 57 train_loss: 0.013505 01:52:30 04:42:15 left. \n",
      "epoch: 58 train_loss: 0.013504 01:54:30 04:40:19 left. \n",
      "epoch: 59 train_loss: 0.0135 01:56:28 04:38:21 left. \n",
      "epoch: 60 train_loss: 0.013492 01:58:26 04:36:20 left. \n",
      "epoch: 61 train_loss: 0.013484 02:00:24 04:34:22 left. \n",
      "epoch: 62 train_loss: 0.013478 02:02:22 04:32:23 left. \n",
      "epoch: 63 train_loss: 0.013473 02:04:20 04:30:22 left. \n",
      "epoch: 64 train_loss: 0.013461 02:06:17 04:28:22 left. \n",
      "epoch: 65 train_loss: 0.01346 02:08:16 04:26:25 left. \n",
      "epoch: 66 train_loss: 0.013448 02:10:17 04:24:31 left. \n",
      "epoch: 67 train_loss: 0.013446 02:12:16 04:22:33 left. \n",
      "epoch: 68 train_loss: 0.013437 02:14:15 04:20:36 left. \n",
      "epoch: 69 train_loss: 0.013438 02:16:15 04:18:42 left. \n",
      "epoch: 70 train_loss: 0.013427 02:18:14 04:16:44 left. \n",
      "epoch: 71 train_loss: 0.01343 02:20:15 04:14:50 left. \n",
      "epoch: 72 train_loss: 0.013419 02:22:14 04:12:51 left. \n",
      "epoch: 73 train_loss: 0.013407 02:24:11 04:10:51 left. \n",
      "epoch: 74 train_loss: 0.013401 02:26:10 04:08:53 left. \n",
      "epoch: 75 train_loss: 0.013391 02:28:10 04:06:56 left. \n",
      "epoch: 76 train_loss: 0.013387 02:30:10 04:05:00 left. \n",
      "epoch: 77 train_loss: 0.013384 02:32:10 04:03:04 left. \n",
      "epoch: 78 train_loss: 0.013381 02:34:09 04:01:06 left. \n",
      "epoch: 79 train_loss: 0.013374 02:36:08 03:59:09 left. \n",
      "epoch: 80 train_loss: 0.013361 02:38:07 03:57:11 left. \n",
      "epoch: 81 train_loss: 0.013357 02:40:07 03:55:14 left. \n",
      "epoch: 82 train_loss: 0.013358 02:42:08 03:53:20 left. \n",
      "epoch: 83 train_loss: 0.01334 02:44:06 03:51:20 left. \n",
      "epoch: 84 train_loss: 0.013332 02:46:03 03:49:19 left. \n",
      "epoch: 85 train_loss: 0.01333 02:48:01 03:47:20 left. \n",
      "epoch: 86 train_loss: 0.013326 02:49:56 03:45:16 left. \n",
      "epoch: 87 train_loss: 0.013315 02:51:54 03:43:16 left. \n",
      "epoch: 88 train_loss: 0.013308 02:53:52 03:41:17 left. \n",
      "epoch: 89 train_loss: 0.013306 02:55:49 03:39:17 left. \n",
      "epoch: 90 train_loss: 0.013295 02:57:48 03:37:19 left. \n",
      "epoch: 91 train_loss: 0.01329 02:59:48 03:35:22 left. \n",
      "epoch: 92 train_loss: 0.013282 03:01:46 03:33:23 left. \n",
      "epoch: 93 train_loss: 0.013273 03:03:45 03:31:25 left. \n",
      "epoch: 94 train_loss: 0.013281 03:05:44 03:29:27 left. \n",
      "epoch: 95 train_loss: 0.01326 03:07:44 03:27:30 left. \n",
      "epoch: 96 train_loss: 0.013255 03:09:42 03:25:30 left. \n",
      "epoch: 97 train_loss: 0.013254 03:11:40 03:23:31 left. \n",
      "epoch: 98 train_loss: 0.013242 03:13:38 03:21:33 left. \n",
      "epoch: 99 train_loss: 0.013236 03:15:37 03:19:34 left. \n",
      "epoch: 100 train_loss: 0.013225 03:17:33 03:17:33 left. \n",
      "epoch: 101 train_loss: 0.013223 03:19:31 03:15:34 left. \n",
      "epoch: 102 train_loss: 0.013221 03:21:29 03:13:35 left. \n",
      "epoch: 103 train_loss: 0.013207 03:23:27 03:11:36 left. \n",
      "epoch: 104 train_loss: 0.013203 03:25:24 03:09:36 left. \n",
      "epoch: 105 train_loss: 0.013192 03:27:21 03:07:36 left. \n",
      "epoch: 106 train_loss: 0.01319 03:29:20 03:05:38 left. \n",
      "epoch: 107 train_loss: 0.013174 03:31:17 03:03:38 left. \n",
      "epoch: 108 train_loss: 0.013164 03:33:15 03:01:40 left. \n",
      "epoch: 109 train_loss: 0.013159 03:35:14 02:59:41 left. \n",
      "epoch: 110 train_loss: 0.01316 03:37:13 02:57:43 left. \n",
      "epoch: 111 train_loss: 0.013149 03:39:13 02:55:46 left. \n",
      "epoch: 112 train_loss: 0.013144 03:41:11 02:53:47 left. \n",
      "epoch: 113 train_loss: 0.01314 03:43:08 02:51:48 left. \n",
      "epoch: 114 train_loss: 0.013125 03:45:07 02:49:49 left. \n",
      "epoch: 115 train_loss: 0.013115 03:47:04 02:47:50 left. \n",
      "epoch: 116 train_loss: 0.013111 03:49:05 02:45:53 left. \n",
      "epoch: 117 train_loss: 0.013103 03:51:03 02:43:54 left. \n",
      "epoch: 118 train_loss: 0.013089 03:53:01 02:41:55 left. \n",
      "epoch: 119 train_loss: 0.013087 03:54:59 02:39:57 left. \n",
      "epoch: 120 train_loss: 0.013084 03:56:58 02:37:59 left. \n",
      "epoch: 121 train_loss: 0.013073 03:58:56 02:36:00 left. \n",
      "epoch: 122 train_loss: 0.013064 04:00:54 02:34:01 left. \n",
      "epoch: 123 train_loss: 0.013058 04:02:53 02:32:03 left. \n",
      "epoch: 124 train_loss: 0.013046 04:04:52 02:30:05 left. \n",
      "epoch: 125 train_loss: 0.013041 04:06:52 02:28:07 left. \n",
      "epoch: 126 train_loss: 0.013037 04:08:50 02:26:08 left. \n",
      "epoch: 127 train_loss: 0.013031 04:10:50 02:24:11 left. \n",
      "epoch: 128 train_loss: 0.01302 04:12:49 02:22:12 left. \n",
      "epoch: 129 train_loss: 0.013014 04:14:49 02:20:14 left. \n",
      "epoch: 130 train_loss: 0.013002 04:16:48 02:18:16 left. \n",
      "epoch: 131 train_loss: 0.012992 04:18:47 02:16:18 left. \n",
      "epoch: 132 train_loss: 0.012989 04:20:49 02:14:21 left. \n",
      "epoch: 133 train_loss: 0.012981 04:22:48 02:12:23 left. \n",
      "epoch: 134 train_loss: 0.012974 04:24:48 02:10:25 left. \n",
      "epoch: 135 train_loss: 0.012967 04:26:47 02:08:27 left. \n",
      "epoch: 136 train_loss: 0.012955 04:28:49 02:06:30 left. \n",
      "epoch: 137 train_loss: 0.012947 04:30:48 02:04:31 left. \n",
      "epoch: 138 train_loss: 0.012943 04:32:47 02:02:33 left. \n",
      "epoch: 139 train_loss: 0.012932 04:34:47 02:00:35 left. \n",
      "epoch: 140 train_loss: 0.012926 04:36:46 01:58:37 left. \n",
      "epoch: 141 train_loss: 0.012918 04:38:46 01:56:39 left. \n",
      "epoch: 142 train_loss: 0.012907 04:40:45 01:54:40 left. \n",
      "epoch: 143 train_loss: 0.012904 04:42:45 01:52:42 left. \n",
      "epoch: 144 train_loss: 0.012892 04:44:43 01:50:43 left. \n",
      "epoch: 145 train_loss: 0.012889 04:46:42 01:48:45 left. \n",
      "epoch: 146 train_loss: 0.012873 04:48:42 01:46:46 left. \n",
      "epoch: 147 train_loss: 0.01287 04:50:41 01:44:48 left. \n",
      "epoch: 148 train_loss: 0.012859 04:52:40 01:42:49 left. \n",
      "epoch: 149 train_loss: 0.012854 04:54:39 01:40:51 left. \n",
      "epoch: 150 train_loss: 0.012843 04:56:30 01:38:50 left. \n",
      "epoch: 151 train_loss: 0.012838 04:58:29 01:36:51 left. \n",
      "epoch: 152 train_loss: 0.012828 05:00:26 01:34:52 left. \n",
      "epoch: 153 train_loss: 0.012821 05:02:25 01:32:53 left. \n",
      "epoch: 154 train_loss: 0.012812 05:04:22 01:30:55 left. \n",
      "epoch: 155 train_loss: 0.012797 05:06:20 01:28:56 left. \n",
      "epoch: 156 train_loss: 0.012798 05:08:18 01:26:57 left. \n",
      "epoch: 157 train_loss: 0.012782 05:10:17 01:24:58 left. \n",
      "epoch: 158 train_loss: 0.012777 05:12:17 01:23:00 left. \n",
      "epoch: 159 train_loss: 0.012769 05:14:17 01:21:02 left. \n",
      "epoch: 160 train_loss: 0.012763 05:16:15 01:19:03 left. \n",
      "epoch: 161 train_loss: 0.012754 05:18:14 01:17:05 left. \n",
      "epoch: 162 train_loss: 0.012745 05:20:15 01:15:07 left. \n",
      "epoch: 163 train_loss: 0.012738 05:22:14 01:13:08 left. \n",
      "epoch: 164 train_loss: 0.012725 05:24:15 01:11:10 left. \n",
      "epoch: 165 train_loss: 0.012724 05:26:13 01:09:11 left. \n",
      "epoch: 166 train_loss: 0.012714 05:28:13 01:07:13 left. \n",
      "epoch: 167 train_loss: 0.012707 05:30:12 01:05:15 left. \n",
      "epoch: 168 train_loss: 0.012693 05:32:10 01:03:16 left. \n",
      "epoch: 169 train_loss: 0.012691 05:34:09 01:01:17 left. \n",
      "epoch: 170 train_loss: 0.01268 05:36:10 00:59:19 left. \n",
      "epoch: 171 train_loss: 0.012672 05:38:05 00:57:20 left. \n",
      "epoch: 172 train_loss: 0.012664 05:40:01 00:55:21 left. \n",
      "epoch: 173 train_loss: 0.012655 05:41:59 00:53:22 left. \n",
      "epoch: 174 train_loss: 0.012647 05:43:58 00:51:23 left. \n",
      "epoch: 175 train_loss: 0.01264 05:45:52 00:49:24 left. \n",
      "epoch: 176 train_loss: 0.012633 05:47:51 00:47:26 left. \n",
      "epoch: 177 train_loss: 0.012628 05:49:50 00:45:27 left. \n",
      "epoch: 178 train_loss: 0.012621 05:51:50 00:43:29 left. \n",
      "epoch: 179 train_loss: 0.012611 05:53:48 00:41:30 left. \n",
      "epoch: 180 train_loss: 0.012606 05:55:46 00:39:31 left. \n",
      "epoch: 181 train_loss: 0.012599 05:57:46 00:37:33 left. \n",
      "epoch: 182 train_loss: 0.012597 05:59:44 00:35:34 left. \n",
      "epoch: 183 train_loss: 0.012586 06:01:40 00:33:35 left. \n",
      "epoch: 184 train_loss: 0.012582 06:03:40 00:31:37 left. \n",
      "epoch: 185 train_loss: 0.012579 06:05:39 00:29:38 left. \n",
      "epoch: 186 train_loss: 0.01257 06:07:38 00:27:40 left. \n",
      "epoch: 187 train_loss: 0.012565 06:09:36 00:25:41 left. \n",
      "epoch: 188 train_loss: 0.012565 06:11:35 00:23:43 left. \n",
      "epoch: 189 train_loss: 0.012555 06:13:34 00:21:44 left. \n",
      "epoch: 190 train_loss: 0.012556 06:15:32 00:19:45 left. \n",
      "epoch: 191 train_loss: 0.012548 06:17:33 00:17:47 left. \n",
      "epoch: 192 train_loss: 0.012547 06:19:31 00:15:48 left. \n",
      "epoch: 193 train_loss: 0.012544 06:21:30 00:13:50 left. \n",
      "epoch: 194 train_loss: 0.012542 06:23:27 00:11:51 left. \n",
      "epoch: 195 train_loss: 0.012536 06:25:27 00:09:53 left. \n",
      "epoch: 196 train_loss: 0.012536 06:27:25 00:07:54 left. \n",
      "epoch: 197 train_loss: 0.012533 06:29:24 00:05:55 left. \n",
      "epoch: 198 train_loss: 0.012532 06:31:24 00:03:57 left. \n",
      "epoch: 199 train_loss: 0.012531 06:33:23 00:01:58 left. \n",
      "epoch: 200 train_loss: 0.012533 06:35:22 00:00:00 left. \n",
      "Start SubNet warmup\n",
      "epoch: 1 train_loss: 0.01365 Test Acc.：246635/255000(96.72%),252526/255000(99.03%) 0.013043 00:03:01 10:03:33 left. \n",
      "epoch: 2 train_loss: 0.013127 Test Acc.：247141/255000(96.92%),253162/255000(99.28%) 0.012926 00:06:16 10:21:43 left. \n",
      "epoch: 3 train_loss: 0.01307 Test Acc.：246982/255000(96.86%),253021/255000(99.22%) 0.012975 00:09:25 10:19:26 left. \n",
      "epoch: 4 train_loss: 0.013025 Test Acc.：247099/255000(96.90%),253161/255000(99.28%) 0.012937 00:12:33 10:15:21 left. \n",
      "epoch: 5 train_loss: 0.013012 Test Acc.：247008/255000(96.87%),253052/255000(99.24%) 0.012941 00:15:39 10:10:59 left. \n",
      "epoch: 6 train_loss: 0.013016 Test Acc.：247172/255000(96.93%),253287/255000(99.33%) 0.012931 00:18:47 10:07:46 left. \n",
      "epoch: 7 train_loss: 0.013109 Test Acc.：247005/255000(96.86%),253010/255000(99.22%) 0.012955 00:21:57 10:05:21 left. \n",
      "epoch: 8 train_loss: 0.01301 Test Acc.：247003/255000(96.86%),252977/255000(99.21%) 0.012952 00:25:16 10:06:40 left. \n",
      "epoch: 9 train_loss: 0.012982 Test Acc.：247228/255000(96.95%),253389/255000(99.37%) 0.012903 00:28:31 10:05:31 left. \n",
      "epoch: 10 train_loss: 0.012965 Test Acc.：247302/255000(96.98%),253474/255000(99.40%) 0.012873 00:31:44 10:03:08 left. \n",
      "epoch: 11 train_loss: 0.012961 Test Acc.：247008/255000(96.87%),253092/255000(99.25%) 0.012961 00:34:58 10:01:00 left. \n",
      "epoch: 12 train_loss: 0.012948 Test Acc.：247340/255000(97.00%),253457/255000(99.39%) 0.012892 00:38:06 09:57:05 left. \n",
      "epoch: 13 train_loss: 0.012947 Test Acc.：247212/255000(96.95%),253252/255000(99.31%) 0.012896 00:41:26 09:56:07 left. \n",
      "epoch: 14 train_loss: 0.012934 Test Acc.：246676/255000(96.74%),252527/255000(99.03%) 0.013028 00:44:42 09:53:54 left. \n",
      "epoch: 15 train_loss: 0.012943 Test Acc.：247061/255000(96.89%),252994/255000(99.21%) 0.012916 00:47:52 09:50:33 left. \n",
      "epoch: 16 train_loss: 0.012953 Test Acc.：247350/255000(97.00%),253564/255000(99.44%) 0.012881 00:51:02 09:46:55 left. \n",
      "epoch: 17 train_loss: 0.012924 Test Acc.：247051/255000(96.88%),253174/255000(99.28%) 0.012931 00:54:12 09:43:31 left. \n",
      "epoch: 18 train_loss: 0.012936 Test Acc.：247531/255000(97.07%),253622/255000(99.46%) 0.012823 00:57:21 09:39:53 left. \n",
      "epoch: 19 train_loss: 0.012926 Test Acc.：247337/255000(96.99%),253408/255000(99.38%) 0.012888 01:00:30 09:36:23 left. \n",
      "epoch: 20 train_loss: 0.012925 Test Acc.：247454/255000(97.04%),253607/255000(99.45%) 0.01285 01:03:38 09:32:45 left. \n",
      "epoch: 21 train_loss: 0.012934 Test Acc.：247299/255000(96.98%),253353/255000(99.35%) 0.012883 01:06:56 09:30:35 left. \n",
      "epoch: 22 train_loss: 0.012927 Test Acc.：247315/255000(96.99%),253466/255000(99.40%) 0.012884 01:10:07 09:27:24 left. \n",
      "epoch: 23 train_loss: 0.01292 Test Acc.：247503/255000(97.06%),253655/255000(99.47%) 0.012836 01:13:12 09:23:23 left. \n",
      "epoch: 24 train_loss: 0.012914 Test Acc.：247489/255000(97.05%),253594/255000(99.45%) 0.01283 01:16:23 09:20:09 left. \n",
      "epoch: 25 train_loss: 0.012921 Test Acc.：247478/255000(97.05%),253641/255000(99.47%) 0.012865 01:19:34 09:17:02 left. \n",
      "epoch: 26 train_loss: 0.01294 Test Acc.：247300/255000(96.98%),253389/255000(99.37%) 0.012913 01:22:45 09:13:52 left. \n",
      "epoch: 27 train_loss: 0.012903 Test Acc.：247155/255000(96.92%),253099/255000(99.25%) 0.012897 01:25:56 09:10:39 left. \n",
      "epoch: 28 train_loss: 0.012911 Test Acc.：247386/255000(97.01%),253532/255000(99.42%) 0.012853 01:29:03 09:07:05 left. \n",
      "epoch: 29 train_loss: 0.012979 Test Acc.：247236/255000(96.96%),253253/255000(99.31%) 0.012898 01:32:13 09:03:46 left. \n",
      "epoch: 30 train_loss: 0.012935 Test Acc.：247511/255000(97.06%),253627/255000(99.46%) 0.012824 01:35:19 09:00:09 left. \n",
      "epoch: 31 train_loss: 0.012947 Test Acc.：247216/255000(96.95%),253243/255000(99.31%) 0.012884 01:38:30 08:57:02 left. \n",
      "epoch: 32 train_loss: 0.01292 Test Acc.：247473/255000(97.05%),253636/255000(99.47%) 0.012836 01:41:39 08:53:42 left. \n",
      "epoch: 33 train_loss: 0.012919 Test Acc.：247462/255000(97.04%),253592/255000(99.45%) 0.012848 01:44:53 08:50:51 left. \n",
      "epoch: 34 train_loss: 0.0129 Test Acc.：247449/255000(97.04%),253501/255000(99.41%) 0.012831 01:48:05 08:47:43 left. \n",
      "epoch: 35 train_loss: 0.012912 Test Acc.：247304/255000(96.98%),253416/255000(99.38%) 0.012904 01:51:14 08:44:24 left. \n",
      "epoch: 36 train_loss: 0.012895 Test Acc.：247492/255000(97.06%),253634/255000(99.46%) 0.012818 01:54:31 08:41:43 left. \n",
      "epoch: 37 train_loss: 0.012905 Test Acc.：247238/255000(96.96%),253351/255000(99.35%) 0.012893 01:57:49 08:39:05 left. \n",
      "epoch: 38 train_loss: 0.012905 Test Acc.：247342/255000(97.00%),253451/255000(99.39%) 0.012888 02:01:01 08:35:55 left. \n",
      "epoch: 39 train_loss: 0.01289 Test Acc.：247404/255000(97.02%),253544/255000(99.43%) 0.012857 02:04:11 08:32:39 left. \n",
      "epoch: 40 train_loss: 0.012904 Test Acc.：244690/255000(95.96%),250177/255000(98.11%) 0.0134 02:07:17 08:29:10 left. \n",
      "epoch: 41 train_loss: 0.012957 Test Acc.：247391/255000(97.02%),253505/255000(99.41%) 0.01287 02:10:30 08:26:06 left. \n",
      "epoch: 42 train_loss: 0.012901 Test Acc.：247537/255000(97.07%),253671/255000(99.48%) 0.012831 02:13:38 08:22:45 left. \n",
      "epoch: 43 train_loss: 0.012934 Test Acc.：247150/255000(96.92%),253199/255000(99.29%) 0.012918 02:16:46 08:19:23 left. \n",
      "epoch: 44 train_loss: 0.012914 Test Acc.：247390/255000(97.02%),253489/255000(99.41%) 0.012872 02:19:59 08:16:20 left. \n",
      "epoch: 45 train_loss: 0.012916 Test Acc.：247424/255000(97.03%),253581/255000(99.44%) 0.012867 02:23:12 08:13:18 left. \n",
      "epoch: 46 train_loss: 0.012887 Test Acc.：247460/255000(97.04%),253622/255000(99.46%) 0.01285 02:26:17 08:09:45 left. \n",
      "epoch: 47 train_loss: 0.012876 Test Acc.：247545/255000(97.08%),253683/255000(99.48%) 0.012831 02:29:27 08:06:30 left. \n",
      "epoch: 48 train_loss: 0.012885 Test Acc.：247465/255000(97.05%),253497/255000(99.41%) 0.012819 02:32:39 08:03:24 left. \n",
      "epoch: 49 train_loss: 0.012904 Test Acc.：247547/255000(97.08%),253696/255000(99.49%) 0.012818 02:35:49 08:00:10 left. \n",
      "epoch: 50 train_loss: 0.012908 Test Acc.：247396/255000(97.02%),253491/255000(99.41%) 0.012873 02:38:57 07:56:53 left. \n",
      "epoch: 51 train_loss: 0.012886 Test Acc.：247586/255000(97.09%),253742/255000(99.51%) 0.012814 02:42:01 07:53:21 left. \n",
      "epoch: 52 train_loss: 0.012892 Test Acc.：247563/255000(97.08%),253716/255000(99.50%) 0.01282 02:45:10 07:50:07 left. \n",
      "epoch: 53 train_loss: 0.012909 Test Acc.：246868/255000(96.81%),252805/255000(99.14%) 0.012946 02:48:18 07:46:48 left. \n",
      "epoch: 54 train_loss: 0.012892 Test Acc.：247308/255000(96.98%),253434/255000(99.39%) 0.012878 02:51:37 07:44:01 left. \n",
      "epoch: 55 train_loss: 0.012867 Test Acc.：247572/255000(97.09%),253724/255000(99.50%) 0.012823 02:54:54 07:41:07 left. \n",
      "epoch: 56 train_loss: 0.012887 Test Acc.：247403/255000(97.02%),253538/255000(99.43%) 0.012842 02:58:06 07:37:58 left. \n",
      "epoch: 57 train_loss: 0.012862 Test Acc.：247515/255000(97.06%),253575/255000(99.44%) 0.012829 03:01:13 07:34:39 left. \n",
      "epoch: 58 train_loss: 0.012868 Test Acc.：247210/255000(96.95%),253373/255000(99.36%) 0.012889 03:04:32 07:31:49 left. \n",
      "epoch: 59 train_loss: 0.012871 Test Acc.：247605/255000(97.10%),253760/255000(99.51%) 0.012797 03:07:44 07:28:40 left. \n",
      "epoch: 60 train_loss: 0.012886 Test Acc.：247132/255000(96.91%),253302/255000(99.33%) 0.012887 03:10:53 07:25:24 left. \n",
      "epoch: 61 train_loss: 0.012877 Test Acc.：247454/255000(97.04%),253657/255000(99.47%) 0.012853 03:14:01 07:22:06 left. \n",
      "epoch: 62 train_loss: 0.01288 Test Acc.：247545/255000(97.08%),253664/255000(99.48%) 0.012819 03:17:13 07:19:00 left. \n",
      "epoch: 63 train_loss: 0.012897 Test Acc.：247391/255000(97.02%),253501/255000(99.41%) 0.012891 03:20:25 07:15:50 left. \n",
      "epoch: 64 train_loss: 0.01287 Test Acc.：246959/255000(96.85%),253016/255000(99.22%) 0.012985 03:23:34 07:12:36 left. \n",
      "epoch: 65 train_loss: 0.012862 Test Acc.：247549/255000(97.08%),253645/255000(99.47%) 0.012823 03:26:45 07:09:24 left. \n",
      "epoch: 66 train_loss: 0.012876 Test Acc.：247563/255000(97.08%),253690/255000(99.49%) 0.012829 03:30:00 07:06:22 left. \n",
      "epoch: 67 train_loss: 0.012867 Test Acc.：247280/255000(96.97%),253375/255000(99.36%) 0.012857 03:33:08 07:03:06 left. \n",
      "epoch: 68 train_loss: 0.012849 Test Acc.：247448/255000(97.04%),253628/255000(99.46%) 0.01286 03:36:14 06:59:45 left. \n",
      "epoch: 69 train_loss: 0.012865 Test Acc.：247632/255000(97.11%),253809/255000(99.53%) 0.012793 03:39:24 06:56:33 left. \n",
      "epoch: 70 train_loss: 0.012857 Test Acc.：246710/255000(96.75%),252792/255000(99.13%) 0.012986 03:42:32 06:53:18 left. \n",
      "epoch: 71 train_loss: 0.012854 Test Acc.：247606/255000(97.10%),253745/255000(99.51%) 0.012807 03:45:40 06:50:01 left. \n",
      "epoch: 72 train_loss: 0.012855 Test Acc.：247565/255000(97.08%),253741/255000(99.51%) 0.01282 03:48:50 06:46:49 left. \n",
      "epoch: 73 train_loss: 0.012858 Test Acc.：247541/255000(97.07%),253678/255000(99.48%) 0.012825 03:51:57 06:43:32 left. \n",
      "epoch: 74 train_loss: 0.012848 Test Acc.：247617/255000(97.10%),253808/255000(99.53%) 0.012791 03:55:04 06:40:16 left. \n",
      "epoch: 75 train_loss: 0.012873 Test Acc.：247330/255000(96.99%),253200/255000(99.29%) 0.012869 03:58:15 06:37:06 left. \n",
      "epoch: 76 train_loss: 0.012836 Test Acc.：247641/255000(97.11%),253835/255000(99.54%) 0.0128 04:01:23 06:33:51 left. \n",
      "epoch: 77 train_loss: 0.012848 Test Acc.：247627/255000(97.11%),253761/255000(99.51%) 0.012815 04:04:29 06:30:32 left. \n",
      "epoch: 78 train_loss: 0.012839 Test Acc.：247673/255000(97.13%),253831/255000(99.54%) 0.012793 04:07:46 06:27:32 left. \n",
      "epoch: 79 train_loss: 0.012849 Test Acc.：247229/255000(96.95%),253362/255000(99.36%) 0.012868 04:10:51 06:24:14 left. \n",
      "epoch: 80 train_loss: 0.012869 Test Acc.：247582/255000(97.09%),253777/255000(99.52%) 0.012811 04:14:11 06:21:17 left. \n",
      "epoch: 81 train_loss: 0.01284 Test Acc.：247643/255000(97.11%),253823/255000(99.54%) 0.0128 04:17:15 06:17:56 left. \n",
      "epoch: 82 train_loss: 0.012834 Test Acc.：247667/255000(97.12%),253830/255000(99.54%) 0.012805 04:20:31 06:14:54 left. \n",
      "epoch: 83 train_loss: 0.012831 Test Acc.：247663/255000(97.12%),253763/255000(99.51%) 0.012786 04:23:51 06:11:56 left. \n",
      "epoch: 84 train_loss: 0.012866 Test Acc.：247496/255000(97.06%),253593/255000(99.45%) 0.012822 04:27:03 06:08:47 left. \n",
      "epoch: 85 train_loss: 0.012834 Test Acc.：247503/255000(97.06%),253615/255000(99.46%) 0.012846 04:30:13 06:05:35 left. \n",
      "epoch: 86 train_loss: 0.012814 Test Acc.：247707/255000(97.14%),253884/255000(99.56%) 0.012775 04:33:24 06:02:25 left. \n",
      "epoch: 87 train_loss: 0.01283 Test Acc.：247622/255000(97.11%),253799/255000(99.53%) 0.012818 04:36:29 05:59:06 left. \n",
      "epoch: 88 train_loss: 0.012847 Test Acc.：247657/255000(97.12%),253826/255000(99.54%) 0.012814 04:39:40 05:55:56 left. \n",
      "epoch: 89 train_loss: 0.012825 Test Acc.：247555/255000(97.08%),253660/255000(99.47%) 0.01281 04:42:53 05:52:49 left. \n",
      "epoch: 90 train_loss: 0.012822 Test Acc.：247348/255000(97.00%),253427/255000(99.38%) 0.012875 04:46:00 05:49:33 left. \n",
      "epoch: 91 train_loss: 0.012826 Test Acc.：247689/255000(97.13%),253884/255000(99.56%) 0.012788 04:49:06 05:46:17 left. \n",
      "epoch: 92 train_loss: 0.012828 Test Acc.：247611/255000(97.10%),253823/255000(99.54%) 0.012802 04:52:12 05:43:02 left. \n",
      "epoch: 93 train_loss: 0.012824 Test Acc.：247567/255000(97.09%),253717/255000(99.50%) 0.012826 04:55:18 05:39:45 left. \n",
      "epoch: 94 train_loss: 0.012819 Test Acc.：247541/255000(97.07%),253593/255000(99.45%) 0.012822 04:58:23 05:36:29 left. \n",
      "epoch: 95 train_loss: 0.01282 Test Acc.：247515/255000(97.06%),253668/255000(99.48%) 0.012841 05:01:38 05:33:23 left. \n",
      "epoch: 96 train_loss: 0.012816 Test Acc.：247624/255000(97.11%),253839/255000(99.54%) 0.012792 05:04:51 05:30:15 left. \n",
      "epoch: 97 train_loss: 0.012814 Test Acc.：247488/255000(97.05%),253597/255000(99.45%) 0.01284 05:08:00 05:27:03 left. \n",
      "epoch: 98 train_loss: 0.012836 Test Acc.：247706/255000(97.14%),253868/255000(99.56%) 0.012795 05:11:09 05:23:51 left. \n",
      "epoch: 99 train_loss: 0.012814 Test Acc.：247735/255000(97.15%),253910/255000(99.57%) 0.012778 05:14:17 05:20:38 left. \n",
      "epoch: 100 train_loss: 0.012831 Test Acc.：247469/255000(97.05%),253600/255000(99.45%) 0.012839 05:17:24 05:17:24 left. \n",
      "epoch: 101 train_loss: 0.012813 Test Acc.：247656/255000(97.12%),253862/255000(99.55%) 0.012803 05:20:38 05:14:17 left. \n",
      "epoch: 102 train_loss: 0.012795 Test Acc.：247547/255000(97.08%),253720/255000(99.50%) 0.012815 05:23:47 05:11:05 left. \n",
      "epoch: 103 train_loss: 0.012791 Test Acc.：247805/255000(97.18%),253979/255000(99.60%) 0.012752 05:27:00 05:07:57 left. \n",
      "epoch: 104 train_loss: 0.012804 Test Acc.：247452/255000(97.04%),253463/255000(99.40%) 0.012829 05:30:07 05:04:43 left. \n",
      "epoch: 105 train_loss: 0.01279 Test Acc.：247618/255000(97.11%),253811/255000(99.53%) 0.012783 05:33:14 05:01:30 left. \n",
      "epoch: 106 train_loss: 0.012797 Test Acc.：247725/255000(97.15%),253949/255000(99.59%) 0.012784 05:36:21 04:58:17 left. \n",
      "epoch: 107 train_loss: 0.012785 Test Acc.：247828/255000(97.19%),253981/255000(99.60%) 0.012761 05:39:33 04:55:07 left. \n",
      "epoch: 108 train_loss: 0.012785 Test Acc.：247655/255000(97.12%),253847/255000(99.55%) 0.012791 05:42:52 04:52:04 left. \n",
      "epoch: 109 train_loss: 0.012778 Test Acc.：247754/255000(97.16%),253969/255000(99.60%) 0.012776 05:46:10 04:49:00 left. \n",
      "epoch: 110 train_loss: 0.012782 Test Acc.：247663/255000(97.12%),253831/255000(99.54%) 0.012779 05:49:20 04:45:49 left. \n",
      "epoch: 111 train_loss: 0.012779 Test Acc.：247716/255000(97.14%),253928/255000(99.58%) 0.012786 05:52:28 04:42:37 left. \n",
      "epoch: 112 train_loss: 0.01277 Test Acc.：247795/255000(97.17%),254023/255000(99.62%) 0.012754 05:55:45 04:39:31 left. \n",
      "epoch: 113 train_loss: 0.01277 Test Acc.：247798/255000(97.18%),253975/255000(99.60%) 0.012757 05:58:52 04:36:17 left. \n",
      "epoch: 114 train_loss: 0.012768 Test Acc.：247757/255000(97.16%),253875/255000(99.56%) 0.012766 06:01:57 04:33:03 left. \n",
      "epoch: 115 train_loss: 0.012776 Test Acc.：247747/255000(97.16%),253902/255000(99.57%) 0.01277 06:05:06 04:29:51 left. \n",
      "epoch: 116 train_loss: 0.01277 Test Acc.：247818/255000(97.18%),254031/255000(99.62%) 0.012745 06:08:16 04:26:40 left. \n",
      "epoch: 117 train_loss: 0.012763 Test Acc.：247840/255000(97.19%),254047/255000(99.63%) 0.012745 06:11:23 04:23:28 left. \n",
      "epoch: 118 train_loss: 0.012757 Test Acc.：247797/255000(97.18%),254011/255000(99.61%) 0.012769 06:14:31 04:20:15 left. \n",
      "epoch: 119 train_loss: 0.012755 Test Acc.：247815/255000(97.18%),253982/255000(99.60%) 0.012745 06:17:42 04:17:05 left. \n",
      "epoch: 120 train_loss: 0.012761 Test Acc.：247764/255000(97.16%),253972/255000(99.60%) 0.012761 06:20:51 04:13:54 left. \n",
      "epoch: 121 train_loss: 0.012764 Test Acc.：247622/255000(97.11%),253819/255000(99.54%) 0.012801 06:23:59 04:10:42 left. \n",
      "epoch: 122 train_loss: 0.012757 Test Acc.：247682/255000(97.13%),253840/255000(99.55%) 0.012789 06:27:05 04:07:29 left. \n",
      "epoch: 123 train_loss: 0.012746 Test Acc.：247832/255000(97.19%),254008/255000(99.61%) 0.012764 06:30:21 04:04:22 left. \n",
      "epoch: 124 train_loss: 0.01275 Test Acc.：247886/255000(97.21%),254067/255000(99.63%) 0.012748 06:33:28 04:01:09 left. \n",
      "epoch: 125 train_loss: 0.012741 Test Acc.：247793/255000(97.17%),253944/255000(99.59%) 0.012738 06:36:41 03:58:00 left. \n",
      "epoch: 126 train_loss: 0.01274 Test Acc.：247711/255000(97.14%),253870/255000(99.56%) 0.012765 06:39:57 03:54:54 left. \n",
      "epoch: 127 train_loss: 0.01274 Test Acc.：247850/255000(97.20%),254041/255000(99.62%) 0.012754 06:43:07 03:51:43 left. \n",
      "epoch: 128 train_loss: 0.012737 Test Acc.：247770/255000(97.16%),253897/255000(99.57%) 0.012753 06:46:19 03:48:33 left. \n",
      "epoch: 129 train_loss: 0.012732 Test Acc.：247879/255000(97.21%),254051/255000(99.63%) 0.012739 06:49:30 03:45:23 left. \n",
      "epoch: 130 train_loss: 0.01273 Test Acc.：247875/255000(97.21%),254059/255000(99.63%) 0.012745 06:52:52 03:42:18 left. \n",
      "epoch: 131 train_loss: 0.012731 Test Acc.：247819/255000(97.18%),254014/255000(99.61%) 0.012738 06:55:58 03:39:06 left. \n",
      "epoch: 132 train_loss: 0.012738 Test Acc.：247809/255000(97.18%),254012/255000(99.61%) 0.012745 06:59:11 03:35:56 left. \n",
      "epoch: 133 train_loss: 0.012728 Test Acc.：247733/255000(97.15%),253787/255000(99.52%) 0.01276 07:02:16 03:32:43 left. \n",
      "epoch: 134 train_loss: 0.01272 Test Acc.：247847/255000(97.19%),254070/255000(99.64%) 0.01274 07:05:27 03:29:33 left. \n",
      "epoch: 135 train_loss: 0.01272 Test Acc.：247872/255000(97.20%),254097/255000(99.65%) 0.012731 07:08:35 03:26:21 left. \n",
      "epoch: 136 train_loss: 0.012724 Test Acc.：247841/255000(97.19%),254026/255000(99.62%) 0.012754 07:11:44 03:23:10 left. \n",
      "epoch: 137 train_loss: 0.012793 Test Acc.：247750/255000(97.16%),253913/255000(99.57%) 0.012774 07:14:50 03:19:57 left. \n",
      "epoch: 138 train_loss: 0.012743 Test Acc.：247852/255000(97.20%),254026/255000(99.62%) 0.012743 07:18:10 03:16:51 left. \n",
      "epoch: 139 train_loss: 0.01273 Test Acc.：247650/255000(97.12%),253645/255000(99.47%) 0.012775 07:21:23 03:13:42 left. \n",
      "epoch: 140 train_loss: 0.012713 Test Acc.：247913/255000(97.22%),254120/255000(99.65%) 0.012726 07:24:32 03:10:31 left. \n",
      "epoch: 141 train_loss: 0.012706 Test Acc.：247930/255000(97.23%),254103/255000(99.65%) 0.012726 07:27:43 03:07:20 left. \n",
      "epoch: 142 train_loss: 0.012709 Test Acc.：247989/255000(97.25%),254193/255000(99.68%) 0.012714 07:30:57 03:04:11 left. \n",
      "epoch: 143 train_loss: 0.012702 Test Acc.：247965/255000(97.24%),254195/255000(99.68%) 0.012722 07:34:02 03:00:59 left. \n",
      "epoch: 144 train_loss: 0.012693 Test Acc.：247927/255000(97.23%),254154/255000(99.67%) 0.012718 07:37:14 02:57:48 left. \n",
      "epoch: 145 train_loss: 0.012695 Test Acc.：247952/255000(97.24%),254146/255000(99.67%) 0.012717 07:40:20 02:54:36 left. \n",
      "epoch: 146 train_loss: 0.012691 Test Acc.：247898/255000(97.21%),254036/255000(99.62%) 0.012721 07:43:33 02:51:27 left. \n",
      "epoch: 147 train_loss: 0.012691 Test Acc.：247949/255000(97.23%),254128/255000(99.66%) 0.012709 07:46:47 02:48:18 left. \n",
      "epoch: 148 train_loss: 0.01269 Test Acc.：247972/255000(97.24%),254211/255000(99.69%) 0.012699 07:49:54 02:45:06 left. \n",
      "epoch: 149 train_loss: 0.012685 Test Acc.：247918/255000(97.22%),254142/255000(99.66%) 0.01272 07:53:08 02:41:56 left. \n",
      "epoch: 150 train_loss: 0.012686 Test Acc.：247924/255000(97.23%),254132/255000(99.66%) 0.012716 07:56:17 02:38:45 left. \n",
      "epoch: 151 train_loss: 0.012678 Test Acc.：247905/255000(97.22%),254082/255000(99.64%) 0.012723 07:59:27 02:35:35 left. \n",
      "epoch: 152 train_loss: 0.012682 Test Acc.：247980/255000(97.25%),254204/255000(99.69%) 0.012714 08:02:34 02:32:23 left. \n",
      "epoch: 153 train_loss: 0.012678 Test Acc.：247960/255000(97.24%),254141/255000(99.66%) 0.012715 08:05:44 02:29:12 left. \n",
      "epoch: 154 train_loss: 0.012679 Test Acc.：247911/255000(97.22%),254162/255000(99.67%) 0.012737 08:09:01 02:26:04 left. \n",
      "epoch: 155 train_loss: 0.012672 Test Acc.：247905/255000(97.22%),254110/255000(99.65%) 0.01274 08:12:14 02:22:54 left. \n",
      "epoch: 156 train_loss: 0.012672 Test Acc.：247941/255000(97.23%),254161/255000(99.67%) 0.01272 08:15:25 02:19:44 left. \n",
      "epoch: 157 train_loss: 0.012672 Test Acc.：247966/255000(97.24%),254154/255000(99.67%) 0.01271 08:18:39 02:16:34 left. \n",
      "epoch: 158 train_loss: 0.012667 Test Acc.：247982/255000(97.25%),254205/255000(99.69%) 0.012716 08:22:09 02:13:28 left. \n",
      "epoch: 159 train_loss: 0.012665 Test Acc.：248024/255000(97.26%),254231/255000(99.70%) 0.012702 08:25:17 02:10:17 left. \n",
      "epoch: 160 train_loss: 0.012667 Test Acc.：247998/255000(97.25%),254218/255000(99.69%) 0.012724 08:28:25 02:07:06 left. \n",
      "epoch: 161 train_loss: 0.012656 Test Acc.：248026/255000(97.27%),254216/255000(99.69%) 0.012705 08:31:31 02:03:54 left. \n",
      "epoch: 162 train_loss: 0.012653 Test Acc.：247917/255000(97.22%),254163/255000(99.67%) 0.012727 08:34:40 02:00:43 left. \n",
      "epoch: 163 train_loss: 0.012653 Test Acc.：247994/255000(97.25%),254228/255000(99.70%) 0.012718 08:37:52 01:57:33 left. \n",
      "epoch: 164 train_loss: 0.012652 Test Acc.：248008/255000(97.26%),254243/255000(99.70%) 0.012708 08:41:10 01:54:24 left. \n",
      "epoch: 165 train_loss: 0.012639 Test Acc.：248023/255000(97.26%),254206/255000(99.69%) 0.012709 08:44:21 01:51:13 left. \n",
      "epoch: 166 train_loss: 0.012645 Test Acc.：247998/255000(97.25%),254202/255000(99.69%) 0.012701 08:47:34 01:48:03 left. \n",
      "epoch: 167 train_loss: 0.012647 Test Acc.：247957/255000(97.24%),254184/255000(99.68%) 0.012717 08:50:48 01:44:53 left. \n",
      "epoch: 168 train_loss: 0.012648 Test Acc.：247945/255000(97.23%),254229/255000(99.70%) 0.012715 08:53:56 01:41:42 left. \n",
      "epoch: 169 train_loss: 0.012644 Test Acc.：248053/255000(97.28%),254244/255000(99.70%) 0.012704 08:57:10 01:38:32 left. \n",
      "epoch: 170 train_loss: 0.012643 Test Acc.：248027/255000(97.27%),254258/255000(99.71%) 0.012708 09:00:16 01:35:20 left. \n",
      "epoch: 171 train_loss: 0.012641 Test Acc.：247935/255000(97.23%),254141/255000(99.66%) 0.012706 09:03:24 01:32:09 left. \n",
      "epoch: 172 train_loss: 0.012638 Test Acc.：248029/255000(97.27%),254229/255000(99.70%) 0.012705 09:06:33 01:28:58 left. \n",
      "epoch: 173 train_loss: 0.012635 Test Acc.：248021/255000(97.26%),254261/255000(99.71%) 0.012694 09:09:40 01:25:47 left. \n",
      "epoch: 174 train_loss: 0.012635 Test Acc.：248037/255000(97.27%),254242/255000(99.70%) 0.012704 09:12:50 01:22:36 left. \n",
      "epoch: 175 train_loss: 0.012636 Test Acc.：247953/255000(97.24%),254208/255000(99.69%) 0.012707 09:16:02 01:19:26 left. \n",
      "epoch: 176 train_loss: 0.01263 Test Acc.：248035/255000(97.27%),254239/255000(99.70%) 0.012698 09:19:17 01:16:16 left. \n",
      "epoch: 177 train_loss: 0.012628 Test Acc.：248015/255000(97.26%),254217/255000(99.69%) 0.012695 09:22:22 01:13:04 left. \n",
      "epoch: 178 train_loss: 0.012629 Test Acc.：248043/255000(97.27%),254256/255000(99.71%) 0.01269 09:25:33 01:09:54 left. \n",
      "epoch: 179 train_loss: 0.012625 Test Acc.：248021/255000(97.26%),254245/255000(99.70%) 0.0127 09:28:43 01:06:43 left. \n",
      "epoch: 180 train_loss: 0.012621 Test Acc.：248025/255000(97.26%),254200/255000(99.69%) 0.012704 09:32:03 01:03:33 left. \n",
      "epoch: 181 train_loss: 0.012622 Test Acc.：248031/255000(97.27%),254255/255000(99.71%) 0.012702 09:35:14 01:00:23 left. \n",
      "epoch: 182 train_loss: 0.012619 Test Acc.：248020/255000(97.26%),254251/255000(99.71%) 0.012696 09:38:27 00:57:12 left. \n",
      "epoch: 183 train_loss: 0.01262 Test Acc.：248053/255000(97.28%),254276/255000(99.72%) 0.012692 09:41:34 00:54:01 left. \n",
      "epoch: 184 train_loss: 0.012621 Test Acc.：248037/255000(97.27%),254253/255000(99.71%) 0.01269 09:44:41 00:50:50 left. \n",
      "epoch: 185 train_loss: 0.012614 Test Acc.：248020/255000(97.26%),254246/255000(99.70%) 0.012695 09:47:51 00:47:39 left. \n",
      "epoch: 186 train_loss: 0.012614 Test Acc.：248024/255000(97.26%),254281/255000(99.72%) 0.012697 09:50:57 00:44:28 left. \n",
      "epoch: 187 train_loss: 0.012617 Test Acc.：248050/255000(97.27%),254267/255000(99.71%) 0.012694 09:54:12 00:41:18 left. \n",
      "epoch: 188 train_loss: 0.012613 Test Acc.：248040/255000(97.27%),254248/255000(99.71%) 0.012701 09:57:25 00:38:07 left. \n",
      "epoch: 189 train_loss: 0.012617 Test Acc.：248025/255000(97.26%),254218/255000(99.69%) 0.0127 10:00:40 00:34:57 left. \n",
      "epoch: 190 train_loss: 0.012614 Test Acc.：248018/255000(97.26%),254268/255000(99.71%) 0.012706 10:03:46 00:31:46 left. \n",
      "epoch: 191 train_loss: 0.012609 Test Acc.：248038/255000(97.27%),254264/255000(99.71%) 0.012702 10:07:02 00:28:36 left. \n",
      "epoch: 192 train_loss: 0.012612 Test Acc.：248020/255000(97.26%),254257/255000(99.71%) 0.012692 10:10:10 00:25:25 left. \n",
      "epoch: 193 train_loss: 0.012609 Test Acc.：248026/255000(97.27%),254260/255000(99.71%) 0.012696 10:13:20 00:22:14 left. \n",
      "epoch: 194 train_loss: 0.012613 Test Acc.：248043/255000(97.27%),254266/255000(99.71%) 0.012693 10:16:25 00:19:03 left. \n",
      "epoch: 195 train_loss: 0.012611 Test Acc.：248052/255000(97.28%),254249/255000(99.71%) 0.012701 10:19:36 00:15:53 left. \n",
      "epoch: 196 train_loss: 0.012608 Test Acc.：248029/255000(97.27%),254267/255000(99.71%) 0.012696 10:22:48 00:12:42 left. \n",
      "epoch: 197 train_loss: 0.012611 Test Acc.：248011/255000(97.26%),254270/255000(99.71%) 0.012709 10:25:57 00:09:31 left. \n",
      "epoch: 198 train_loss: 0.012606 Test Acc.：248049/255000(97.27%),254262/255000(99.71%) 0.012688 10:29:03 00:06:21 left. \n",
      "epoch: 199 train_loss: 0.012611 Test Acc.：248035/255000(97.27%),254271/255000(99.71%) 0.012709 10:32:20 00:03:10 left. \n",
      "epoch: 200 train_loss: 0.01261 Test Acc.：248048/255000(97.27%),254269/255000(99.71%) 0.012694 10:35:28 00:00:00 left. \n",
      "Test Acc.：248048/255000(97.27%),254269/255000(99.71%) 0.012694 \n",
      "epoch: 1 train_loss: 0.014703 Test Acc.：242621/255000(95.15%),233586/255000(91.60%) 0.013872 Save model 00:03:14 10:45:20 left. \n",
      "epoch: 2 train_loss: 0.013994 Test Acc.：243454/255000(95.47%),233520/255000(91.58%) 0.013645 Save model 00:06:12 10:14:29 left. \n",
      "epoch: 3 train_loss: 0.013942 Test Acc.：243487/255000(95.49%),233204/255000(91.45%) 0.013634 Save model 00:09:20 10:13:01 left. \n",
      "epoch: 4 train_loss: 0.013901 Test Acc.：243594/255000(95.53%),233374/255000(91.52%) 0.013583 Save model 00:12:29 10:12:28 left. \n",
      "epoch: 5 train_loss: 0.013865 Test Acc.：243896/255000(95.65%),233360/255000(91.51%) 0.013599 Save model 00:15:37 10:09:19 left. \n",
      "epoch: 6 train_loss: 0.013878 Test Acc.：243328/255000(95.42%),233033/255000(91.39%) 0.013585 00:18:52 10:10:25 left. \n",
      "epoch: 7 train_loss: 0.013875 Test Acc.：243441/255000(95.47%),232812/255000(91.30%) 0.013662 00:22:04 10:08:31 left. \n",
      "epoch: 8 train_loss: 0.013857 Test Acc.：244125/255000(95.74%),233200/255000(91.45%) 0.013528 Save model 00:25:09 10:03:59 left. \n",
      "epoch: 9 train_loss: 0.013864 Test Acc.：242691/255000(95.17%),231771/255000(90.89%) 0.013765 00:28:18 10:00:38 left. \n",
      "epoch: 10 train_loss: 0.013854 Test Acc.：243756/255000(95.59%),233325/255000(91.50%) 0.013572 Save model 00:31:22 09:56:03 left. \n",
      "epoch: 11 train_loss: 0.013832 Test Acc.：244007/255000(95.69%),233224/255000(91.46%) 0.013599 Save model 00:34:37 09:54:58 left. \n",
      "epoch: 12 train_loss: 0.013827 Test Acc.：243434/255000(95.46%),233072/255000(91.40%) 0.013611 Save model 00:37:55 09:54:17 left. \n",
      "epoch: 13 train_loss: 0.01381 Test Acc.：244019/255000(95.69%),233216/255000(91.46%) 0.013637 Save model 00:41:04 09:50:43 left. \n",
      "epoch: 14 train_loss: 0.013804 Test Acc.：244010/255000(95.69%),232917/255000(91.34%) 0.013534 Save model 00:44:15 09:47:58 left. \n",
      "epoch: 15 train_loss: 0.013843 Test Acc.：243748/255000(95.59%),232319/255000(91.11%) 0.01352 00:47:34 09:46:48 left. \n",
      "epoch: 16 train_loss: 0.01381 Test Acc.：243615/255000(95.54%),231849/255000(90.92%) 0.013648 00:50:49 09:44:25 left. \n",
      "epoch: 17 train_loss: 0.013805 Test Acc.：243804/255000(95.61%),232537/255000(91.19%) 0.013543 00:53:55 09:40:31 left. \n",
      "epoch: 18 train_loss: 0.013804 Test Acc.：243610/255000(95.53%),232638/255000(91.23%) 0.013582 Save model 00:57:03 09:36:59 left. \n",
      "epoch: 19 train_loss: 0.013793 Test Acc.：243808/255000(95.61%),232637/255000(91.23%) 0.013643 Save model 01:00:18 09:34:32 left. \n",
      "epoch: 20 train_loss: 0.013787 Test Acc.：244009/255000(95.69%),233048/255000(91.39%) 0.013473 Save model 01:03:35 09:32:19 left. \n",
      "epoch: 21 train_loss: 0.013781 Test Acc.：243586/255000(95.52%),233006/255000(91.37%) 0.013575 Save model 01:06:43 09:28:47 left. \n",
      "epoch: 22 train_loss: 0.013779 Test Acc.：244171/255000(95.75%),232945/255000(91.35%) 0.013512 Save model 01:09:51 09:25:09 left. \n",
      "epoch: 23 train_loss: 0.013789 Test Acc.：244091/255000(95.72%),232597/255000(91.21%) 0.013553 01:12:56 09:21:20 left. \n",
      "epoch: 24 train_loss: 0.013773 Test Acc.：243795/255000(95.61%),232445/255000(91.15%) 0.013635 Save model 01:16:03 09:17:47 left. \n",
      "epoch: 25 train_loss: 0.013773 Test Acc.：243864/255000(95.63%),232270/255000(91.09%) 0.013521 Save model 01:19:06 09:13:46 left. \n",
      "epoch: 26 train_loss: 0.013761 Test Acc.：243882/255000(95.64%),232595/255000(91.21%) 0.01352 Save model 01:22:12 09:10:09 left. \n",
      "epoch: 27 train_loss: 0.013755 Test Acc.：243990/255000(95.68%),232448/255000(91.16%) 0.013558 Save model 01:25:22 09:07:01 left. \n",
      "epoch: 28 train_loss: 0.013762 Test Acc.：244091/255000(95.72%),232721/255000(91.26%) 0.01353 01:28:29 09:03:35 left. \n",
      "epoch: 29 train_loss: 0.013765 Test Acc.：244334/255000(95.82%),232702/255000(91.26%) 0.013487 01:31:37 09:00:17 left. \n",
      "epoch: 30 train_loss: 0.013748 Test Acc.：244512/255000(95.89%),232878/255000(91.32%) 0.013519 Save model 01:34:46 08:57:03 left. \n",
      "epoch: 31 train_loss: 0.013759 Test Acc.：244057/255000(95.71%),232845/255000(91.31%) 0.013576 01:37:52 08:53:34 left. \n",
      "epoch: 32 train_loss: 0.013733 Test Acc.：244271/255000(95.79%),232537/255000(91.19%) 0.013578 Save model 01:41:05 08:50:43 left. \n",
      "epoch: 33 train_loss: 0.01374 Test Acc.：244326/255000(95.81%),232468/255000(91.16%) 0.013435 01:44:14 08:47:32 left. \n",
      "epoch: 34 train_loss: 0.013729 Test Acc.：244304/255000(95.81%),232811/255000(91.30%) 0.013509 Save model 01:47:13 08:43:29 left. \n",
      "epoch: 35 train_loss: 0.013726 Test Acc.：243915/255000(95.65%),232899/255000(91.33%) 0.013525 Save model 01:50:32 08:41:07 left. \n",
      "epoch: 36 train_loss: 0.013726 Test Acc.：244828/255000(96.01%),232656/255000(91.24%) 0.013449 Save model 01:53:42 08:38:01 left. \n",
      "epoch: 37 train_loss: 0.01371 Test Acc.：243981/255000(95.68%),232870/255000(91.32%) 0.013472 Save model 01:56:51 08:34:48 left. \n",
      "epoch: 38 train_loss: 0.013713 Test Acc.：244329/255000(95.82%),232827/255000(91.30%) 0.013537 02:00:02 08:31:47 left. \n",
      "epoch: 39 train_loss: 0.013692 Test Acc.：242688/255000(95.17%),232951/255000(91.35%) 0.013771 Save model 02:03:16 08:28:54 left. \n",
      "epoch: 40 train_loss: 0.013702 Test Acc.：244056/255000(95.71%),232777/255000(91.29%) 0.013574 02:06:36 08:26:26 left. \n",
      "epoch: 41 train_loss: 0.013707 Test Acc.：244194/255000(95.76%),232826/255000(91.30%) 0.013477 02:09:48 08:23:24 left. \n",
      "epoch: 42 train_loss: 0.013684 Test Acc.：243760/255000(95.59%),232290/255000(91.09%) 0.013529 Save model 02:12:54 08:20:00 left. \n",
      "epoch: 43 train_loss: 0.013692 Test Acc.：243805/255000(95.61%),232601/255000(91.22%) 0.013573 02:16:07 08:17:02 left. \n",
      "epoch: 44 train_loss: 0.013692 Test Acc.：244893/255000(96.04%),231966/255000(90.97%) 0.013515 02:19:27 08:14:26 left. \n",
      "epoch: 45 train_loss: 0.013675 Test Acc.：244615/255000(95.93%),232548/255000(91.20%) 0.013443 Save model 02:22:38 08:11:19 left. \n",
      "epoch: 46 train_loss: 0.013668 Test Acc.：244397/255000(95.84%),232523/255000(91.19%) 0.013478 Save model 02:25:43 08:07:52 left. \n",
      "epoch: 47 train_loss: 0.013663 Test Acc.：244089/255000(95.72%),232445/255000(91.15%) 0.0135 Save model 02:28:41 08:04:01 left. \n",
      "epoch: 48 train_loss: 0.013652 Test Acc.：244363/255000(95.83%),232954/255000(91.35%) 0.013507 Save model 02:31:45 08:00:32 left. \n",
      "epoch: 49 train_loss: 0.013661 Test Acc.：243850/255000(95.63%),232804/255000(91.30%) 0.013531 02:34:50 07:57:09 left. \n",
      "epoch: 50 train_loss: 0.013644 Test Acc.：244099/255000(95.73%),233061/255000(91.40%) 0.013518 Save model 02:37:55 07:53:46 left. \n",
      "epoch: 51 train_loss: 0.013653 Test Acc.：244390/255000(95.84%),232700/255000(91.25%) 0.013402 02:41:05 07:50:39 left. \n",
      "epoch: 52 train_loss: 0.013643 Test Acc.：244865/255000(96.03%),232937/255000(91.35%) 0.013371 Save model 02:44:24 07:47:56 left. \n",
      "epoch: 53 train_loss: 0.013633 Test Acc.：244716/255000(95.97%),230857/255000(90.53%) 0.013481 Save model 02:47:42 07:45:10 left. \n",
      "epoch: 54 train_loss: 0.013641 Test Acc.：244607/255000(95.92%),232804/255000(91.30%) 0.013413 02:50:54 07:42:04 left. \n",
      "epoch: 55 train_loss: 0.013615 Test Acc.：244542/255000(95.90%),232580/255000(91.21%) 0.013436 Save model 02:54:04 07:38:56 left. \n",
      "epoch: 56 train_loss: 0.013625 Test Acc.：244586/255000(95.92%),232777/255000(91.29%) 0.013338 02:57:12 07:35:41 left. \n",
      "epoch: 57 train_loss: 0.013612 Test Acc.：244804/255000(96.00%),232907/255000(91.34%) 0.013368 Save model 03:00:19 07:32:24 left. \n",
      "epoch: 58 train_loss: 0.013598 Test Acc.：244044/255000(95.70%),232701/255000(91.26%) 0.013485 Save model 03:03:28 07:29:12 left. \n",
      "epoch: 59 train_loss: 0.013597 Test Acc.：245012/255000(96.08%),232919/255000(91.34%) 0.013307 Save model 03:06:36 07:25:56 left. \n",
      "epoch: 60 train_loss: 0.013595 Test Acc.：245037/255000(96.09%),232624/255000(91.23%) 0.013337 Save model 03:09:43 07:22:41 left. \n",
      "epoch: 61 train_loss: 0.013595 Test Acc.：244440/255000(95.86%),232753/255000(91.28%) 0.01347 03:13:03 07:19:55 left. \n",
      "epoch: 62 train_loss: 0.01358 Test Acc.：244641/255000(95.94%),232746/255000(91.27%) 0.013366 Save model 03:16:19 07:16:58 left. \n",
      "epoch: 63 train_loss: 0.013598 Test Acc.：244558/255000(95.91%),232571/255000(91.20%) 0.013346 03:19:27 07:13:43 left. \n",
      "epoch: 64 train_loss: 0.01357 Test Acc.：244984/255000(96.07%),232779/255000(91.29%) 0.01338 Save model 03:22:33 07:10:27 left. \n",
      "epoch: 65 train_loss: 0.013569 Test Acc.：244832/255000(96.01%),232638/255000(91.23%) 0.013352 Save model 03:25:58 07:07:47 left. \n",
      "epoch: 66 train_loss: 0.013584 Test Acc.：244848/255000(96.02%),232507/255000(91.18%) 0.013353 03:29:13 07:04:48 left. \n",
      "epoch: 67 train_loss: 0.01356 Test Acc.：244825/255000(96.01%),232297/255000(91.10%) 0.013333 Save model 03:32:24 07:01:38 left. \n",
      "epoch: 68 train_loss: 0.013572 Test Acc.：244676/255000(95.95%),232153/255000(91.04%) 0.013417 03:35:42 06:58:43 left. \n",
      "epoch: 69 train_loss: 0.013537 Test Acc.：244961/255000(96.06%),232554/255000(91.20%) 0.013314 Save model 03:38:48 06:55:24 left. \n",
      "epoch: 70 train_loss: 0.013547 Test Acc.：245178/255000(96.15%),232542/255000(91.19%) 0.013294 03:41:58 06:52:14 left. \n",
      "epoch: 71 train_loss: 0.013537 Test Acc.：244702/255000(95.96%),232501/255000(91.18%) 0.013344 Save model 03:45:11 06:49:08 left. \n",
      "epoch: 72 train_loss: 0.013531 Test Acc.：244871/255000(96.03%),232174/255000(91.05%) 0.01339 Save model 03:48:23 06:46:02 left. \n",
      "epoch: 73 train_loss: 0.013521 Test Acc.：244779/255000(95.99%),232654/255000(91.24%) 0.013325 Save model 03:51:35 06:42:54 left. \n",
      "epoch: 74 train_loss: 0.013514 Test Acc.：244575/255000(95.91%),232486/255000(91.17%) 0.013429 Save model 03:54:45 06:39:42 left. \n",
      "epoch: 75 train_loss: 0.013507 Test Acc.：245338/255000(96.21%),232589/255000(91.21%) 0.013326 Save model 03:58:00 06:36:40 left. \n",
      "epoch: 76 train_loss: 0.0135 Test Acc.：245007/255000(96.08%),232227/255000(91.07%) 0.013316 Save model 04:01:08 06:33:25 left. \n",
      "epoch: 77 train_loss: 0.013503 Test Acc.：244450/255000(95.86%),232174/255000(91.05%) 0.013402 04:04:23 06:30:23 left. \n",
      "epoch: 78 train_loss: 0.013497 Test Acc.：245182/255000(96.15%),232110/255000(91.02%) 0.013344 Save model 04:07:30 06:27:06 left. \n",
      "epoch: 79 train_loss: 0.013475 Test Acc.：244995/255000(96.08%),232744/255000(91.27%) 0.013318 Save model 04:10:45 06:24:04 left. \n",
      "epoch: 80 train_loss: 0.01348 Test Acc.：245224/255000(96.17%),232533/255000(91.19%) 0.013282 04:14:00 06:21:00 left. \n",
      "epoch: 81 train_loss: 0.013486 Test Acc.：245390/255000(96.23%),232368/255000(91.12%) 0.013237 04:17:15 06:17:56 left. \n",
      "epoch: 82 train_loss: 0.013469 Test Acc.：245482/255000(96.27%),232486/255000(91.17%) 0.013315 Save model 04:20:26 06:14:47 left. \n",
      "epoch: 83 train_loss: 0.013455 Test Acc.：244874/255000(96.03%),232190/255000(91.05%) 0.013337 Save model 04:23:33 06:11:31 left. \n",
      "epoch: 84 train_loss: 0.013446 Test Acc.：245300/255000(96.20%),232596/255000(91.21%) 0.013266 Save model 04:26:43 06:08:20 left. \n",
      "epoch: 85 train_loss: 0.013464 Test Acc.：245276/255000(96.19%),232542/255000(91.19%) 0.013317 04:29:58 06:05:15 left. \n",
      "epoch: 86 train_loss: 0.013427 Test Acc.：245070/255000(96.11%),232311/255000(91.10%) 0.013318 Save model 04:33:03 06:01:57 left. \n",
      "epoch: 87 train_loss: 0.013438 Test Acc.：244939/255000(96.05%),231423/255000(90.75%) 0.013349 04:36:17 05:58:51 left. \n",
      "epoch: 88 train_loss: 0.01343 Test Acc.：245307/255000(96.20%),232291/255000(91.09%) 0.013292 04:39:26 05:55:39 left. \n",
      "epoch: 89 train_loss: 0.013428 Test Acc.：245046/255000(96.10%),232474/255000(91.17%) 0.013297 04:42:32 05:52:22 left. \n",
      "epoch: 90 train_loss: 0.013413 Test Acc.：245164/255000(96.14%),232710/255000(91.26%) 0.013308 Save model 04:45:36 05:49:04 left. \n",
      "epoch: 91 train_loss: 0.013401 Test Acc.：245465/255000(96.26%),232669/255000(91.24%) 0.01325 Save model 04:48:42 05:45:48 left. \n",
      "epoch: 92 train_loss: 0.013391 Test Acc.：245475/255000(96.26%),232329/255000(91.11%) 0.013227 Save model 04:51:52 05:42:37 left. \n",
      "epoch: 93 train_loss: 0.013388 Test Acc.：245896/255000(96.43%),232592/255000(91.21%) 0.013207 Save model 04:54:59 05:39:23 left. \n",
      "epoch: 94 train_loss: 0.013378 Test Acc.：245335/255000(96.21%),232272/255000(91.09%) 0.01325 Save model 04:58:10 05:36:14 left. \n",
      "epoch: 95 train_loss: 0.013382 Test Acc.：245742/255000(96.37%),232533/255000(91.19%) 0.01316 05:01:23 05:33:07 left. \n",
      "epoch: 96 train_loss: 0.013366 Test Acc.：245497/255000(96.27%),232279/255000(91.09%) 0.01317 Save model 05:04:38 05:30:01 left. \n",
      "epoch: 97 train_loss: 0.013357 Test Acc.：245835/255000(96.41%),232306/255000(91.10%) 0.013205 Save model 05:07:54 05:26:56 left. \n",
      "epoch: 98 train_loss: 0.013349 Test Acc.：245636/255000(96.33%),232540/255000(91.19%) 0.013158 Save model 05:11:07 05:23:49 left. \n",
      "epoch: 99 train_loss: 0.01334 Test Acc.：245968/255000(96.46%),232712/255000(91.26%) 0.013155 Save model 05:14:11 05:20:32 left. \n",
      "epoch: 100 train_loss: 0.013342 Test Acc.：245729/255000(96.36%),232548/255000(91.20%) 0.013197 05:17:18 05:17:18 left. \n",
      "epoch: 101 train_loss: 0.013322 Test Acc.：245541/255000(96.29%),232666/255000(91.24%) 0.013203 Save model 05:20:26 05:14:05 left. \n",
      "epoch: 102 train_loss: 0.013321 Test Acc.：245422/255000(96.24%),232713/255000(91.26%) 0.013191 Save model 05:23:41 05:10:59 left. \n",
      "epoch: 103 train_loss: 0.013317 Test Acc.：245404/255000(96.24%),232250/255000(91.08%) 0.013263 Save model 05:26:53 05:07:50 left. \n",
      "epoch: 104 train_loss: 0.013301 Test Acc.：246089/255000(96.51%),232639/255000(91.23%) 0.013154 Save model 05:30:00 05:04:37 left. \n",
      "epoch: 105 train_loss: 0.013302 Test Acc.：246080/255000(96.50%),232615/255000(91.22%) 0.013165 05:33:14 05:01:29 left. \n",
      "epoch: 106 train_loss: 0.013287 Test Acc.：246060/255000(96.49%),232340/255000(91.11%) 0.013155 Save model 05:36:31 04:58:25 left. \n",
      "epoch: 107 train_loss: 0.013287 Test Acc.：245940/255000(96.45%),232662/255000(91.24%) 0.013093 Save model 05:39:40 04:55:13 left. \n",
      "epoch: 108 train_loss: 0.013269 Test Acc.：246123/255000(96.52%),232394/255000(91.13%) 0.013163 Save model 05:42:50 04:52:03 left. \n",
      "epoch: 109 train_loss: 0.013266 Test Acc.：245449/255000(96.25%),232635/255000(91.23%) 0.013169 Save model 05:45:57 04:48:49 left. \n",
      "epoch: 110 train_loss: 0.013258 Test Acc.：246033/255000(96.48%),232535/255000(91.19%) 0.013141 Save model 05:49:04 04:45:36 left. \n",
      "epoch: 111 train_loss: 0.013253 Test Acc.：246125/255000(96.52%),232594/255000(91.21%) 0.013099 Save model 05:52:12 04:42:24 left. \n",
      "epoch: 112 train_loss: 0.013249 Test Acc.：246050/255000(96.49%),232618/255000(91.22%) 0.013112 Save model 05:55:17 04:39:09 left. \n",
      "epoch: 113 train_loss: 0.013235 Test Acc.：246036/255000(96.48%),232639/255000(91.23%) 0.013132 Save model 05:58:26 04:35:57 left. \n",
      "epoch: 114 train_loss: 0.013225 Test Acc.：246239/255000(96.56%),232066/255000(91.01%) 0.013083 Save model 06:01:34 04:32:46 left. \n",
      "epoch: 115 train_loss: 0.013223 Test Acc.：245942/255000(96.45%),231441/255000(90.76%) 0.013103 Save model 06:04:42 04:29:34 left. \n",
      "epoch: 116 train_loss: 0.013219 Test Acc.：246197/255000(96.55%),232381/255000(91.13%) 0.013057 Save model 06:07:50 04:26:22 left. \n",
      "epoch: 117 train_loss: 0.013198 Test Acc.：246122/255000(96.52%),232408/255000(91.14%) 0.01308 Save model 06:10:55 04:23:08 left. \n",
      "epoch: 118 train_loss: 0.013206 Test Acc.：246256/255000(96.57%),231898/255000(90.94%) 0.013089 06:14:03 04:19:56 left. \n",
      "epoch: 119 train_loss: 0.013199 Test Acc.：246422/255000(96.64%),232388/255000(91.13%) 0.013049 06:17:16 04:16:48 left. \n",
      "epoch: 120 train_loss: 0.013184 Test Acc.：246341/255000(96.60%),232186/255000(91.05%) 0.013051 Save model 06:20:27 04:13:38 left. \n",
      "epoch: 121 train_loss: 0.01317 Test Acc.：246217/255000(96.56%),232122/255000(91.03%) 0.01306 Save model 06:23:35 04:10:26 left. \n",
      "epoch: 122 train_loss: 0.013159 Test Acc.：246296/255000(96.59%),232184/255000(91.05%) 0.013072 Save model 06:26:41 04:07:13 left. \n",
      "epoch: 123 train_loss: 0.013154 Test Acc.：246366/255000(96.61%),231960/255000(90.96%) 0.013062 Save model 06:29:51 04:04:03 left. \n",
      "epoch: 124 train_loss: 0.013145 Test Acc.：246492/255000(96.66%),231830/255000(90.91%) 0.013011 Save model 06:33:03 04:00:54 left. \n",
      "epoch: 125 train_loss: 0.013135 Test Acc.：246371/255000(96.62%),232204/255000(91.06%) 0.013074 Save model 06:36:10 03:57:42 left. \n",
      "epoch: 126 train_loss: 0.013129 Test Acc.：246576/255000(96.70%),232200/255000(91.06%) 0.013048 Save model 06:39:20 03:54:31 left. \n",
      "epoch: 127 train_loss: 0.013123 Test Acc.：246537/255000(96.68%),232011/255000(90.98%) 0.013008 Save model 06:42:31 03:51:22 left. \n",
      "epoch: 128 train_loss: 0.013116 Test Acc.：246228/255000(96.56%),231151/255000(90.65%) 0.013032 Save model 06:45:59 03:48:22 left. \n",
      "epoch: 129 train_loss: 0.013105 Test Acc.：246592/255000(96.70%),231849/255000(90.92%) 0.013016 Save model 06:49:27 03:45:21 left. \n",
      "epoch: 130 train_loss: 0.013095 Test Acc.：246673/255000(96.73%),231916/255000(90.95%) 0.012971 Save model 06:52:51 03:42:18 left. \n",
      "epoch: 131 train_loss: 0.013081 Test Acc.：246641/255000(96.72%),230916/255000(90.56%) 0.012981 Save model 06:56:18 03:39:16 left. \n",
      "epoch: 132 train_loss: 0.013076 Test Acc.：246692/255000(96.74%),231995/255000(90.98%) 0.012979 Save model 06:59:40 03:36:11 left. \n",
      "epoch: 133 train_loss: 0.013074 Test Acc.：246603/255000(96.71%),231042/255000(90.60%) 0.013016 Save model 07:03:12 03:33:11 left. \n",
      "epoch: 134 train_loss: 0.013062 Test Acc.：246700/255000(96.75%),230659/255000(90.45%) 0.012977 Save model 07:06:30 03:30:04 left. \n",
      "epoch: 135 train_loss: 0.013049 Test Acc.：246878/255000(96.81%),231095/255000(90.63%) 0.012981 Save model 07:09:53 03:26:59 left. \n",
      "epoch: 136 train_loss: 0.013045 Test Acc.：246809/255000(96.79%),230473/255000(90.38%) 0.012951 Save model 07:13:22 03:23:56 left. \n",
      "epoch: 137 train_loss: 0.013037 Test Acc.：246826/255000(96.79%),229547/255000(90.02%) 0.012937 Save model 07:16:48 03:20:52 left. \n",
      "epoch: 138 train_loss: 0.013026 Test Acc.：246879/255000(96.82%),229883/255000(90.15%) 0.012918 Save model 07:20:08 03:17:44 left. \n",
      "epoch: 139 train_loss: 0.013013 Test Acc.：246926/255000(96.83%),229202/255000(89.88%) 0.012938 Save model 07:23:37 03:14:40 left. \n",
      "epoch: 140 train_loss: 0.013005 Test Acc.：246886/255000(96.82%),230267/255000(90.30%) 0.01294 Save model 07:27:15 03:11:40 left. \n",
      "epoch: 141 train_loss: 0.012997 Test Acc.：247004/255000(96.86%),228377/255000(89.56%) 0.012922 Save model 07:30:42 03:08:35 left. \n",
      "epoch: 142 train_loss: 0.012982 Test Acc.：246883/255000(96.82%),227045/255000(89.04%) 0.012906 Save model 07:34:12 03:05:31 left. \n",
      "epoch: 143 train_loss: 0.012974 Test Acc.：246907/255000(96.83%),225283/255000(88.35%) 0.012922 Save model 07:37:49 03:02:29 left. \n",
      "epoch: 144 train_loss: 0.012966 Test Acc.：247018/255000(96.87%),229190/255000(89.88%) 0.012908 Save model 07:41:18 02:59:23 left. \n",
      "epoch: 145 train_loss: 0.012956 Test Acc.：247165/255000(96.93%),222171/255000(87.13%) 0.012877 Save model 07:44:51 02:56:19 left. \n",
      "epoch: 146 train_loss: 0.012955 Test Acc.：247124/255000(96.91%),225440/255000(88.41%) 0.012873 Save model 07:48:20 02:53:13 left. \n",
      "epoch: 147 train_loss: 0.012944 Test Acc.：247262/255000(96.97%),224701/255000(88.12%) 0.012904 Save model 07:51:48 02:50:06 left. \n",
      "epoch: 148 train_loss: 0.012931 Test Acc.：247238/255000(96.96%),226907/255000(88.98%) 0.012862 Save model 07:55:13 02:46:58 left. \n",
      "epoch: 149 train_loss: 0.012921 Test Acc.：247118/255000(96.91%),224552/255000(88.06%) 0.012878 Save model 07:58:39 02:43:50 left. \n",
      "epoch: 150 train_loss: 0.012915 Test Acc.：247217/255000(96.95%),227038/255000(89.03%) 0.012841 Save model 08:02:06 02:40:42 left. \n",
      "epoch: 151 train_loss: 0.012901 Test Acc.：247223/255000(96.95%),222095/255000(87.10%) 0.012867 Save model 08:05:30 02:37:32 left. \n",
      "epoch: 152 train_loss: 0.012893 Test Acc.：247345/255000(97.00%),225626/255000(88.48%) 0.012825 Save model 08:09:00 02:34:25 left. \n",
      "epoch: 153 train_loss: 0.012889 Test Acc.：247296/255000(96.98%),221997/255000(87.06%) 0.012858 Save model 08:12:37 02:31:19 left. \n",
      "epoch: 154 train_loss: 0.012878 Test Acc.：247433/255000(97.03%),222935/255000(87.43%) 0.01285 Save model 08:16:07 02:28:11 left. \n",
      "epoch: 155 train_loss: 0.012868 Test Acc.：247340/255000(97.00%),222284/255000(87.17%) 0.01284 Save model 08:19:42 02:25:04 left. \n",
      "epoch: 156 train_loss: 0.012854 Test Acc.：247567/255000(97.09%),222829/255000(87.38%) 0.012816 Save model 08:23:09 02:21:55 left. \n",
      "epoch: 157 train_loss: 0.012849 Test Acc.：247475/255000(97.05%),219977/255000(86.27%) 0.012808 Save model 08:26:48 02:18:48 left. \n",
      "epoch: 158 train_loss: 0.012837 Test Acc.：247195/255000(96.94%),220376/255000(86.42%) 0.012832 Save model 08:30:15 02:15:38 left. \n",
      "epoch: 159 train_loss: 0.012831 Test Acc.：247619/255000(97.11%),224617/255000(88.09%) 0.012787 Save model 08:33:44 02:12:28 left. \n",
      "epoch: 160 train_loss: 0.012822 Test Acc.：247586/255000(97.09%),225038/255000(88.25%) 0.01278 Save model 08:37:10 02:09:17 left. \n",
      "epoch: 161 train_loss: 0.01281 Test Acc.：247610/255000(97.10%),218823/255000(85.81%) 0.012793 Save model 08:40:35 02:06:06 left. \n",
      "epoch: 162 train_loss: 0.012803 Test Acc.：247617/255000(97.10%),220057/255000(86.30%) 0.012782 Save model 08:44:03 02:02:55 left. \n",
      "epoch: 163 train_loss: 0.012792 Test Acc.：247644/255000(97.12%),219938/255000(86.25%) 0.012773 Save model 08:47:29 01:59:44 left. \n",
      "epoch: 164 train_loss: 0.012783 Test Acc.：247712/255000(97.14%),219492/255000(86.08%) 0.012766 Save model 08:50:50 01:56:31 left. \n",
      "epoch: 165 train_loss: 0.012777 Test Acc.：247758/255000(97.16%),222235/255000(87.15%) 0.01277 Save model 08:54:20 01:53:20 left. \n",
      "epoch: 166 train_loss: 0.012769 Test Acc.：247809/255000(97.18%),220184/255000(86.35%) 0.012745 Save model 08:57:56 01:50:10 left. \n",
      "epoch: 167 train_loss: 0.012759 Test Acc.：247637/255000(97.11%),224862/255000(88.18%) 0.012747 Save model 09:01:29 01:47:00 left. \n",
      "epoch: 168 train_loss: 0.012749 Test Acc.：247750/255000(97.16%),225397/255000(88.39%) 0.012741 Save model 09:04:58 01:43:48 left. \n",
      "epoch: 169 train_loss: 0.012742 Test Acc.：247835/255000(97.19%),224472/255000(88.03%) 0.012736 Save model 09:08:20 01:40:34 left. \n",
      "epoch: 170 train_loss: 0.012735 Test Acc.：247884/255000(97.21%),223961/255000(87.83%) 0.012728 Save model 09:11:48 01:37:22 left. \n",
      "epoch: 171 train_loss: 0.012724 Test Acc.：247981/255000(97.25%),223851/255000(87.78%) 0.012708 Save model 09:15:16 01:34:10 left. \n",
      "epoch: 172 train_loss: 0.012717 Test Acc.：247905/255000(97.22%),224154/255000(87.90%) 0.012719 Save model 09:18:46 01:30:57 left. \n",
      "epoch: 173 train_loss: 0.012709 Test Acc.：247992/255000(97.25%),222330/255000(87.19%) 0.012707 Save model 09:22:18 01:27:45 left. \n",
      "epoch: 174 train_loss: 0.012698 Test Acc.：248038/255000(97.27%),222902/255000(87.41%) 0.0127 Save model 09:25:40 01:24:31 left. \n",
      "epoch: 175 train_loss: 0.01269 Test Acc.：248026/255000(97.27%),225597/255000(88.47%) 0.012687 Save model 09:29:06 01:21:18 left. \n",
      "epoch: 176 train_loss: 0.012683 Test Acc.：248016/255000(97.26%),221205/255000(86.75%) 0.012688 Save model 09:32:30 01:18:04 left. \n",
      "epoch: 177 train_loss: 0.012676 Test Acc.：247998/255000(97.25%),224660/255000(88.10%) 0.012697 Save model 09:36:05 01:14:51 left. \n",
      "epoch: 178 train_loss: 0.01267 Test Acc.：248096/255000(97.29%),222512/255000(87.26%) 0.012689 Save model 09:39:43 01:11:39 left. \n",
      "epoch: 179 train_loss: 0.012657 Test Acc.：248132/255000(97.31%),222139/255000(87.11%) 0.012671 Save model 09:43:09 01:08:24 left. \n",
      "epoch: 180 train_loss: 0.012652 Test Acc.：248043/255000(97.27%),224226/255000(87.93%) 0.012677 Save model 09:46:33 01:05:10 left. \n",
      "epoch: 181 train_loss: 0.012645 Test Acc.：248158/255000(97.32%),225172/255000(88.30%) 0.012678 Save model 09:50:02 01:01:56 left. \n",
      "epoch: 182 train_loss: 0.012641 Test Acc.：248137/255000(97.31%),223666/255000(87.71%) 0.012673 Save model 09:53:27 00:58:41 left. \n",
      "epoch: 183 train_loss: 0.012631 Test Acc.：248244/255000(97.35%),223934/255000(87.82%) 0.012659 Save model 09:56:53 00:55:26 left. \n",
      "epoch: 184 train_loss: 0.012625 Test Acc.：248237/255000(97.35%),224769/255000(88.14%) 0.012662 Save model 10:00:25 00:52:12 left. \n",
      "epoch: 185 train_loss: 0.012623 Test Acc.：248181/255000(97.33%),226028/255000(88.64%) 0.012652 Save model 10:03:52 00:48:57 left. \n",
      "epoch: 186 train_loss: 0.012614 Test Acc.：248239/255000(97.35%),223871/255000(87.79%) 0.012646 Save model 10:07:22 00:45:42 left. \n",
      "epoch: 187 train_loss: 0.012609 Test Acc.：248255/255000(97.35%),225157/255000(88.30%) 0.01265 Save model 10:10:47 00:42:27 left. \n",
      "epoch: 188 train_loss: 0.012604 Test Acc.：248255/255000(97.35%),224283/255000(87.95%) 0.012643 Save model 10:14:10 00:39:12 left. \n",
      "epoch: 189 train_loss: 0.012599 Test Acc.：248271/255000(97.36%),223814/255000(87.77%) 0.012637 Save model 10:17:38 00:35:56 left. \n",
      "epoch: 190 train_loss: 0.012594 Test Acc.：248328/255000(97.38%),225088/255000(88.27%) 0.012646 Save model 10:21:04 00:32:41 left. \n",
      "epoch: 191 train_loss: 0.012589 Test Acc.：248349/255000(97.39%),224016/255000(87.85%) 0.012631 Save model 10:24:26 00:29:25 left. \n",
      "epoch: 192 train_loss: 0.012588 Test Acc.：248321/255000(97.38%),224360/255000(87.98%) 0.012637 Save model 10:27:52 00:26:09 left. \n",
      "epoch: 193 train_loss: 0.012583 Test Acc.：248368/255000(97.40%),224871/255000(88.18%) 0.012633 Save model 10:31:21 00:22:53 left. \n",
      "epoch: 194 train_loss: 0.012581 Test Acc.：248358/255000(97.40%),224706/255000(88.12%) 0.012631 Save model 10:34:46 00:19:37 left. \n",
      "epoch: 195 train_loss: 0.01258 Test Acc.：248365/255000(97.40%),224577/255000(88.07%) 0.012633 Save model 10:38:15 00:16:21 left. \n",
      "epoch: 196 train_loss: 0.012576 Test Acc.：248348/255000(97.39%),223950/255000(87.82%) 0.012629 Save model 10:41:40 00:13:05 left. \n",
      "epoch: 197 train_loss: 0.012573 Test Acc.：248394/255000(97.41%),223939/255000(87.82%) 0.012625 Save model 10:45:02 00:09:49 left. \n",
      "epoch: 198 train_loss: 0.012574 Test Acc.：248387/255000(97.41%),224508/255000(88.04%) 0.012634 10:48:23 00:06:32 left. \n",
      "epoch: 199 train_loss: 0.012571 Test Acc.：248336/255000(97.39%),225086/255000(88.27%) 0.012633 Save model 10:51:43 00:03:16 left. \n",
      "epoch: 200 train_loss: 0.012569 Test Acc.：248365/255000(97.40%),224683/255000(88.11%) 0.012629 Save model 10:55:16 00:00:00 left. \n",
      "Test Acc.：202537/255000(79.43%),13774/255000(5.40%) 0.024111 \n",
      "01_1_VC2003_32bit_none 01_2_VC2017_32bit_none 02_1_VC2003_32bit_max 02_2_VC2017_32bit_max 03_1_gcc6.3.0_x86_none 03_2_x86-O0-gcc7.5.0 04_1_gcc6.3.0_x86_O3 04_2_x86-O3-gcc7.5.0 05_1_clang5.0.2_32_none 05_2_x86-O0-Clang10.0.0 06_1_clang5.0.2_32_O3 06_2_x86-O3-Clang10.0.0 07_intel_32_none 08_intel_32bit_max 11_VC2017_64bit_none 12_VC2017_64bit_max 13_1_gcc6.3.0_64bit_none 13_2_x86_64-O0-gcc7.5.0 14_1_gcc6.3.0_64bit_max 14_2_x86_64-O3-gcc7.5.0 15_1_clang5.0.2_64bit_none 15_2_x86_64-O0-Clang10.0.0 16_1_clang5.0.2_64bit_max 16_2_x86_64-O3-Clang10.0.0 17_intel_64_none 18_intel_64bit_max 21_arm-O0-gcc 22_arm-O3-gcc 23_arm-O0-Clang 24_arm-O3-Clang 31_arm64-O0-gcc 32_arm64-O3-gcc 33_arm64-O0-Clang 34_arm64-O3-Clang 41_mips-O0-gcc 42_mips-O3-gcc 43_mips-O0-Clang 44_mips-O3-Clang 51_mips64-O0-gcc 52_mips64-O3-gcc 53_mips64-O0-Clang 54_mips64-O3-Clang 61_powerpc-O0-gcc 62_powerpc-O3-gcc 63_powerpc-O0-Clang 64_powerpc-O3-Clang 71_powerpc64-O0-gcc 72_powerpc64-O3-gcc 73_powerpc64-O0-Clang 74_powerpc64-O3-Clang 80_document \n",
      "01_1_VC2003_32bit_none 4809 179 4 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 \n",
      "01_2_VC2017_32bit_none 229 4765 2 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 \n",
      "02_1_VC2003_32bit_max 11 0 4941 24 0 0 3 1 0 3 3 3 0 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 \n",
      "02_2_VC2017_32bit_max 3 9 36 4922 0 1 1 1 2 1 3 1 0 16 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 2 \n",
      "03_1_gcc6.3.0_x86_none 0 0 0 0 4879 110 4 2 0 2 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 \n",
      "03_2_x86-O0-gcc7.5.0 0 0 0 0 171 4823 1 2 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 \n",
      "04_1_gcc6.3.0_x86_O3 1 0 2 3 2 1 4679 247 0 2 35 23 0 0 0 0 0 0 3 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "04_2_x86-O3-gcc7.5.0 1 0 2 1 0 3 326 4621 3 1 20 18 0 2 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "05_1_clang5.0.2_32_none 2 2 0 0 2 0 0 0 4748 237 0 0 0 0 0 0 0 0 0 0 8 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "05_2_x86-O0-Clang10.0.0 1 1 0 0 0 0 1 1 311 4680 2 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "06_1_clang5.0.2_32_O3 1 0 1 11 1 2 26 14 1 0 4738 182 0 6 0 1 0 0 2 0 0 0 10 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "06_2_x86-O3-Clang10.0.0 0 0 4 0 1 1 28 20 1 0 200 4736 0 3 1 0 0 0 0 1 0 0 3 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "07_intel_32_none 0 0 0 0 0 1 0 0 0 0 0 1 4997 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "08_intel_32bit_max 0 2 5 15 1 0 5 1 0 1 8 0 0 4959 0 2 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "11_VC2017_64bit_none 0 0 1 0 0 0 0 0 0 0 0 0 0 0 4993 2 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 \n",
      "12_VC2017_64bit_max 0 0 0 2 0 0 0 0 0 0 0 0 0 1 3 4964 0 3 5 4 0 0 6 1 0 10 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 \n",
      "13_1_gcc6.3.0_64bit_none 1 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 4495 493 1 1 1 3 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "13_2_x86_64-O0-gcc7.5.0 0 2 0 0 4 4 0 0 0 0 0 0 0 0 0 0 710 4277 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "14_1_gcc6.3.0_64bit_max 0 0 0 1 0 0 8 3 0 0 1 0 0 0 1 0 2 1 4645 265 0 0 40 31 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "14_2_x86_64-O3-gcc7.5.0 0 0 0 0 0 0 4 4 0 0 6 1 0 1 0 1 1 3 462 4460 1 2 20 29 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "15_1_clang5.0.2_64bit_none 0 0 0 0 0 0 0 0 11 2 0 0 0 0 0 0 3 1 0 2 4716 265 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "15_2_x86_64-O0-Clang10.0.0 1 0 0 0 0 0 0 0 4 6 1 1 0 0 0 0 2 4 1 0 265 4715 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "16_1_clang5.0.2_64bit_max 0 0 0 4 0 0 0 0 1 1 20 3 0 0 1 2 2 0 54 30 4 2 4686 187 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "16_2_x86_64-O3-Clang10.0.0 0 0 2 0 0 0 4 0 0 0 4 2 0 0 0 1 2 0 57 32 0 0 244 4652 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "17_intel_64_none 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 4997 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 \n",
      "18_intel_64bit_max 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 4 0 1 2 5 0 0 4 1 0 4982 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "21_arm-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 4983 10 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 \n",
      "22_arm-O3-gcc 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 2 0 0 3 4986 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 \n",
      "23_arm-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 4988 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 \n",
      "24_arm-O3-Clang 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 2 4 22 4960 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 \n",
      "31_arm64-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "32_arm64-O3-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4981 1 18 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "33_arm64-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4993 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 \n",
      "34_arm64-O3-Clang 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 25 10 4955 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 \n",
      "41_mips-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4998 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "42_mips-O3-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 4975 0 14 0 9 0 0 0 0 0 0 0 0 0 1 0 \n",
      "43_mips-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4998 2 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "44_mips-O3-Clang 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 20 1 4951 0 2 0 24 0 0 0 0 0 0 0 1 0 \n",
      "51_mips64-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 4998 1 0 0 0 0 0 0 0 0 0 0 0 \n",
      "52_mips64-O3-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 26 0 7 1 4957 0 9 0 0 0 0 0 0 0 0 0 \n",
      "53_mips64-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 4997 0 0 0 0 0 0 0 0 0 0 \n",
      "54_mips64-O3-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 6 0 28 0 18 1 4943 0 0 0 0 0 0 0 0 1 \n",
      "61_powerpc-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4997 0 1 0 1 0 0 0 0 \n",
      "62_powerpc-O3-gcc 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 4979 0 8 0 8 0 1 0 \n",
      "63_powerpc-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4997 3 0 0 0 0 0 \n",
      "64_powerpc-O3-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 17 2 4973 0 1 0 4 1 \n",
      "71_powerpc64-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 2 0 0 4989 2 0 0 1 \n",
      "72_powerpc64-O3-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 23 0 2 2 4964 0 6 0 \n",
      "73_powerpc64-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 9 0 0 0 4990 1 0 \n",
      "74_powerpc64-O3-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 1 37 0 10 0 4947 0 \n",
      "80_document 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 7 2 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4987 \n",
      "no label Num TP FP FN TN R P F1 Acc.\n",
      "0 01_1_VC2003_32bit_none 5000 4809 191 251 4809 0.950395256916996 0.9618 0.9560636182902584 0.9560636182902584\n",
      "1 01_2_VC2017_32bit_none 5000 4765 235 195 4765 0.9606854838709677 0.953 0.9568273092369477 0.9568273092369478\n",
      "2 02_1_VC2003_32bit_max 5000 4941 59 60 4941 0.9880023995200959 0.9882 0.9881011898810118 0.9881011898810119\n",
      "3 02_2_VC2017_32bit_max 5000 4922 78 63 4922 0.9873620862587763 0.9844 0.985878818227341 0.985878818227341\n",
      "4 03_1_gcc6.3.0_x86_none 5000 4879 121 184 4879 0.9636579103298439 0.9758 0.9696909470336877 0.9696909470336877\n",
      "5 03_2_x86-O0-gcc7.5.0 5000 4823 177 124 4823 0.9749343036183545 0.9646 0.9697396199859255 0.9697396199859254\n",
      "6 04_1_gcc6.3.0_x86_O3 5000 4679 321 412 4679 0.919072873698684 0.9358 0.9273610147656327 0.9273610147656327\n",
      "7 04_2_x86-O3-gcc7.5.0 5000 4621 379 298 4621 0.9394185810124009 0.9242 0.9317471519306382 0.9317471519306382\n",
      "8 05_1_clang5.0.2_32_none 5000 4748 252 337 4748 0.9337266470009833 0.9496 0.9415964303420922 0.9415964303420922\n",
      "9 05_2_x86-O0-Clang10.0.0 5000 4680 320 256 4680 0.9481361426256077 0.936 0.9420289855072465 0.9420289855072463\n",
      "10 06_1_clang5.0.2_32_O3 5000 4738 262 306 4738 0.9393338620142744 0.9476 0.9434488251692552 0.9434488251692553\n",
      "11 06_2_x86-O3-Clang10.0.0 5000 4736 264 238 4736 0.952151186168074 0.9472 0.9496691397633849 0.9496691397633849\n",
      "12 07_intel_32_none 5000 4997 3 1 4997 0.9997999199679872 0.9994 0.9995999199839969 0.9995999199839968\n",
      "13 08_intel_32bit_max 5000 4959 41 42 4959 0.9916016796640672 0.9918 0.9917008299170083 0.9917008299170083\n",
      "14 11_VC2017_64bit_none 5000 4993 7 7 4993 0.9986 0.9986 0.9986 0.9986\n",
      "15 12_VC2017_64bit_max 5000 4964 36 15 4964 0.9969873468567986 0.9928 0.9948892674616695 0.9948892674616695\n",
      "16 13_1_gcc6.3.0_64bit_none 5000 4495 505 723 4495 0.8614411651973937 0.899 0.8798199256214523 0.8798199256214524\n",
      "17 13_2_x86_64-O0-gcc7.5.0 5000 4277 723 509 4277 0.8936481404095278 0.8554 0.8741058655221745 0.8741058655221745\n",
      "18 14_1_gcc6.3.0_64bit_max 5000 4645 355 595 4645 0.8864503816793893 0.929 0.9072265625000001 0.9072265625\n",
      "19 14_2_x86_64-O3-gcc7.5.0 5000 4460 540 346 4460 0.928006658343737 0.892 0.9096471548031816 0.9096471548031817\n",
      "20 15_1_clang5.0.2_64bit_none 5000 4716 284 282 4716 0.943577430972389 0.9432 0.9433886777355471 0.9433886777355471\n",
      "21 15_2_x86_64-O0-Clang10.0.0 5000 4715 285 278 4715 0.9443220508712197 0.943 0.9436605623936756 0.9436605623936756\n",
      "22 16_1_clang5.0.2_64bit_max 5000 4686 314 336 4686 0.9330943847072879 0.9372 0.9351426860906007 0.9351426860906007\n",
      "23 16_2_x86_64-O3-Clang10.0.0 5000 4652 348 258 4652 0.9474541751527495 0.9304 0.9388496468213926 0.9388496468213925\n",
      "24 17_intel_64_none 5000 4997 3 1 4997 0.9997999199679872 0.9994 0.9995999199839969 0.9995999199839968\n",
      "25 18_intel_64bit_max 5000 4982 18 20 4982 0.9960015993602559 0.9964 0.9962007598480305 0.9962007598480304\n",
      "26 21_arm-O0-gcc 5000 4983 17 14 4983 0.9971983189913949 0.9966 0.9968990697209162 0.9968990697209162\n",
      "27 22_arm-O3-gcc 5000 4986 14 18 4986 0.9964028776978417 0.9972 0.9968012794882048 0.9968012794882047\n",
      "28 23_arm-O0-Clang 5000 4988 12 22 4988 0.9956087824351297 0.9976 0.9966033966033966 0.9966033966033966\n",
      "29 24_arm-O3-Clang 5000 4960 40 9 4960 0.9981887703763332 0.992 0.9950847627645701 0.9950847627645701\n",
      "30 31_arm64-O0-gcc 5000 5000 0 3 5000 0.9994003597841296 1.0 0.9997000899730081 0.9997000899730081\n",
      "31 32_arm64-O3-gcc 5000 4981 19 27 4981 0.9946086261980831 0.9962 0.9954036770583533 0.9954036770583533\n",
      "32 33_arm64-O0-Clang 5000 4993 7 11 4993 0.9978017585931255 0.9986 0.9982007197121151 0.9982007197121151\n",
      "33 34_arm64-O3-Clang 5000 4955 45 30 4955 0.9939819458375125 0.991 0.9924887330996494 0.9924887330996495\n",
      "34 41_mips-O0-gcc 5000 4998 2 3 4998 0.9994001199760048 0.9996 0.9995000499950005 0.9995000499950005\n",
      "35 42_mips-O3-gcc 5000 4975 25 54 4975 0.9892622787830583 0.995 0.992122843753116 0.992122843753116\n",
      "36 43_mips-O0-Clang 5000 4998 2 3 4998 0.9994001199760048 0.9996 0.9995000499950005 0.9995000499950005\n",
      "37 44_mips-O3-Clang 5000 4951 49 51 4951 0.9898040783686526 0.9902 0.9900019996000801 0.99000199960008\n",
      "38 51_mips64-O0-gcc 5000 4998 2 4 4998 0.9992003198720512 0.9996 0.9994001199760049 0.9994001199760048\n",
      "39 52_mips64-O3-gcc 5000 4957 43 31 4957 0.993785084202085 0.9914 0.9925911093311974 0.9925911093311974\n",
      "40 53_mips64-O0-Clang 5000 4997 3 1 4997 0.9997999199679872 0.9994 0.9995999199839969 0.9995999199839968\n",
      "41 54_mips64-O3-Clang 5000 4943 57 34 4943 0.9931685754470565 0.9886 0.9908790217500252 0.9908790217500251\n",
      "42 61_powerpc-O0-gcc 5000 4997 3 4 4997 0.9992001599680064 0.9994 0.9993000699930007 0.9993000699930007\n",
      "43 62_powerpc-O3-gcc 5000 4979 21 47 4979 0.9906486271388778 0.9958 0.9932176341512068 0.9932176341512069\n",
      "44 63_powerpc-O0-Clang 5000 4997 3 14 4997 0.997206146477749 0.9994 0.9983018679452602 0.9983018679452602\n",
      "45 64_powerpc-O3-Clang 5000 4973 27 53 4973 0.9894548348587345 0.9946 0.9920207460602434 0.9920207460602434\n",
      "46 71_powerpc64-O0-gcc 5000 4989 11 3 4989 0.9993990384615384 0.9978 0.9985988791032826 0.9985988791032826\n",
      "47 72_powerpc64-O3-gcc 5000 4964 36 22 4964 0.99558764540714 0.9928 0.9941918686160623 0.9941918686160625\n",
      "48 73_powerpc64-O0-Clang 5000 4990 10 1 4990 0.9997996393508315 0.998 0.9988990091081974 0.9988990091081974\n",
      "49 74_powerpc64-O3-Clang 5000 4947 53 16 4947 0.9967761434616159 0.9894 0.9930743751881963 0.9930743751881963\n",
      "50 80_document 5000 4987 13 23 4987 0.9954091816367265 0.9974 0.9964035964035964 0.9964035964035964\n",
      "  255000 248365 6635 6635 248365 0.9739803921568627 0.9739803921568627 0.9739803921568627 0.9739803921568627\n",
      "2\n",
      "Start MainNet PreTrain\n",
      "epoch: 1 train_loss: 0.018173 00:01:59 06:37:00 left. \n",
      "epoch: 2 train_loss: 0.015315 00:04:00 06:37:13 left. \n",
      "epoch: 3 train_loss: 0.014759 00:06:02 06:36:43 left. \n",
      "epoch: 4 train_loss: 0.0145 00:08:01 06:33:23 left. \n",
      "epoch: 5 train_loss: 0.014324 00:10:02 06:31:41 left. \n",
      "epoch: 6 train_loss: 0.014217 00:12:02 06:29:15 left. \n",
      "epoch: 7 train_loss: 0.014125 00:14:03 06:27:35 left. \n",
      "epoch: 8 train_loss: 0.014064 00:16:09 06:27:46 left. \n",
      "epoch: 9 train_loss: 0.01401 00:18:10 06:25:47 left. \n",
      "epoch: 10 train_loss: 0.013974 00:20:09 06:23:06 left. \n",
      "epoch: 11 train_loss: 0.013954 00:22:12 06:21:28 left. \n",
      "epoch: 12 train_loss: 0.013918 00:24:13 06:19:28 left. \n",
      "epoch: 13 train_loss: 0.013894 00:26:13 06:17:11 left. \n",
      "epoch: 14 train_loss: 0.013873 00:28:15 06:15:25 left. \n",
      "epoch: 15 train_loss: 0.013856 00:30:14 06:12:58 left. \n",
      "epoch: 16 train_loss: 0.013825 00:32:19 06:11:42 left. \n",
      "epoch: 17 train_loss: 0.013816 00:34:22 06:09:59 left. \n",
      "epoch: 18 train_loss: 0.013817 00:36:22 06:07:45 left. \n",
      "epoch: 19 train_loss: 0.013795 00:38:23 06:05:40 left. \n",
      "epoch: 20 train_loss: 0.013781 00:40:25 06:03:45 left. \n",
      "epoch: 21 train_loss: 0.01377 00:42:25 06:01:35 left. \n",
      "epoch: 22 train_loss: 0.013753 00:44:25 05:59:24 left. \n",
      "epoch: 23 train_loss: 0.013752 00:46:29 05:57:45 left. \n",
      "epoch: 24 train_loss: 0.013732 00:48:28 05:55:28 left. \n",
      "epoch: 25 train_loss: 0.013729 00:50:29 05:53:23 left. \n",
      "epoch: 26 train_loss: 0.013715 00:52:30 05:51:21 left. \n",
      "epoch: 27 train_loss: 0.0137 00:54:29 05:49:06 left. \n",
      "epoch: 28 train_loss: 0.013689 00:56:30 05:47:05 left. \n",
      "epoch: 29 train_loss: 0.013692 00:58:31 05:45:07 left. \n",
      "epoch: 30 train_loss: 0.013673 01:00:30 05:42:51 left. \n",
      "epoch: 31 train_loss: 0.013667 01:02:32 05:40:55 left. \n",
      "epoch: 32 train_loss: 0.013663 01:04:32 05:38:48 left. \n",
      "epoch: 33 train_loss: 0.013663 01:06:30 05:36:32 left. \n",
      "epoch: 34 train_loss: 0.013638 01:08:31 05:34:36 left. \n",
      "epoch: 35 train_loss: 0.013642 01:10:32 05:32:32 left. \n",
      "epoch: 36 train_loss: 0.013624 01:12:31 05:30:23 left. \n",
      "epoch: 37 train_loss: 0.013623 01:14:31 05:28:17 left. \n",
      "epoch: 38 train_loss: 0.013618 01:16:30 05:26:09 left. \n",
      "epoch: 39 train_loss: 0.013607 01:18:32 05:24:15 left. \n",
      "epoch: 40 train_loss: 0.013614 01:20:34 05:22:19 left. \n",
      "epoch: 41 train_loss: 0.013596 01:22:34 05:20:15 left. \n",
      "epoch: 42 train_loss: 0.013586 01:24:33 05:18:05 left. \n",
      "epoch: 43 train_loss: 0.013582 01:26:31 05:15:55 left. \n",
      "epoch: 44 train_loss: 0.013576 01:28:30 05:13:47 left. \n",
      "epoch: 45 train_loss: 0.013562 01:30:28 05:11:37 left. \n",
      "epoch: 46 train_loss: 0.013557 01:32:30 05:09:43 left. \n",
      "epoch: 47 train_loss: 0.013548 01:34:30 05:07:39 left. \n",
      "epoch: 48 train_loss: 0.013552 01:36:28 05:05:31 left. \n",
      "epoch: 49 train_loss: 0.013539 01:38:28 05:03:29 left. \n",
      "epoch: 50 train_loss: 0.013538 01:40:25 05:01:17 left. \n",
      "epoch: 51 train_loss: 0.013524 01:42:25 04:59:14 left. \n",
      "epoch: 52 train_loss: 0.013522 01:44:27 04:57:16 left. \n",
      "epoch: 53 train_loss: 0.013512 01:46:25 04:55:11 left. \n",
      "epoch: 54 train_loss: 0.013502 01:48:25 04:53:08 left. \n",
      "epoch: 55 train_loss: 0.013496 01:50:27 04:51:12 left. \n",
      "epoch: 56 train_loss: 0.013493 01:52:24 04:49:03 left. \n",
      "epoch: 57 train_loss: 0.013478 01:54:23 04:46:58 left. \n",
      "epoch: 58 train_loss: 0.01348 01:56:22 04:44:55 left. \n",
      "epoch: 59 train_loss: 0.013473 01:58:22 04:42:52 left. \n",
      "epoch: 60 train_loss: 0.013464 02:00:25 04:40:59 left. \n",
      "epoch: 61 train_loss: 0.01347 02:02:25 04:38:58 left. \n",
      "epoch: 62 train_loss: 0.01346 02:04:27 04:37:01 left. \n",
      "epoch: 63 train_loss: 0.013442 02:06:31 04:35:07 left. \n",
      "epoch: 64 train_loss: 0.013439 02:08:31 04:33:06 left. \n",
      "epoch: 65 train_loss: 0.013437 02:10:32 04:31:06 left. \n",
      "epoch: 66 train_loss: 0.013431 02:12:32 04:29:05 left. \n",
      "epoch: 67 train_loss: 0.013416 02:14:35 04:27:09 left. \n",
      "epoch: 68 train_loss: 0.013416 02:16:33 04:25:05 left. \n",
      "epoch: 69 train_loss: 0.013406 02:18:32 04:23:02 left. \n",
      "epoch: 70 train_loss: 0.013402 02:20:33 04:21:01 left. \n",
      "epoch: 71 train_loss: 0.013405 02:22:33 04:19:01 left. \n",
      "epoch: 72 train_loss: 0.013389 02:24:35 04:17:02 left. \n",
      "epoch: 73 train_loss: 0.013391 02:26:35 04:15:01 left. \n",
      "epoch: 74 train_loss: 0.013375 02:28:38 04:13:05 left. \n",
      "epoch: 75 train_loss: 0.01337 02:30:38 04:11:03 left. \n",
      "epoch: 76 train_loss: 0.01337 02:32:39 04:09:04 left. \n",
      "epoch: 77 train_loss: 0.013358 02:34:40 04:07:04 left. \n",
      "epoch: 78 train_loss: 0.013347 02:36:39 04:05:01 left. \n",
      "epoch: 79 train_loss: 0.013347 02:38:38 04:02:58 left. \n",
      "epoch: 80 train_loss: 0.013337 02:40:41 04:01:01 left. \n",
      "epoch: 81 train_loss: 0.013334 02:42:42 03:59:02 left. \n",
      "epoch: 82 train_loss: 0.013325 02:44:41 03:56:59 left. \n",
      "epoch: 83 train_loss: 0.013317 02:46:40 03:54:57 left. \n",
      "epoch: 84 train_loss: 0.013315 02:48:38 03:52:53 left. \n",
      "epoch: 85 train_loss: 0.013299 02:50:40 03:50:54 left. \n",
      "epoch: 86 train_loss: 0.013298 02:52:41 03:48:55 left. \n",
      "epoch: 87 train_loss: 0.013287 02:54:42 03:46:55 left. \n",
      "epoch: 88 train_loss: 0.013283 02:56:40 03:44:51 left. \n",
      "epoch: 89 train_loss: 0.013282 02:58:42 03:42:53 left. \n",
      "epoch: 90 train_loss: 0.013269 03:00:42 03:40:52 left. \n",
      "epoch: 91 train_loss: 0.01326 03:02:41 03:38:49 left. \n",
      "epoch: 92 train_loss: 0.013256 03:04:41 03:36:48 left. \n",
      "epoch: 93 train_loss: 0.013244 03:06:41 03:34:47 left. \n",
      "epoch: 94 train_loss: 0.013243 03:08:42 03:32:48 left. \n",
      "epoch: 95 train_loss: 0.013239 03:10:47 03:30:52 left. \n",
      "epoch: 96 train_loss: 0.013224 03:12:45 03:28:49 left. \n",
      "epoch: 97 train_loss: 0.013224 03:14:45 03:26:48 left. \n",
      "epoch: 98 train_loss: 0.013216 03:16:41 03:24:43 left. \n",
      "epoch: 99 train_loss: 0.013206 03:18:47 03:22:48 left. \n",
      "epoch: 100 train_loss: 0.013196 03:20:48 03:20:48 left. \n",
      "epoch: 101 train_loss: 0.013192 03:22:46 03:18:45 left. \n",
      "epoch: 102 train_loss: 0.013188 03:24:46 03:16:44 left. \n",
      "epoch: 103 train_loss: 0.013177 03:26:47 03:14:44 left. \n",
      "epoch: 104 train_loss: 0.013168 03:28:48 03:12:44 left. \n",
      "epoch: 105 train_loss: 0.013164 03:30:49 03:10:44 left. \n",
      "epoch: 106 train_loss: 0.013161 03:32:53 03:08:47 left. \n",
      "epoch: 107 train_loss: 0.013145 03:34:56 03:06:49 left. \n",
      "epoch: 108 train_loss: 0.013145 03:36:57 03:04:48 left. \n",
      "epoch: 109 train_loss: 0.013132 03:38:59 03:02:49 left. \n",
      "epoch: 110 train_loss: 0.013126 03:40:59 03:00:48 left. \n",
      "epoch: 111 train_loss: 0.013119 03:42:59 02:58:47 left. \n",
      "epoch: 112 train_loss: 0.013107 03:44:59 02:56:47 left. \n",
      "epoch: 113 train_loss: 0.013103 03:47:00 02:54:46 left. \n",
      "epoch: 114 train_loss: 0.013098 03:49:01 02:52:46 left. \n",
      "epoch: 115 train_loss: 0.013089 03:51:00 02:50:44 left. \n",
      "epoch: 116 train_loss: 0.013084 03:53:02 02:48:45 left. \n",
      "epoch: 117 train_loss: 0.013076 03:55:02 02:46:44 left. \n",
      "epoch: 118 train_loss: 0.013063 03:57:05 02:44:45 left. \n",
      "epoch: 119 train_loss: 0.013055 03:59:04 02:42:43 left. \n",
      "epoch: 120 train_loss: 0.013051 04:01:07 02:40:44 left. \n",
      "epoch: 121 train_loss: 0.013046 04:03:10 02:38:46 left. \n",
      "epoch: 122 train_loss: 0.013034 04:05:10 02:36:45 left. \n",
      "epoch: 123 train_loss: 0.013033 04:07:07 02:34:42 left. \n",
      "epoch: 124 train_loss: 0.013023 04:09:08 02:32:41 left. \n",
      "epoch: 125 train_loss: 0.013007 04:11:08 02:30:41 left. \n",
      "epoch: 126 train_loss: 0.013006 04:13:07 02:28:39 left. \n",
      "epoch: 127 train_loss: 0.012996 04:15:04 02:26:37 left. \n",
      "epoch: 128 train_loss: 0.012988 04:17:09 02:24:39 left. \n",
      "epoch: 129 train_loss: 0.012983 04:19:09 02:22:38 left. \n",
      "epoch: 130 train_loss: 0.012972 04:21:10 02:20:38 left. \n",
      "epoch: 131 train_loss: 0.012966 04:23:13 02:18:38 left. \n",
      "epoch: 132 train_loss: 0.01296 04:25:13 02:16:37 left. \n",
      "epoch: 133 train_loss: 0.012944 04:27:14 02:14:37 left. \n",
      "epoch: 134 train_loss: 0.012944 04:29:15 02:12:37 left. \n",
      "epoch: 135 train_loss: 0.012933 04:31:15 02:10:36 left. \n",
      "epoch: 136 train_loss: 0.012923 04:33:17 02:08:36 left. \n",
      "epoch: 137 train_loss: 0.012921 04:35:15 02:06:34 left. \n",
      "epoch: 138 train_loss: 0.012905 04:37:16 02:04:34 left. \n",
      "epoch: 139 train_loss: 0.012898 04:39:17 02:02:33 left. \n",
      "epoch: 140 train_loss: 0.012893 04:41:17 02:00:33 left. \n",
      "epoch: 141 train_loss: 0.012883 04:43:22 01:58:34 left. \n",
      "epoch: 142 train_loss: 0.012873 04:45:24 01:56:34 left. \n",
      "epoch: 143 train_loss: 0.012868 04:47:24 01:54:33 left. \n",
      "epoch: 144 train_loss: 0.012859 04:49:26 01:52:33 left. \n",
      "epoch: 145 train_loss: 0.012848 04:51:27 01:50:33 left. \n",
      "epoch: 146 train_loss: 0.012845 04:53:26 01:48:32 left. \n",
      "epoch: 147 train_loss: 0.012831 04:55:26 01:46:31 left. \n",
      "epoch: 148 train_loss: 0.012826 04:57:28 01:44:31 left. \n",
      "epoch: 149 train_loss: 0.012818 04:59:26 01:42:29 left. \n",
      "epoch: 150 train_loss: 0.012806 05:01:28 01:40:29 left. \n",
      "epoch: 151 train_loss: 0.012798 05:03:30 01:38:29 left. \n",
      "epoch: 152 train_loss: 0.012796 05:05:30 01:36:28 left. \n",
      "epoch: 153 train_loss: 0.012786 05:07:31 01:34:28 left. \n",
      "epoch: 154 train_loss: 0.012778 05:09:33 01:32:28 left. \n",
      "epoch: 155 train_loss: 0.012765 05:11:32 01:30:26 left. \n",
      "epoch: 156 train_loss: 0.012764 05:13:32 01:28:26 left. \n",
      "epoch: 157 train_loss: 0.012754 05:15:32 01:26:25 left. \n",
      "epoch: 158 train_loss: 0.012748 05:17:32 01:24:24 left. \n",
      "epoch: 159 train_loss: 0.012739 05:19:32 01:22:23 left. \n",
      "epoch: 160 train_loss: 0.012727 05:21:33 01:20:23 left. \n",
      "epoch: 161 train_loss: 0.012724 05:23:33 01:18:22 left. \n",
      "epoch: 162 train_loss: 0.012716 05:25:34 01:16:22 left. \n",
      "epoch: 163 train_loss: 0.012706 05:27:33 01:14:21 left. \n",
      "epoch: 164 train_loss: 0.012697 05:29:36 01:12:21 left. \n",
      "epoch: 165 train_loss: 0.012692 05:31:39 01:10:21 left. \n",
      "epoch: 166 train_loss: 0.01268 05:33:38 01:08:20 left. \n",
      "epoch: 167 train_loss: 0.012671 05:35:39 01:06:19 left. \n",
      "epoch: 168 train_loss: 0.012666 05:37:40 01:04:19 left. \n",
      "epoch: 169 train_loss: 0.012654 05:39:39 01:02:18 left. \n",
      "epoch: 170 train_loss: 0.012649 05:41:37 01:00:17 left. \n",
      "epoch: 171 train_loss: 0.012642 05:43:39 00:58:16 left. \n",
      "epoch: 172 train_loss: 0.012636 05:45:40 00:56:16 left. \n",
      "epoch: 173 train_loss: 0.01263 05:47:40 00:54:15 left. \n",
      "epoch: 174 train_loss: 0.012618 05:49:39 00:52:14 left. \n",
      "epoch: 175 train_loss: 0.012615 05:51:40 00:50:14 left. \n",
      "epoch: 176 train_loss: 0.012605 05:53:38 00:48:13 left. \n",
      "epoch: 177 train_loss: 0.012595 05:55:40 00:46:13 left. \n",
      "epoch: 178 train_loss: 0.01259 05:57:37 00:44:12 left. \n",
      "epoch: 179 train_loss: 0.012586 05:59:38 00:42:11 left. \n",
      "epoch: 180 train_loss: 0.01258 06:01:38 00:40:10 left. \n",
      "epoch: 181 train_loss: 0.012574 06:03:38 00:38:10 left. \n",
      "epoch: 182 train_loss: 0.012566 06:05:38 00:36:09 left. \n",
      "epoch: 183 train_loss: 0.01256 06:07:39 00:34:09 left. \n",
      "epoch: 184 train_loss: 0.012554 06:09:40 00:32:08 left. \n",
      "epoch: 185 train_loss: 0.012547 06:11:39 00:30:08 left. \n",
      "epoch: 186 train_loss: 0.012539 06:13:28 00:28:06 left. \n",
      "epoch: 187 train_loss: 0.012535 06:15:30 00:26:06 left. \n",
      "epoch: 188 train_loss: 0.012531 06:17:31 00:24:05 left. \n",
      "epoch: 189 train_loss: 0.012529 06:19:32 00:22:05 left. \n",
      "epoch: 190 train_loss: 0.012525 06:21:29 00:20:04 left. \n",
      "epoch: 191 train_loss: 0.012523 06:23:30 00:18:04 left. \n",
      "epoch: 192 train_loss: 0.012517 06:25:29 00:16:03 left. \n",
      "epoch: 193 train_loss: 0.012515 06:27:29 00:14:03 left. \n",
      "epoch: 194 train_loss: 0.012512 06:29:28 00:12:02 left. \n",
      "epoch: 195 train_loss: 0.012511 06:31:26 00:10:02 left. \n",
      "epoch: 196 train_loss: 0.012508 06:33:26 00:08:01 left. \n",
      "epoch: 197 train_loss: 0.012506 06:35:29 00:06:01 left. \n",
      "epoch: 198 train_loss: 0.012506 06:37:29 00:04:00 left. \n",
      "epoch: 199 train_loss: 0.012504 06:39:29 00:02:00 left. \n",
      "epoch: 200 train_loss: 0.012504 06:41:26 00:00:00 left. \n",
      "Start SubNet warmup\n",
      "epoch: 1 train_loss: 0.013596 Test Acc.：246363/255000(96.61%),252387/255000(98.98%) 0.013118 00:02:59 09:54:33 left. \n",
      "epoch: 2 train_loss: 0.013101 Test Acc.：246809/255000(96.79%),252913/255000(99.18%) 0.013007 00:05:59 09:53:03 left. \n",
      "epoch: 3 train_loss: 0.013035 Test Acc.：246685/255000(96.74%),252737/255000(99.11%) 0.013014 00:08:59 09:50:43 left. \n",
      "epoch: 4 train_loss: 0.01301 Test Acc.：246988/255000(96.86%),253120/255000(99.26%) 0.012983 00:11:59 09:47:41 left. \n",
      "epoch: 5 train_loss: 0.012988 Test Acc.：247233/255000(96.95%),253370/255000(99.36%) 0.012882 00:15:00 09:45:29 left. \n",
      "epoch: 6 train_loss: 0.012967 Test Acc.：247205/255000(96.94%),253326/255000(99.34%) 0.012892 00:18:02 09:43:36 left. \n",
      "epoch: 7 train_loss: 0.012962 Test Acc.：246400/255000(96.63%),252547/255000(99.04%) 0.013055 00:21:03 09:40:34 left. \n",
      "epoch: 8 train_loss: 0.012935 Test Acc.：247232/255000(96.95%),253390/255000(99.37%) 0.012897 00:24:01 09:36:38 left. \n",
      "epoch: 9 train_loss: 0.012984 Test Acc.：247356/255000(97.00%),253541/255000(99.43%) 0.01287 00:26:59 09:32:39 left. \n",
      "epoch: 10 train_loss: 0.012943 Test Acc.：245023/255000(96.09%),250823/255000(98.36%) 0.013382 00:29:59 09:29:47 left. \n",
      "epoch: 11 train_loss: 0.01292 Test Acc.：247316/255000(96.99%),253473/255000(99.40%) 0.012865 00:33:00 09:27:15 left. \n",
      "epoch: 12 train_loss: 0.012924 Test Acc.：247197/255000(96.94%),253231/255000(99.31%) 0.012878 00:36:01 09:24:26 left. \n",
      "epoch: 13 train_loss: 0.012889 Test Acc.：247387/255000(97.01%),253573/255000(99.44%) 0.012855 00:39:01 09:21:14 left. \n",
      "epoch: 14 train_loss: 0.012894 Test Acc.：247472/255000(97.05%),253624/255000(99.46%) 0.012831 00:42:01 09:18:16 left. \n",
      "epoch: 15 train_loss: 0.012879 Test Acc.：247410/255000(97.02%),253566/255000(99.44%) 0.012858 00:45:03 09:15:42 left. \n",
      "epoch: 16 train_loss: 0.012908 Test Acc.：247272/255000(96.97%),253442/255000(99.39%) 0.012875 00:48:06 09:13:14 left. \n",
      "epoch: 17 train_loss: 0.012895 Test Acc.：247089/255000(96.90%),253176/255000(99.28%) 0.012902 00:51:04 09:09:51 left. \n",
      "epoch: 18 train_loss: 0.012878 Test Acc.：247286/255000(96.97%),253460/255000(99.40%) 0.012876 00:54:03 09:06:38 left. \n",
      "epoch: 19 train_loss: 0.012874 Test Acc.：247579/255000(97.09%),253765/255000(99.52%) 0.012813 00:57:04 09:03:40 left. \n",
      "epoch: 20 train_loss: 0.012872 Test Acc.：247562/255000(97.08%),253822/255000(99.54%) 0.012812 01:00:01 09:00:14 left. \n",
      "epoch: 21 train_loss: 0.012878 Test Acc.：247349/255000(97.00%),253418/255000(99.38%) 0.012859 01:03:00 08:57:02 left. \n",
      "epoch: 22 train_loss: 0.012872 Test Acc.：247159/255000(96.93%),253254/255000(99.32%) 0.012902 01:05:59 08:53:56 left. \n",
      "epoch: 23 train_loss: 0.012872 Test Acc.：247269/255000(96.97%),253481/255000(99.40%) 0.012913 01:09:00 08:51:01 left. \n",
      "epoch: 24 train_loss: 0.012886 Test Acc.：247516/255000(97.07%),253733/255000(99.50%) 0.012821 01:11:59 08:47:55 left. \n",
      "epoch: 25 train_loss: 0.012878 Test Acc.：247179/255000(96.93%),253322/255000(99.34%) 0.012934 01:15:00 08:45:04 left. \n",
      "epoch: 26 train_loss: 0.012867 Test Acc.：247427/255000(97.03%),253657/255000(99.47%) 0.012853 01:17:59 08:41:58 left. \n",
      "epoch: 27 train_loss: 0.01289 Test Acc.：247444/255000(97.04%),253653/255000(99.47%) 0.012838 01:20:58 08:38:53 left. \n",
      "epoch: 28 train_loss: 0.012885 Test Acc.：247243/255000(96.96%),253299/255000(99.33%) 0.012877 01:23:59 08:35:55 left. \n",
      "epoch: 29 train_loss: 0.012906 Test Acc.：247321/255000(96.99%),253471/255000(99.40%) 0.012851 01:26:57 08:32:42 left. \n",
      "epoch: 30 train_loss: 0.012902 Test Acc.：247330/255000(96.99%),253480/255000(99.40%) 0.012889 01:29:58 08:29:54 left. \n",
      "epoch: 31 train_loss: 0.012876 Test Acc.：247266/255000(96.97%),253468/255000(99.40%) 0.012907 01:32:56 08:26:42 left. \n",
      "epoch: 32 train_loss: 0.012891 Test Acc.：247131/255000(96.91%),253182/255000(99.29%) 0.012902 01:35:54 08:23:33 left. \n",
      "epoch: 33 train_loss: 0.012896 Test Acc.：245156/255000(96.14%),250753/255000(98.33%) 0.013317 01:38:53 08:20:28 left. \n",
      "epoch: 34 train_loss: 0.01289 Test Acc.：246764/255000(96.77%),252612/255000(99.06%) 0.012952 01:41:50 08:17:11 left. \n",
      "epoch: 35 train_loss: 0.012878 Test Acc.：247185/255000(96.94%),253305/255000(99.34%) 0.012896 01:44:48 08:14:03 left. \n",
      "epoch: 36 train_loss: 0.012856 Test Acc.：247530/255000(97.07%),253721/255000(99.50%) 0.012824 01:47:48 08:11:07 left. \n",
      "epoch: 37 train_loss: 0.012857 Test Acc.：247332/255000(96.99%),253501/255000(99.41%) 0.012885 01:50:47 08:08:05 left. \n",
      "epoch: 38 train_loss: 0.012849 Test Acc.：247465/255000(97.05%),253587/255000(99.45%) 0.012835 01:53:47 08:05:07 left. \n",
      "epoch: 39 train_loss: 0.012853 Test Acc.：247543/255000(97.08%),253821/255000(99.54%) 0.012813 01:56:47 08:02:09 left. \n",
      "epoch: 40 train_loss: 0.012849 Test Acc.：247639/255000(97.11%),253837/255000(99.54%) 0.012793 01:59:44 07:58:56 left. \n",
      "epoch: 41 train_loss: 0.012855 Test Acc.：247575/255000(97.09%),253784/255000(99.52%) 0.01283 02:02:44 07:55:59 left. \n",
      "epoch: 42 train_loss: 0.012865 Test Acc.：247253/255000(96.96%),253425/255000(99.38%) 0.012892 02:05:40 07:52:47 left. \n",
      "epoch: 43 train_loss: 0.012866 Test Acc.：247488/255000(97.05%),253726/255000(99.50%) 0.012828 02:08:37 07:49:39 left. \n",
      "epoch: 44 train_loss: 0.012855 Test Acc.：247363/255000(97.01%),253523/255000(99.42%) 0.012856 02:11:35 07:46:32 left. \n",
      "epoch: 45 train_loss: 0.012844 Test Acc.：247514/255000(97.06%),253783/255000(99.52%) 0.012838 02:14:26 07:43:02 left. \n",
      "epoch: 46 train_loss: 0.012867 Test Acc.：247504/255000(97.06%),253711/255000(99.49%) 0.012809 02:17:25 07:40:03 left. \n",
      "epoch: 47 train_loss: 0.012835 Test Acc.：247688/255000(97.13%),253895/255000(99.57%) 0.012771 02:20:22 07:36:59 left. \n",
      "epoch: 48 train_loss: 0.012845 Test Acc.：247347/255000(97.00%),253471/255000(99.40%) 0.012876 02:23:18 07:33:47 left. \n",
      "epoch: 49 train_loss: 0.012833 Test Acc.：247614/255000(97.10%),253775/255000(99.52%) 0.012809 02:26:18 07:30:50 left. \n",
      "epoch: 50 train_loss: 0.012851 Test Acc.：247589/255000(97.09%),253804/255000(99.53%) 0.01281 02:29:16 07:27:49 left. \n",
      "epoch: 51 train_loss: 0.012851 Test Acc.：247395/255000(97.02%),253587/255000(99.45%) 0.012852 02:32:16 07:24:53 left. \n",
      "epoch: 52 train_loss: 0.012825 Test Acc.：247497/255000(97.06%),253687/255000(99.49%) 0.012844 02:35:16 07:21:56 left. \n",
      "epoch: 53 train_loss: 0.012866 Test Acc.：247397/255000(97.02%),253548/255000(99.43%) 0.012845 02:38:17 07:19:02 left. \n",
      "epoch: 54 train_loss: 0.012834 Test Acc.：247316/255000(96.99%),253488/255000(99.41%) 0.012887 02:41:17 07:16:06 left. \n",
      "epoch: 55 train_loss: 0.012817 Test Acc.：247282/255000(96.97%),253529/255000(99.42%) 0.01285 02:44:18 07:13:11 left. \n",
      "epoch: 56 train_loss: 0.012832 Test Acc.：247007/255000(96.87%),253038/255000(99.23%) 0.012908 02:47:19 07:10:15 left. \n",
      "epoch: 57 train_loss: 0.012842 Test Acc.：247327/255000(96.99%),253561/255000(99.44%) 0.012868 02:50:18 07:07:16 left. \n",
      "epoch: 58 train_loss: 0.01282 Test Acc.：247592/255000(97.09%),253847/255000(99.55%) 0.012825 02:53:21 07:04:25 left. \n",
      "epoch: 59 train_loss: 0.012854 Test Acc.：247290/255000(96.98%),253263/255000(99.32%) 0.012861 02:56:21 07:01:27 left. \n",
      "epoch: 60 train_loss: 0.012839 Test Acc.：247412/255000(97.02%),253615/255000(99.46%) 0.012858 02:59:22 06:58:31 left. \n",
      "epoch: 61 train_loss: 0.012821 Test Acc.：247589/255000(97.09%),253767/255000(99.52%) 0.012818 03:02:23 06:55:37 left. \n",
      "epoch: 62 train_loss: 0.012833 Test Acc.：247672/255000(97.13%),253890/255000(99.56%) 0.012798 03:05:16 06:52:23 left. \n",
      "epoch: 63 train_loss: 0.012831 Test Acc.：247494/255000(97.06%),253695/255000(99.49%) 0.012847 03:08:17 06:49:26 left. \n",
      "epoch: 64 train_loss: 0.01284 Test Acc.：247557/255000(97.08%),253837/255000(99.54%) 0.01281 03:11:13 06:46:21 left. \n",
      "epoch: 65 train_loss: 0.012818 Test Acc.：247564/255000(97.08%),253782/255000(99.52%) 0.012811 03:14:16 06:43:29 left. \n",
      "epoch: 66 train_loss: 0.01284 Test Acc.：247514/255000(97.06%),253696/255000(99.49%) 0.012822 03:17:10 06:40:20 left. \n",
      "epoch: 67 train_loss: 0.012824 Test Acc.：247481/255000(97.05%),253695/255000(99.49%) 0.012802 03:20:10 06:37:22 left. \n",
      "epoch: 68 train_loss: 0.012824 Test Acc.：247446/255000(97.04%),253543/255000(99.43%) 0.012833 03:23:12 06:34:27 left. \n",
      "epoch: 69 train_loss: 0.01281 Test Acc.：247511/255000(97.06%),253735/255000(99.50%) 0.012806 03:26:12 06:31:30 left. \n",
      "epoch: 70 train_loss: 0.012831 Test Acc.：247593/255000(97.10%),253856/255000(99.55%) 0.012801 03:29:13 06:28:34 left. \n",
      "epoch: 71 train_loss: 0.012818 Test Acc.：247678/255000(97.13%),253945/255000(99.59%) 0.012791 03:32:13 06:25:36 left. \n",
      "epoch: 72 train_loss: 0.012804 Test Acc.：247560/255000(97.08%),253774/255000(99.52%) 0.012807 03:35:10 06:22:31 left. \n",
      "epoch: 73 train_loss: 0.01282 Test Acc.：247342/255000(97.00%),253484/255000(99.41%) 0.012853 03:38:09 06:19:32 left. \n",
      "epoch: 74 train_loss: 0.012812 Test Acc.：247164/255000(96.93%),253229/255000(99.31%) 0.012889 03:41:11 06:16:37 left. \n",
      "epoch: 75 train_loss: 0.012839 Test Acc.：247577/255000(97.09%),253806/255000(99.53%) 0.012796 03:44:10 06:13:37 left. \n",
      "epoch: 76 train_loss: 0.01284 Test Acc.：247612/255000(97.10%),253840/255000(99.55%) 0.012803 03:47:11 06:10:40 left. \n",
      "epoch: 77 train_loss: 0.012794 Test Acc.：247759/255000(97.16%),254008/255000(99.61%) 0.012776 03:50:12 06:07:43 left. \n",
      "epoch: 78 train_loss: 0.012802 Test Acc.：247657/255000(97.12%),253889/255000(99.56%) 0.012799 03:53:11 06:04:44 left. \n",
      "epoch: 79 train_loss: 0.012811 Test Acc.：247425/255000(97.03%),253619/255000(99.46%) 0.012841 03:56:15 06:01:51 left. \n",
      "epoch: 80 train_loss: 0.01279 Test Acc.：247612/255000(97.10%),253838/255000(99.54%) 0.012794 03:59:13 05:58:50 left. \n",
      "epoch: 81 train_loss: 0.012788 Test Acc.：247586/255000(97.09%),253832/255000(99.54%) 0.01282 04:02:15 05:55:54 left. \n",
      "epoch: 82 train_loss: 0.012813 Test Acc.：247539/255000(97.07%),253709/255000(99.49%) 0.012826 04:05:13 05:52:53 left. \n",
      "epoch: 83 train_loss: 0.012796 Test Acc.：247530/255000(97.07%),253785/255000(99.52%) 0.012807 04:08:15 05:49:57 left. \n",
      "epoch: 84 train_loss: 0.012801 Test Acc.：247649/255000(97.12%),253843/255000(99.55%) 0.012777 04:11:14 05:46:56 left. \n",
      "epoch: 85 train_loss: 0.012786 Test Acc.：247717/255000(97.14%),253960/255000(99.59%) 0.012782 04:14:11 05:43:53 left. \n",
      "epoch: 86 train_loss: 0.012789 Test Acc.：247673/255000(97.13%),253935/255000(99.58%) 0.012785 04:17:10 05:40:54 left. \n",
      "epoch: 87 train_loss: 0.012783 Test Acc.：247609/255000(97.10%),253765/255000(99.52%) 0.012793 04:20:13 05:37:59 left. \n",
      "epoch: 88 train_loss: 0.012787 Test Acc.：247204/255000(96.94%),253136/255000(99.27%) 0.012886 04:23:12 05:34:59 left. \n",
      "epoch: 89 train_loss: 0.012777 Test Acc.：247460/255000(97.04%),253703/255000(99.49%) 0.012844 04:26:10 05:31:57 left. \n",
      "epoch: 90 train_loss: 0.012786 Test Acc.：247597/255000(97.10%),253848/255000(99.55%) 0.012799 04:29:08 05:28:57 left. \n",
      "epoch: 91 train_loss: 0.012775 Test Acc.：247128/255000(96.91%),253240/255000(99.31%) 0.012937 04:32:09 05:25:58 left. \n",
      "epoch: 92 train_loss: 0.01279 Test Acc.：247695/255000(97.14%),253878/255000(99.56%) 0.012767 04:35:11 05:23:03 left. \n",
      "epoch: 93 train_loss: 0.01281 Test Acc.：247473/255000(97.05%),253662/255000(99.48%) 0.012817 04:38:09 05:20:02 left. \n",
      "epoch: 94 train_loss: 0.012779 Test Acc.：247641/255000(97.11%),253869/255000(99.56%) 0.012806 04:41:11 05:17:05 left. \n",
      "epoch: 95 train_loss: 0.012772 Test Acc.：247686/255000(97.13%),253918/255000(99.58%) 0.012785 04:44:08 05:14:02 left. \n",
      "epoch: 96 train_loss: 0.012798 Test Acc.：247587/255000(97.09%),253743/255000(99.51%) 0.012798 04:47:07 05:11:02 left. \n",
      "epoch: 97 train_loss: 0.012777 Test Acc.：247656/255000(97.12%),253886/255000(99.56%) 0.01279 04:50:05 05:08:02 left. \n",
      "epoch: 98 train_loss: 0.012773 Test Acc.：247486/255000(97.05%),253658/255000(99.47%) 0.012829 04:53:08 05:05:06 left. \n",
      "epoch: 99 train_loss: 0.012782 Test Acc.：247588/255000(97.09%),253790/255000(99.53%) 0.012803 04:56:09 05:02:08 left. \n",
      "epoch: 100 train_loss: 0.012756 Test Acc.：247694/255000(97.13%),253909/255000(99.57%) 0.012775 04:59:08 04:59:08 left. \n",
      "epoch: 101 train_loss: 0.012761 Test Acc.：247699/255000(97.14%),253955/255000(99.59%) 0.012776 05:02:07 04:56:08 left. \n",
      "epoch: 102 train_loss: 0.012762 Test Acc.：247666/255000(97.12%),253901/255000(99.57%) 0.012787 05:05:07 04:53:09 left. \n",
      "epoch: 103 train_loss: 0.012762 Test Acc.：247683/255000(97.13%),253978/255000(99.60%) 0.012766 05:08:06 04:50:09 left. \n",
      "epoch: 104 train_loss: 0.012763 Test Acc.：247673/255000(97.13%),253953/255000(99.59%) 0.012788 05:11:05 04:47:09 left. \n",
      "epoch: 105 train_loss: 0.012752 Test Acc.：247684/255000(97.13%),253874/255000(99.56%) 0.012782 05:14:04 04:44:10 left. \n",
      "epoch: 106 train_loss: 0.012755 Test Acc.：247688/255000(97.13%),253945/255000(99.59%) 0.012786 05:17:06 04:41:12 left. \n",
      "epoch: 107 train_loss: 0.012747 Test Acc.：247764/255000(97.16%),254020/255000(99.62%) 0.012748 05:20:09 04:38:16 left. \n",
      "epoch: 108 train_loss: 0.012751 Test Acc.：247740/255000(97.15%),253948/255000(99.59%) 0.012764 05:23:07 04:35:15 left. \n",
      "epoch: 109 train_loss: 0.012762 Test Acc.：247715/255000(97.14%),253906/255000(99.57%) 0.012758 05:26:08 04:32:17 left. \n",
      "epoch: 110 train_loss: 0.012741 Test Acc.：247733/255000(97.15%),253982/255000(99.60%) 0.012743 05:29:09 04:29:18 left. \n",
      "epoch: 111 train_loss: 0.012752 Test Acc.：247421/255000(97.03%),253541/255000(99.43%) 0.012843 05:32:08 04:26:18 left. \n",
      "epoch: 112 train_loss: 0.012749 Test Acc.：247796/255000(97.17%),254051/255000(99.63%) 0.012745 05:35:07 04:23:18 left. \n",
      "epoch: 113 train_loss: 0.012742 Test Acc.：247820/255000(97.18%),254103/255000(99.65%) 0.012736 05:38:05 04:20:17 left. \n",
      "epoch: 114 train_loss: 0.012729 Test Acc.：247767/255000(97.16%),253984/255000(99.60%) 0.012766 05:41:05 04:17:19 left. \n",
      "epoch: 115 train_loss: 0.012729 Test Acc.：247649/255000(97.12%),253921/255000(99.58%) 0.012766 05:44:06 04:14:20 left. \n",
      "epoch: 116 train_loss: 0.012732 Test Acc.：247784/255000(97.17%),254045/255000(99.63%) 0.012758 05:47:04 04:11:19 left. \n",
      "epoch: 117 train_loss: 0.012719 Test Acc.：247710/255000(97.14%),253929/255000(99.58%) 0.012765 05:50:04 04:08:20 left. \n",
      "epoch: 118 train_loss: 0.012719 Test Acc.：247876/255000(97.21%),254148/255000(99.67%) 0.012742 05:53:02 04:05:20 left. \n",
      "epoch: 119 train_loss: 0.012725 Test Acc.：247666/255000(97.12%),253912/255000(99.57%) 0.012796 05:55:59 04:02:19 left. \n",
      "epoch: 120 train_loss: 0.012737 Test Acc.：247842/255000(97.19%),254095/255000(99.65%) 0.012753 05:59:01 03:59:20 left. \n",
      "epoch: 121 train_loss: 0.012714 Test Acc.：247805/255000(97.18%),254063/255000(99.63%) 0.01274 06:02:03 03:56:22 left. \n",
      "epoch: 122 train_loss: 0.012712 Test Acc.：247685/255000(97.13%),253900/255000(99.57%) 0.012774 06:04:58 03:53:20 left. \n",
      "epoch: 123 train_loss: 0.012708 Test Acc.：247746/255000(97.16%),253967/255000(99.59%) 0.012778 06:07:56 03:50:20 left. \n",
      "epoch: 124 train_loss: 0.012703 Test Acc.：247840/255000(97.19%),254105/255000(99.65%) 0.012726 06:10:57 03:47:21 left. \n",
      "epoch: 125 train_loss: 0.012711 Test Acc.：247808/255000(97.18%),254044/255000(99.63%) 0.012743 06:14:04 03:44:26 left. \n",
      "epoch: 126 train_loss: 0.012708 Test Acc.：247754/255000(97.16%),254033/255000(99.62%) 0.01275 06:17:13 03:41:32 left. \n",
      "epoch: 127 train_loss: 0.012703 Test Acc.：247772/255000(97.17%),253987/255000(99.60%) 0.012738 06:20:22 03:38:38 left. \n",
      "epoch: 128 train_loss: 0.012703 Test Acc.：247833/255000(97.19%),254050/255000(99.63%) 0.012737 06:23:33 03:35:45 left. \n",
      "epoch: 129 train_loss: 0.012696 Test Acc.：247662/255000(97.12%),253850/255000(99.55%) 0.012777 06:26:42 03:32:50 left. \n",
      "epoch: 130 train_loss: 0.012698 Test Acc.：247886/255000(97.21%),254111/255000(99.65%) 0.012722 06:29:52 03:29:55 left. \n",
      "epoch: 131 train_loss: 0.012692 Test Acc.：247830/255000(97.19%),254087/255000(99.64%) 0.012738 06:33:09 03:27:04 left. \n",
      "epoch: 132 train_loss: 0.012698 Test Acc.：247831/255000(97.19%),254086/255000(99.64%) 0.012756 06:36:20 03:24:10 left. \n",
      "epoch: 133 train_loss: 0.012696 Test Acc.：247877/255000(97.21%),254103/255000(99.65%) 0.012744 06:39:28 03:21:14 left. \n",
      "epoch: 134 train_loss: 0.012689 Test Acc.：247795/255000(97.17%),254012/255000(99.61%) 0.012742 06:42:39 03:18:19 left. \n",
      "epoch: 135 train_loss: 0.012694 Test Acc.：247889/255000(97.21%),254134/255000(99.66%) 0.012721 06:45:47 03:15:22 left. \n",
      "epoch: 136 train_loss: 0.012686 Test Acc.：247912/255000(97.22%),254193/255000(99.68%) 0.012729 06:49:01 03:12:28 left. \n",
      "epoch: 137 train_loss: 0.012686 Test Acc.：247570/255000(97.09%),253773/255000(99.52%) 0.012795 06:52:08 03:09:31 left. \n",
      "epoch: 138 train_loss: 0.012682 Test Acc.：247764/255000(97.16%),254040/255000(99.62%) 0.012749 06:55:18 03:06:35 left. \n",
      "epoch: 139 train_loss: 0.012677 Test Acc.：247805/255000(97.18%),254106/255000(99.65%) 0.01275 06:58:29 03:03:39 left. \n",
      "epoch: 140 train_loss: 0.012679 Test Acc.：247835/255000(97.19%),254081/255000(99.64%) 0.012749 07:01:39 03:00:42 left. \n",
      "epoch: 141 train_loss: 0.012675 Test Acc.：247882/255000(97.21%),254083/255000(99.64%) 0.012732 07:04:47 02:57:45 left. \n",
      "epoch: 142 train_loss: 0.012661 Test Acc.：247826/255000(97.19%),254084/255000(99.64%) 0.012728 07:07:57 02:54:47 left. \n",
      "epoch: 143 train_loss: 0.012666 Test Acc.：247834/255000(97.19%),254107/255000(99.65%) 0.012723 07:11:05 02:51:50 left. \n",
      "epoch: 144 train_loss: 0.012664 Test Acc.：247950/255000(97.24%),254179/255000(99.68%) 0.012729 07:14:20 02:48:54 left. \n",
      "epoch: 145 train_loss: 0.012663 Test Acc.：247897/255000(97.21%),254171/255000(99.67%) 0.012734 07:17:31 02:45:57 left. \n",
      "epoch: 146 train_loss: 0.012669 Test Acc.：247881/255000(97.21%),254149/255000(99.67%) 0.012717 07:20:44 02:43:01 left. \n",
      "epoch: 147 train_loss: 0.012661 Test Acc.：247859/255000(97.20%),254058/255000(99.63%) 0.012722 07:23:55 02:40:03 left. \n",
      "epoch: 148 train_loss: 0.012657 Test Acc.：247941/255000(97.23%),254153/255000(99.67%) 0.012717 07:27:06 02:37:05 left. \n",
      "epoch: 149 train_loss: 0.012654 Test Acc.：247617/255000(97.10%),253826/255000(99.54%) 0.012786 07:30:17 02:34:07 left. \n",
      "epoch: 150 train_loss: 0.012655 Test Acc.：247946/255000(97.23%),254181/255000(99.68%) 0.012709 07:33:26 02:31:08 left. \n",
      "epoch: 151 train_loss: 0.012643 Test Acc.：247931/255000(97.23%),254191/255000(99.68%) 0.01272 07:36:35 02:28:09 left. \n",
      "epoch: 152 train_loss: 0.012644 Test Acc.：247955/255000(97.24%),254206/255000(99.69%) 0.012707 07:39:50 02:25:12 left. \n",
      "epoch: 153 train_loss: 0.012646 Test Acc.：247948/255000(97.23%),254229/255000(99.70%) 0.012705 07:43:00 02:22:13 left. \n",
      "epoch: 154 train_loss: 0.012643 Test Acc.：247979/255000(97.25%),254215/255000(99.69%) 0.0127 07:46:08 02:19:14 left. \n",
      "epoch: 155 train_loss: 0.01264 Test Acc.：247867/255000(97.20%),254109/255000(99.65%) 0.012752 07:49:19 02:16:15 left. \n",
      "epoch: 156 train_loss: 0.012641 Test Acc.：247996/255000(97.25%),254235/255000(99.70%) 0.012696 07:52:32 02:13:16 left. \n",
      "epoch: 157 train_loss: 0.012629 Test Acc.：247928/255000(97.23%),254162/255000(99.67%) 0.012735 07:55:47 02:10:18 left. \n",
      "epoch: 158 train_loss: 0.012637 Test Acc.：247943/255000(97.23%),254248/255000(99.71%) 0.012733 07:58:57 02:07:19 left. \n",
      "epoch: 159 train_loss: 0.01263 Test Acc.：247960/255000(97.24%),254228/255000(99.70%) 0.012716 08:02:10 02:04:20 left. \n",
      "epoch: 160 train_loss: 0.012628 Test Acc.：247950/255000(97.24%),254205/255000(99.69%) 0.012704 08:05:26 02:01:21 left. \n",
      "epoch: 161 train_loss: 0.012631 Test Acc.：247941/255000(97.23%),254160/255000(99.67%) 0.012714 08:08:34 01:58:21 left. \n",
      "epoch: 162 train_loss: 0.012624 Test Acc.：247960/255000(97.24%),254231/255000(99.70%) 0.012703 08:11:42 01:55:20 left. \n",
      "epoch: 163 train_loss: 0.012625 Test Acc.：248029/255000(97.27%),254229/255000(99.70%) 0.012701 08:14:51 01:52:19 left. \n",
      "epoch: 164 train_loss: 0.012617 Test Acc.：247999/255000(97.25%),254241/255000(99.70%) 0.012702 08:17:58 01:49:18 left. \n",
      "epoch: 165 train_loss: 0.01262 Test Acc.：248029/255000(97.27%),254280/255000(99.72%) 0.012685 08:21:12 01:46:18 left. \n",
      "epoch: 166 train_loss: 0.012619 Test Acc.：248052/255000(97.28%),254283/255000(99.72%) 0.0127 08:24:25 01:43:18 left. \n",
      "epoch: 167 train_loss: 0.01261 Test Acc.：247954/255000(97.24%),254214/255000(99.69%) 0.012716 08:27:34 01:40:18 left. \n",
      "epoch: 168 train_loss: 0.012608 Test Acc.：248009/255000(97.26%),254265/255000(99.71%) 0.012687 08:30:45 01:37:17 left. \n",
      "epoch: 169 train_loss: 0.012609 Test Acc.：247992/255000(97.25%),254243/255000(99.70%) 0.012713 08:33:49 01:34:15 left. \n",
      "epoch: 170 train_loss: 0.01261 Test Acc.：248024/255000(97.26%),254278/255000(99.72%) 0.012706 08:36:58 01:31:13 left. \n",
      "epoch: 171 train_loss: 0.012606 Test Acc.：247998/255000(97.25%),254272/255000(99.71%) 0.0127 08:40:07 01:28:12 left. \n",
      "epoch: 172 train_loss: 0.012605 Test Acc.：248062/255000(97.28%),254282/255000(99.72%) 0.012691 08:43:16 01:25:11 left. \n",
      "epoch: 173 train_loss: 0.012602 Test Acc.：248001/255000(97.26%),254246/255000(99.70%) 0.012703 08:46:27 01:22:09 left. \n",
      "epoch: 174 train_loss: 0.012599 Test Acc.：248021/255000(97.26%),254280/255000(99.72%) 0.012696 08:49:31 01:19:07 left. \n",
      "epoch: 175 train_loss: 0.012598 Test Acc.：248013/255000(97.26%),254276/255000(99.72%) 0.012689 08:52:40 01:16:05 left. \n",
      "epoch: 176 train_loss: 0.012596 Test Acc.：248025/255000(97.26%),254282/255000(99.72%) 0.012692 08:55:51 01:13:04 left. \n",
      "epoch: 177 train_loss: 0.01259 Test Acc.：247985/255000(97.25%),254223/255000(99.70%) 0.012692 08:59:03 01:10:02 left. \n",
      "epoch: 178 train_loss: 0.012596 Test Acc.：248050/255000(97.27%),254296/255000(99.72%) 0.012701 09:02:11 01:07:00 left. \n",
      "epoch: 179 train_loss: 0.012593 Test Acc.：248046/255000(97.27%),254289/255000(99.72%) 0.012696 09:05:22 01:03:58 left. \n",
      "epoch: 180 train_loss: 0.012594 Test Acc.：248009/255000(97.26%),254270/255000(99.71%) 0.012697 09:08:34 01:00:57 left. \n",
      "epoch: 181 train_loss: 0.012591 Test Acc.：248037/255000(97.27%),254277/255000(99.72%) 0.012692 09:11:47 00:57:55 left. \n",
      "epoch: 182 train_loss: 0.012591 Test Acc.：248035/255000(97.27%),254275/255000(99.72%) 0.012691 09:14:53 00:54:52 left. \n",
      "epoch: 183 train_loss: 0.01259 Test Acc.：248026/255000(97.27%),254284/255000(99.72%) 0.012701 09:18:01 00:51:50 left. \n",
      "epoch: 184 train_loss: 0.012583 Test Acc.：248002/255000(97.26%),254282/255000(99.72%) 0.012697 09:21:11 00:48:47 left. \n",
      "epoch: 185 train_loss: 0.01259 Test Acc.：248083/255000(97.29%),254303/255000(99.73%) 0.012685 09:24:18 00:45:45 left. \n",
      "epoch: 186 train_loss: 0.012583 Test Acc.：248067/255000(97.28%),254303/255000(99.73%) 0.012689 09:27:23 00:42:42 left. \n",
      "epoch: 187 train_loss: 0.012585 Test Acc.：248022/255000(97.26%),254295/255000(99.72%) 0.012693 09:30:38 00:39:40 left. \n",
      "epoch: 188 train_loss: 0.012588 Test Acc.：248001/255000(97.26%),254279/255000(99.72%) 0.012697 09:33:46 00:36:37 left. \n",
      "epoch: 189 train_loss: 0.012584 Test Acc.：248025/255000(97.26%),254295/255000(99.72%) 0.0127 09:36:59 00:33:34 left. \n",
      "epoch: 190 train_loss: 0.012583 Test Acc.：248016/255000(97.26%),254294/255000(99.72%) 0.012702 09:40:07 00:30:31 left. \n",
      "epoch: 191 train_loss: 0.012582 Test Acc.：248031/255000(97.27%),254303/255000(99.73%) 0.012695 09:43:14 00:27:28 left. \n",
      "epoch: 192 train_loss: 0.012582 Test Acc.：248058/255000(97.28%),254305/255000(99.73%) 0.012691 09:46:23 00:24:25 left. \n",
      "epoch: 193 train_loss: 0.012581 Test Acc.：248075/255000(97.28%),254305/255000(99.73%) 0.012686 09:49:30 00:21:22 left. \n",
      "epoch: 194 train_loss: 0.012581 Test Acc.：248033/255000(97.27%),254306/255000(99.73%) 0.012686 09:52:46 00:18:19 left. \n",
      "epoch: 195 train_loss: 0.012582 Test Acc.：248043/255000(97.27%),254307/255000(99.73%) 0.012687 09:55:59 00:15:16 left. \n",
      "epoch: 196 train_loss: 0.012579 Test Acc.：248085/255000(97.29%),254306/255000(99.73%) 0.012701 09:59:11 00:12:13 left. \n",
      "epoch: 197 train_loss: 0.012579 Test Acc.：248057/255000(97.28%),254304/255000(99.73%) 0.012688 10:02:22 00:09:10 left. \n",
      "epoch: 198 train_loss: 0.012585 Test Acc.：248040/255000(97.27%),254306/255000(99.73%) 0.012686 10:05:30 00:06:06 left. \n",
      "epoch: 199 train_loss: 0.012577 Test Acc.：248069/255000(97.28%),254315/255000(99.73%) 0.012698 10:08:40 00:03:03 left. \n",
      "epoch: 200 train_loss: 0.012578 Test Acc.：248063/255000(97.28%),254310/255000(99.73%) 0.012683 10:11:53 00:00:00 left. \n",
      "Test Acc.：248063/255000(97.28%),254310/255000(99.73%) 0.012683 \n",
      "epoch: 1 train_loss: 0.014546 Test Acc.：242674/255000(95.17%),253233/255000(99.31%) 0.013786 Save model 00:03:24 11:19:33 left. \n",
      "epoch: 2 train_loss: 0.013949 Test Acc.：242714/255000(95.18%),253081/255000(99.25%) 0.013679 Save model 00:06:49 11:16:17 left. \n",
      "epoch: 3 train_loss: 0.013816 Test Acc.：243980/255000(95.68%),253418/255000(99.38%) 0.013497 Save model 00:10:09 11:07:23 left. \n",
      "epoch: 4 train_loss: 0.01381 Test Acc.：242820/255000(95.22%),253210/255000(99.30%) 0.013736 Save model 00:13:36 11:06:42 left. \n",
      "epoch: 5 train_loss: 0.013788 Test Acc.：243494/255000(95.49%),253241/255000(99.31%) 0.013664 Save model 00:17:03 11:05:02 left. \n",
      "epoch: 6 train_loss: 0.013784 Test Acc.：243375/255000(95.44%),253283/255000(99.33%) 0.013607 Save model 00:20:25 11:00:13 left. \n",
      "epoch: 7 train_loss: 0.0138 Test Acc.：242942/255000(95.27%),252108/255000(98.87%) 0.013791 00:23:45 10:55:14 left. \n",
      "epoch: 8 train_loss: 0.013771 Test Acc.：243602/255000(95.53%),253105/255000(99.26%) 0.013579 Save model 00:27:10 10:52:22 left. \n",
      "epoch: 9 train_loss: 0.013762 Test Acc.：243365/255000(95.44%),253123/255000(99.26%) 0.013617 Save model 00:30:36 10:49:45 left. \n",
      "epoch: 10 train_loss: 0.01379 Test Acc.：243560/255000(95.51%),252348/255000(98.96%) 0.013587 00:33:58 10:45:40 left. \n",
      "epoch: 11 train_loss: 0.013775 Test Acc.：243973/255000(95.68%),252988/255000(99.21%) 0.013439 00:37:21 10:41:57 left. \n",
      "epoch: 12 train_loss: 0.013766 Test Acc.：243335/255000(95.43%),252601/255000(99.06%) 0.0136 00:40:50 10:39:57 left. \n",
      "epoch: 13 train_loss: 0.01376 Test Acc.：242792/255000(95.21%),252924/255000(99.19%) 0.013625 Save model 00:44:15 10:36:39 left. \n",
      "epoch: 14 train_loss: 0.013754 Test Acc.：243695/255000(95.57%),253099/255000(99.25%) 0.013597 Save model 00:47:42 10:33:44 left. \n",
      "epoch: 15 train_loss: 0.01373 Test Acc.：243955/255000(95.67%),253150/255000(99.27%) 0.013546 Save model 00:51:07 10:30:36 left. \n",
      "epoch: 16 train_loss: 0.013744 Test Acc.：243670/255000(95.56%),252687/255000(99.09%) 0.01354 00:54:35 10:27:43 left. \n",
      "epoch: 17 train_loss: 0.013733 Test Acc.：243766/255000(95.59%),253056/255000(99.24%) 0.013466 00:58:02 10:24:46 left. \n",
      "epoch: 18 train_loss: 0.013741 Test Acc.：243458/255000(95.47%),252994/255000(99.21%) 0.013537 01:01:25 10:21:01 left. \n",
      "epoch: 19 train_loss: 0.013732 Test Acc.：244337/255000(95.82%),253231/255000(99.31%) 0.013474 01:04:45 10:16:55 left. \n",
      "epoch: 20 train_loss: 0.013715 Test Acc.：243419/255000(95.46%),253275/255000(99.32%) 0.013557 Save model 01:08:10 10:13:33 left. \n",
      "epoch: 21 train_loss: 0.013702 Test Acc.：243918/255000(95.65%),252925/255000(99.19%) 0.013568 Save model 01:11:35 10:10:14 left. \n",
      "epoch: 22 train_loss: 0.013716 Test Acc.：243605/255000(95.53%),252926/255000(99.19%) 0.01354 01:14:56 10:06:17 left. \n",
      "epoch: 23 train_loss: 0.013721 Test Acc.：244078/255000(95.72%),253099/255000(99.25%) 0.013489 01:18:21 10:03:03 left. \n",
      "epoch: 24 train_loss: 0.01371 Test Acc.：243703/255000(95.57%),253178/255000(99.29%) 0.013498 01:21:46 09:59:41 left. \n",
      "epoch: 25 train_loss: 0.013714 Test Acc.：243899/255000(95.65%),253028/255000(99.23%) 0.013489 01:25:07 09:55:54 left. \n",
      "epoch: 26 train_loss: 0.013725 Test Acc.：244006/255000(95.69%),252767/255000(99.12%) 0.013505 01:28:37 09:53:05 left. \n",
      "epoch: 27 train_loss: 0.013689 Test Acc.：242994/255000(95.29%),251299/255000(98.55%) 0.013632 Save model 01:32:01 09:49:39 left. \n",
      "epoch: 28 train_loss: 0.013698 Test Acc.：243885/255000(95.64%),253271/255000(99.32%) 0.013617 01:35:26 09:46:15 left. \n",
      "epoch: 29 train_loss: 0.013678 Test Acc.：244267/255000(95.79%),253196/255000(99.29%) 0.01343 Save model 01:38:41 09:41:57 left. \n",
      "epoch: 30 train_loss: 0.013692 Test Acc.：243679/255000(95.56%),253155/255000(99.28%) 0.013522 01:42:07 09:38:40 left. \n",
      "epoch: 31 train_loss: 0.01368 Test Acc.：243655/255000(95.55%),252827/255000(99.15%) 0.013592 01:45:25 09:34:44 left. \n",
      "epoch: 32 train_loss: 0.01367 Test Acc.：244072/255000(95.71%),253264/255000(99.32%) 0.013456 Save model 01:48:51 09:31:30 left. \n",
      "epoch: 33 train_loss: 0.013683 Test Acc.：244025/255000(95.70%),252971/255000(99.20%) 0.013446 01:52:18 09:28:19 left. \n",
      "epoch: 34 train_loss: 0.013674 Test Acc.：243885/255000(95.64%),252824/255000(99.15%) 0.013491 01:55:46 09:25:14 left. \n",
      "epoch: 35 train_loss: 0.013652 Test Acc.：243963/255000(95.67%),252714/255000(99.10%) 0.013547 Save model 01:59:11 09:21:53 left. \n",
      "epoch: 36 train_loss: 0.013673 Test Acc.：243795/255000(95.61%),252737/255000(99.11%) 0.013444 02:02:35 09:18:28 left. \n",
      "epoch: 37 train_loss: 0.013673 Test Acc.：243851/255000(95.63%),252318/255000(98.95%) 0.013502 02:06:01 09:15:10 left. \n",
      "epoch: 38 train_loss: 0.013661 Test Acc.：243927/255000(95.66%),252727/255000(99.11%) 0.013595 02:09:26 09:11:50 left. \n",
      "epoch: 39 train_loss: 0.013656 Test Acc.：243305/255000(95.41%),249692/255000(97.92%) 0.013624 02:12:54 09:08:41 left. \n",
      "epoch: 40 train_loss: 0.013661 Test Acc.：244069/255000(95.71%),252909/255000(99.18%) 0.013587 02:16:20 09:05:20 left. \n",
      "epoch: 41 train_loss: 0.013643 Test Acc.：243404/255000(95.45%),252858/255000(99.16%) 0.01354 Save model 02:19:47 09:02:08 left. \n",
      "epoch: 42 train_loss: 0.01364 Test Acc.：244050/255000(95.71%),253056/255000(99.24%) 0.013491 Save model 02:23:10 08:58:35 left. \n",
      "epoch: 43 train_loss: 0.013634 Test Acc.：244035/255000(95.70%),252736/255000(99.11%) 0.01342 Save model 02:26:40 08:55:33 left. \n",
      "epoch: 44 train_loss: 0.013636 Test Acc.：243519/255000(95.50%),252682/255000(99.09%) 0.01352 02:29:59 08:51:46 left. \n",
      "epoch: 45 train_loss: 0.013617 Test Acc.：244002/255000(95.69%),252705/255000(99.10%) 0.013427 Save model 02:33:20 08:48:11 left. \n",
      "epoch: 46 train_loss: 0.013621 Test Acc.：243979/255000(95.68%),252337/255000(98.96%) 0.013524 02:36:41 08:44:35 left. \n",
      "epoch: 47 train_loss: 0.013612 Test Acc.：244393/255000(95.84%),252858/255000(99.16%) 0.013433 Save model 02:40:06 08:41:13 left. \n",
      "epoch: 48 train_loss: 0.013615 Test Acc.：244106/255000(95.73%),252536/255000(99.03%) 0.013469 02:43:31 08:37:50 left. \n",
      "epoch: 49 train_loss: 0.013598 Test Acc.：243554/255000(95.51%),252557/255000(99.04%) 0.013563 Save model 02:46:56 08:34:26 left. \n",
      "epoch: 50 train_loss: 0.013597 Test Acc.：244041/255000(95.70%),252934/255000(99.19%) 0.01343 Save model 02:50:19 08:30:59 left. \n",
      "epoch: 51 train_loss: 0.013586 Test Acc.：244133/255000(95.74%),252860/255000(99.16%) 0.013399 Save model 02:53:47 08:27:44 left. \n",
      "epoch: 52 train_loss: 0.013589 Test Acc.：244442/255000(95.86%),252791/255000(99.13%) 0.013454 02:57:24 08:24:56 left. \n",
      "epoch: 53 train_loss: 0.013589 Test Acc.：244115/255000(95.73%),252786/255000(99.13%) 0.013451 03:00:44 08:21:18 left. \n",
      "epoch: 54 train_loss: 0.013572 Test Acc.：244392/255000(95.84%),252662/255000(99.08%) 0.013448 Save model 03:04:09 08:17:54 left. \n",
      "epoch: 55 train_loss: 0.013559 Test Acc.：243615/255000(95.54%),252613/255000(99.06%) 0.013464 Save model 03:07:30 08:14:19 left. \n",
      "epoch: 56 train_loss: 0.013566 Test Acc.：243787/255000(95.60%),252326/255000(98.95%) 0.013447 03:10:56 08:10:59 left. \n",
      "epoch: 57 train_loss: 0.013559 Test Acc.：244188/255000(95.76%),252858/255000(99.16%) 0.013432 Save model 03:14:24 08:07:43 left. \n",
      "epoch: 58 train_loss: 0.013557 Test Acc.：244344/255000(95.82%),252565/255000(99.05%) 0.013411 Save model 03:17:47 08:04:13 left. \n",
      "epoch: 59 train_loss: 0.013548 Test Acc.：244584/255000(95.92%),252847/255000(99.16%) 0.013376 Save model 03:21:10 08:00:46 left. \n",
      "epoch: 60 train_loss: 0.013532 Test Acc.：244441/255000(95.86%),252668/255000(99.09%) 0.013355 Save model 03:24:36 07:57:25 left. \n",
      "epoch: 61 train_loss: 0.013538 Test Acc.：244313/255000(95.81%),252910/255000(99.18%) 0.013398 03:28:04 07:54:08 left. \n",
      "epoch: 62 train_loss: 0.013542 Test Acc.：244334/255000(95.82%),252752/255000(99.12%) 0.013456 03:31:26 07:50:36 left. \n",
      "epoch: 63 train_loss: 0.013543 Test Acc.：244609/255000(95.93%),252879/255000(99.17%) 0.01336 03:34:55 07:47:21 left. \n",
      "epoch: 64 train_loss: 0.013524 Test Acc.：244652/255000(95.94%),252810/255000(99.14%) 0.013413 Save model 03:38:23 07:44:04 left. \n",
      "epoch: 65 train_loss: 0.013513 Test Acc.：244608/255000(95.92%),252880/255000(99.17%) 0.013415 Save model 03:41:40 07:40:23 left. \n",
      "epoch: 66 train_loss: 0.013505 Test Acc.：244394/255000(95.84%),252950/255000(99.20%) 0.013372 Save model 03:45:10 07:37:09 left. \n",
      "epoch: 67 train_loss: 0.013503 Test Acc.：244606/255000(95.92%),252792/255000(99.13%) 0.013314 Save model 03:48:29 07:33:35 left. \n",
      "epoch: 68 train_loss: 0.013495 Test Acc.：244450/255000(95.86%),252903/255000(99.18%) 0.013424 Save model 03:51:52 07:30:06 left. \n",
      "epoch: 69 train_loss: 0.013492 Test Acc.：244889/255000(96.03%),252980/255000(99.21%) 0.013309 Save model 03:55:17 07:26:42 left. \n",
      "epoch: 70 train_loss: 0.013496 Test Acc.：244583/255000(95.91%),252407/255000(98.98%) 0.013345 03:58:43 07:23:21 left. \n",
      "epoch: 71 train_loss: 0.013484 Test Acc.：244752/255000(95.98%),252874/255000(99.17%) 0.013333 Save model 04:02:05 07:19:52 left. \n",
      "epoch: 72 train_loss: 0.013476 Test Acc.：245137/255000(96.13%),253033/255000(99.23%) 0.013287 Save model 04:05:30 07:16:28 left. \n",
      "epoch: 73 train_loss: 0.013465 Test Acc.：244658/255000(95.94%),252812/255000(99.14%) 0.013315 Save model 04:08:53 07:13:00 left. \n",
      "epoch: 74 train_loss: 0.013469 Test Acc.：244747/255000(95.98%),252905/255000(99.18%) 0.013353 04:12:14 07:09:29 left. \n",
      "epoch: 75 train_loss: 0.013451 Test Acc.：244802/255000(96.00%),252800/255000(99.14%) 0.013309 Save model 04:15:41 07:06:09 left. \n",
      "epoch: 76 train_loss: 0.013445 Test Acc.：244252/255000(95.79%),250643/255000(98.29%) 0.013441 Save model 04:18:58 07:02:31 left. \n",
      "epoch: 77 train_loss: 0.013442 Test Acc.：244662/255000(95.95%),252907/255000(99.18%) 0.013285 Save model 04:22:18 06:59:00 left. \n",
      "epoch: 78 train_loss: 0.013431 Test Acc.：245159/255000(96.14%),252881/255000(99.17%) 0.013262 Save model 04:25:40 06:55:32 left. \n",
      "epoch: 79 train_loss: 0.013427 Test Acc.：245110/255000(96.12%),252690/255000(99.09%) 0.013251 Save model 04:29:01 06:52:02 left. \n",
      "epoch: 80 train_loss: 0.013446 Test Acc.：244908/255000(96.04%),252724/255000(99.11%) 0.013331 04:32:29 06:48:44 left. \n",
      "epoch: 81 train_loss: 0.013413 Test Acc.：244603/255000(95.92%),252853/255000(99.16%) 0.013361 Save model 04:35:55 06:45:22 left. \n",
      "epoch: 82 train_loss: 0.013413 Test Acc.：244795/255000(96.00%),252166/255000(98.89%) 0.013314 Save model 04:39:13 06:41:48 left. \n",
      "epoch: 83 train_loss: 0.013404 Test Acc.：245071/255000(96.11%),252958/255000(99.20%) 0.013233 Save model 04:42:37 06:38:23 left. \n",
      "epoch: 84 train_loss: 0.013397 Test Acc.：244861/255000(96.02%),252549/255000(99.04%) 0.013315 Save model 04:46:01 06:34:59 left. \n",
      "epoch: 85 train_loss: 0.013387 Test Acc.：245217/255000(96.16%),252911/255000(99.18%) 0.013231 Save model 04:49:30 06:31:41 left. \n",
      "epoch: 86 train_loss: 0.013379 Test Acc.：244882/255000(96.03%),252903/255000(99.18%) 0.013344 Save model 04:52:59 06:28:23 left. \n",
      "epoch: 87 train_loss: 0.013379 Test Acc.：245242/255000(96.17%),252831/255000(99.15%) 0.013246 04:56:19 06:24:53 left. \n",
      "epoch: 88 train_loss: 0.013366 Test Acc.：245132/255000(96.13%),252880/255000(99.17%) 0.013263 Save model 04:59:47 06:21:33 left. \n",
      "epoch: 89 train_loss: 0.013365 Test Acc.：245002/255000(96.08%),252988/255000(99.21%) 0.013259 Save model 05:03:14 06:18:11 left. \n",
      "epoch: 90 train_loss: 0.013353 Test Acc.：245646/255000(96.33%),252996/255000(99.21%) 0.013228 Save model 05:06:35 06:14:43 left. \n",
      "epoch: 91 train_loss: 0.013353 Test Acc.：245389/255000(96.23%),252990/255000(99.21%) 0.013169 Save model 05:09:59 06:11:19 left. \n",
      "epoch: 92 train_loss: 0.013336 Test Acc.：245434/255000(96.25%),252726/255000(99.11%) 0.013271 Save model 05:13:26 06:07:57 left. \n",
      "epoch: 93 train_loss: 0.013331 Test Acc.：245499/255000(96.27%),252764/255000(99.12%) 0.013195 Save model 05:16:54 06:04:36 left. \n",
      "epoch: 94 train_loss: 0.013326 Test Acc.：245352/255000(96.22%),253049/255000(99.23%) 0.013219 Save model 05:20:20 06:01:14 left. \n",
      "epoch: 95 train_loss: 0.013309 Test Acc.：245183/255000(96.15%),252911/255000(99.18%) 0.013216 Save model 05:23:50 05:57:55 left. \n",
      "epoch: 96 train_loss: 0.013314 Test Acc.：245483/255000(96.27%),252981/255000(99.21%) 0.013221 05:27:16 05:54:32 left. \n",
      "epoch: 97 train_loss: 0.013305 Test Acc.：245620/255000(96.32%),252846/255000(99.16%) 0.013199 Save model 05:30:38 05:51:06 left. \n",
      "epoch: 98 train_loss: 0.013291 Test Acc.：245683/255000(96.35%),252971/255000(99.20%) 0.01317 Save model 05:34:01 05:47:39 left. \n",
      "epoch: 99 train_loss: 0.01329 Test Acc.：245286/255000(96.19%),252868/255000(99.16%) 0.013203 Save model 05:37:27 05:44:16 left. \n",
      "epoch: 100 train_loss: 0.013277 Test Acc.：245674/255000(96.34%),252676/255000(99.09%) 0.013135 Save model 05:40:49 05:40:49 left. \n",
      "epoch: 101 train_loss: 0.013278 Test Acc.：245783/255000(96.39%),252828/255000(99.15%) 0.013137 05:44:23 05:37:34 left. \n",
      "epoch: 102 train_loss: 0.013271 Test Acc.：245768/255000(96.38%),252791/255000(99.13%) 0.013148 Save model 05:47:42 05:34:04 left. \n",
      "epoch: 103 train_loss: 0.013254 Test Acc.：245858/255000(96.41%),253088/255000(99.25%) 0.013092 Save model 05:51:06 05:30:39 left. \n",
      "epoch: 104 train_loss: 0.013249 Test Acc.：245909/255000(96.43%),253021/255000(99.22%) 0.013111 Save model 05:54:30 05:27:14 left. \n",
      "epoch: 105 train_loss: 0.013245 Test Acc.：245984/255000(96.46%),253081/255000(99.25%) 0.013126 Save model 05:57:56 05:23:51 left. \n",
      "epoch: 106 train_loss: 0.013248 Test Acc.：245709/255000(96.36%),253009/255000(99.22%) 0.013139 06:01:21 05:20:27 left. \n",
      "epoch: 107 train_loss: 0.013229 Test Acc.：246136/255000(96.52%),252963/255000(99.20%) 0.013041 Save model 06:04:46 05:17:03 left. \n",
      "epoch: 108 train_loss: 0.013212 Test Acc.：245768/255000(96.38%),252874/255000(99.17%) 0.013118 Save model 06:08:10 05:13:37 left. \n",
      "epoch: 109 train_loss: 0.013213 Test Acc.：245904/255000(96.43%),252111/255000(98.87%) 0.013135 06:11:30 05:10:09 left. \n",
      "epoch: 110 train_loss: 0.0132 Test Acc.：245992/255000(96.47%),253094/255000(99.25%) 0.013068 Save model 06:14:59 05:06:48 left. \n",
      "epoch: 111 train_loss: 0.013195 Test Acc.：245963/255000(96.46%),253018/255000(99.22%) 0.013073 Save model 06:18:20 05:03:21 left. \n",
      "epoch: 112 train_loss: 0.013182 Test Acc.：245862/255000(96.42%),253077/255000(99.25%) 0.013092 Save model 06:21:42 04:59:55 left. \n",
      "epoch: 113 train_loss: 0.013184 Test Acc.：245796/255000(96.39%),252934/255000(99.19%) 0.013098 06:25:06 04:56:29 left. \n",
      "epoch: 114 train_loss: 0.013174 Test Acc.：245700/255000(96.35%),252989/255000(99.21%) 0.013099 Save model 06:28:28 04:53:03 left. \n",
      "epoch: 115 train_loss: 0.013161 Test Acc.：246110/255000(96.51%),253152/255000(99.28%) 0.013076 Save model 06:31:52 04:49:38 left. \n",
      "epoch: 116 train_loss: 0.013166 Test Acc.：246142/255000(96.53%),253055/255000(99.24%) 0.01307 06:35:15 04:46:13 left. \n",
      "epoch: 117 train_loss: 0.013152 Test Acc.：246202/255000(96.55%),253147/255000(99.27%) 0.013014 Save model 06:38:47 04:42:54 left. \n",
      "epoch: 118 train_loss: 0.013141 Test Acc.：246172/255000(96.54%),253024/255000(99.23%) 0.013031 Save model 06:41:42 04:39:09 left. \n",
      "epoch: 119 train_loss: 0.013134 Test Acc.：246194/255000(96.55%),253043/255000(99.23%) 0.013041 Save model 06:44:50 04:35:33 left. \n",
      "epoch: 120 train_loss: 0.013125 Test Acc.：245818/255000(96.40%),253054/255000(99.24%) 0.013117 Save model 06:47:57 04:31:58 left. \n",
      "epoch: 121 train_loss: 0.013115 Test Acc.：246300/255000(96.59%),253078/255000(99.25%) 0.013013 Save model 06:51:03 04:28:22 left. \n",
      "epoch: 122 train_loss: 0.013112 Test Acc.：246203/255000(96.55%),253086/255000(99.25%) 0.013057 Save model 06:54:09 04:24:47 left. \n",
      "epoch: 123 train_loss: 0.013099 Test Acc.：246396/255000(96.63%),253132/255000(99.27%) 0.012979 Save model 06:57:18 04:21:14 left. \n",
      "epoch: 124 train_loss: 0.013091 Test Acc.：246193/255000(96.55%),253136/255000(99.27%) 0.013016 Save model 07:00:28 04:17:42 left. \n",
      "epoch: 125 train_loss: 0.013088 Test Acc.：246404/255000(96.63%),253156/255000(99.28%) 0.012973 Save model 07:03:38 04:14:10 left. \n",
      "epoch: 126 train_loss: 0.013081 Test Acc.：246653/255000(96.73%),253199/255000(99.29%) 0.012958 Save model 07:06:44 04:10:37 left. \n",
      "epoch: 127 train_loss: 0.013073 Test Acc.：246477/255000(96.66%),253140/255000(99.27%) 0.012982 Save model 07:09:57 04:07:08 left. \n",
      "epoch: 128 train_loss: 0.013069 Test Acc.：246560/255000(96.69%),253151/255000(99.27%) 0.012961 Save model 07:13:05 04:03:37 left. \n",
      "epoch: 129 train_loss: 0.013048 Test Acc.：246554/255000(96.69%),253173/255000(99.28%) 0.012968 Save model 07:16:13 04:00:05 left. \n",
      "epoch: 130 train_loss: 0.013042 Test Acc.：246558/255000(96.69%),253038/255000(99.23%) 0.012968 Save model 07:19:21 03:56:34 left. \n",
      "epoch: 131 train_loss: 0.013033 Test Acc.：246266/255000(96.57%),253251/255000(99.31%) 0.012996 Save model 07:22:29 03:53:03 left. \n",
      "epoch: 132 train_loss: 0.013032 Test Acc.：246527/255000(96.68%),253288/255000(99.33%) 0.012958 Save model 07:25:37 03:49:33 left. \n",
      "epoch: 133 train_loss: 0.013018 Test Acc.：246700/255000(96.75%),253173/255000(99.28%) 0.012978 Save model 07:28:47 03:46:04 left. \n",
      "epoch: 134 train_loss: 0.013015 Test Acc.：246656/255000(96.73%),253094/255000(99.25%) 0.012966 Save model 07:31:54 03:42:34 left. \n",
      "epoch: 135 train_loss: 0.013004 Test Acc.：246675/255000(96.74%),253082/255000(99.25%) 0.012955 Save model 07:35:04 03:39:06 left. \n",
      "epoch: 136 train_loss: 0.012998 Test Acc.：246699/255000(96.74%),253217/255000(99.30%) 0.012937 Save model 07:38:16 03:35:39 left. \n",
      "epoch: 137 train_loss: 0.012983 Test Acc.：246722/255000(96.75%),253186/255000(99.29%) 0.012922 Save model 07:41:28 03:32:12 left. \n",
      "epoch: 138 train_loss: 0.012973 Test Acc.：246570/255000(96.69%),253250/255000(99.31%) 0.012941 Save model 07:44:36 03:28:44 left. \n",
      "epoch: 139 train_loss: 0.012974 Test Acc.：246831/255000(96.80%),253272/255000(99.32%) 0.012917 07:47:45 03:25:16 left. \n",
      "epoch: 140 train_loss: 0.012958 Test Acc.：246969/255000(96.85%),253252/255000(99.31%) 0.012885 Save model 07:50:48 03:21:46 left. \n",
      "epoch: 141 train_loss: 0.012948 Test Acc.：246733/255000(96.76%),253229/255000(99.31%) 0.012927 Save model 07:53:55 03:18:18 left. \n",
      "epoch: 142 train_loss: 0.012942 Test Acc.：246723/255000(96.75%),253183/255000(99.29%) 0.012938 Save model 07:57:06 03:14:52 left. \n",
      "epoch: 143 train_loss: 0.012933 Test Acc.：246793/255000(96.78%),253249/255000(99.31%) 0.012904 Save model 08:00:15 03:11:25 left. \n",
      "epoch: 144 train_loss: 0.012924 Test Acc.：247004/255000(96.86%),253254/255000(99.32%) 0.012867 Save model 08:03:23 03:07:58 left. \n",
      "epoch: 145 train_loss: 0.012914 Test Acc.：247055/255000(96.88%),253215/255000(99.30%) 0.012883 Save model 08:06:28 03:04:31 left. \n",
      "epoch: 146 train_loss: 0.01291 Test Acc.：246942/255000(96.84%),253111/255000(99.26%) 0.01288 Save model 08:09:38 03:01:05 left. \n",
      "epoch: 147 train_loss: 0.012895 Test Acc.：247141/255000(96.92%),253245/255000(99.31%) 0.012856 Save model 08:12:46 02:57:39 left. \n",
      "epoch: 148 train_loss: 0.012893 Test Acc.：247229/255000(96.95%),253338/255000(99.35%) 0.012863 Save model 08:15:55 02:54:14 left. \n",
      "epoch: 149 train_loss: 0.012879 Test Acc.：247131/255000(96.91%),253326/255000(99.34%) 0.012856 Save model 08:19:06 02:50:50 left. \n",
      "epoch: 150 train_loss: 0.012877 Test Acc.：247142/255000(96.92%),253321/255000(99.34%) 0.012852 Save model 08:22:15 02:47:25 left. \n",
      "epoch: 151 train_loss: 0.012866 Test Acc.：246967/255000(96.85%),253135/255000(99.27%) 0.01288 Save model 08:25:20 02:43:58 left. \n",
      "epoch: 152 train_loss: 0.012854 Test Acc.：247166/255000(96.93%),253319/255000(99.34%) 0.012861 Save model 08:28:27 02:40:33 left. \n",
      "epoch: 153 train_loss: 0.012848 Test Acc.：247244/255000(96.96%),253207/255000(99.30%) 0.012827 Save model 08:31:32 02:37:08 left. \n",
      "epoch: 154 train_loss: 0.012837 Test Acc.：247072/255000(96.89%),253254/255000(99.32%) 0.01282 Save model 08:34:44 02:33:45 left. \n",
      "epoch: 155 train_loss: 0.01283 Test Acc.：247392/255000(97.02%),253226/255000(99.30%) 0.012801 Save model 08:37:54 02:30:21 left. \n",
      "epoch: 156 train_loss: 0.012822 Test Acc.：247332/255000(96.99%),253272/255000(99.32%) 0.012802 Save model 08:41:01 02:26:57 left. \n",
      "epoch: 157 train_loss: 0.012813 Test Acc.：247428/255000(97.03%),253297/255000(99.33%) 0.012791 Save model 08:44:12 02:23:34 left. \n",
      "epoch: 158 train_loss: 0.012807 Test Acc.：247346/255000(97.00%),253341/255000(99.35%) 0.012795 Save model 08:47:26 02:20:12 left. \n",
      "epoch: 159 train_loss: 0.012793 Test Acc.：247330/255000(96.99%),253257/255000(99.32%) 0.012797 Save model 08:50:37 02:16:49 left. \n",
      "epoch: 160 train_loss: 0.012788 Test Acc.：247210/255000(96.95%),253402/255000(99.37%) 0.012813 Save model 08:53:47 02:13:26 left. \n",
      "epoch: 161 train_loss: 0.012783 Test Acc.：247554/255000(97.08%),253310/255000(99.34%) 0.012768 Save model 08:56:55 02:10:03 left. \n",
      "epoch: 162 train_loss: 0.012767 Test Acc.：247428/255000(97.03%),253221/255000(99.30%) 0.012792 Save model 09:00:02 02:06:40 left. \n",
      "epoch: 163 train_loss: 0.012762 Test Acc.：247649/255000(97.12%),253417/255000(99.38%) 0.012764 Save model 09:03:12 02:03:18 left. \n",
      "epoch: 164 train_loss: 0.012754 Test Acc.：247525/255000(97.07%),253348/255000(99.35%) 0.012777 Save model 09:06:21 01:59:55 left. \n",
      "epoch: 165 train_loss: 0.012746 Test Acc.：247610/255000(97.10%),253392/255000(99.37%) 0.012766 Save model 09:09:28 01:56:33 left. \n",
      "epoch: 166 train_loss: 0.012734 Test Acc.：247635/255000(97.11%),253407/255000(99.38%) 0.012762 Save model 09:12:32 01:53:10 left. \n",
      "epoch: 167 train_loss: 0.012726 Test Acc.：247765/255000(97.16%),253297/255000(99.33%) 0.012734 Save model 09:15:47 01:49:49 left. \n",
      "epoch: 168 train_loss: 0.012719 Test Acc.：247760/255000(97.16%),253369/255000(99.36%) 0.012724 Save model 09:18:57 01:46:28 left. \n",
      "epoch: 169 train_loss: 0.012714 Test Acc.：247720/255000(97.15%),253391/255000(99.37%) 0.012754 Save model 09:22:07 01:43:06 left. \n",
      "epoch: 170 train_loss: 0.012703 Test Acc.：247693/255000(97.13%),253336/255000(99.35%) 0.012741 Save model 09:25:20 01:39:45 left. \n",
      "epoch: 171 train_loss: 0.012694 Test Acc.：247562/255000(97.08%),253295/255000(99.33%) 0.012726 Save model 09:28:32 01:36:25 left. \n",
      "epoch: 172 train_loss: 0.012687 Test Acc.：247809/255000(97.18%),253373/255000(99.36%) 0.012715 Save model 09:31:40 01:33:03 left. \n",
      "epoch: 173 train_loss: 0.01268 Test Acc.：247792/255000(97.17%),253375/255000(99.36%) 0.012715 Save model 09:34:49 01:29:42 left. \n",
      "epoch: 174 train_loss: 0.012669 Test Acc.：247859/255000(97.20%),253404/255000(99.37%) 0.012702 Save model 09:37:56 01:26:21 left. \n",
      "epoch: 175 train_loss: 0.012662 Test Acc.：247821/255000(97.18%),253404/255000(99.37%) 0.012705 Save model 09:41:03 01:23:00 left. \n",
      "epoch: 176 train_loss: 0.012655 Test Acc.：248023/255000(97.26%),253338/255000(99.35%) 0.012683 Save model 09:44:16 01:19:40 left. \n",
      "epoch: 177 train_loss: 0.012645 Test Acc.：247969/255000(97.24%),253315/255000(99.34%) 0.012691 Save model 09:47:28 01:16:20 left. \n",
      "epoch: 178 train_loss: 0.01264 Test Acc.：248048/255000(97.27%),253352/255000(99.35%) 0.01268 Save model 09:50:39 01:13:00 left. \n",
      "epoch: 179 train_loss: 0.012632 Test Acc.：247963/255000(97.24%),253337/255000(99.35%) 0.012696 Save model 09:53:48 01:09:39 left. \n",
      "epoch: 180 train_loss: 0.012626 Test Acc.：248106/255000(97.30%),253341/255000(99.35%) 0.012662 Save model 09:57:00 01:06:20 left. \n",
      "epoch: 181 train_loss: 0.012617 Test Acc.：248077/255000(97.29%),253387/255000(99.37%) 0.012669 Save model 10:00:09 01:02:59 left. \n",
      "epoch: 182 train_loss: 0.012611 Test Acc.：248093/255000(97.29%),253303/255000(99.33%) 0.012676 Save model 10:03:19 00:59:40 left. \n",
      "epoch: 183 train_loss: 0.012605 Test Acc.：248102/255000(97.29%),253402/255000(99.37%) 0.012662 Save model 10:06:25 00:56:20 left. \n",
      "epoch: 184 train_loss: 0.0126 Test Acc.：248107/255000(97.30%),253352/255000(99.35%) 0.012656 Save model 10:09:36 00:53:00 left. \n",
      "epoch: 185 train_loss: 0.012598 Test Acc.：248145/255000(97.31%),253352/255000(99.35%) 0.012652 Save model 10:12:47 00:49:41 left. \n",
      "epoch: 186 train_loss: 0.012594 Test Acc.：248152/255000(97.31%),253309/255000(99.34%) 0.012652 Save model 10:15:56 00:46:21 left. \n",
      "epoch: 187 train_loss: 0.012583 Test Acc.：248183/255000(97.33%),253341/255000(99.35%) 0.012652 Save model 10:19:04 00:43:02 left. \n",
      "epoch: 188 train_loss: 0.012581 Test Acc.：248198/255000(97.33%),253372/255000(99.36%) 0.012644 Save model 10:22:11 00:39:42 left. \n",
      "epoch: 189 train_loss: 0.012573 Test Acc.：248205/255000(97.34%),253376/255000(99.36%) 0.012646 Save model 10:25:18 00:36:23 left. \n",
      "epoch: 190 train_loss: 0.01257 Test Acc.：248260/255000(97.36%),253281/255000(99.33%) 0.01264 Save model 10:28:28 00:33:04 left. \n",
      "epoch: 191 train_loss: 0.012567 Test Acc.：248232/255000(97.35%),253302/255000(99.33%) 0.012643 Save model 10:31:35 00:29:45 left. \n",
      "epoch: 192 train_loss: 0.012563 Test Acc.：248224/255000(97.34%),253393/255000(99.37%) 0.012633 Save model 10:34:42 00:26:26 left. \n",
      "epoch: 193 train_loss: 0.01256 Test Acc.：248255/255000(97.35%),253367/255000(99.36%) 0.012641 Save model 10:37:49 00:23:08 left. \n",
      "epoch: 194 train_loss: 0.012557 Test Acc.：248250/255000(97.35%),253353/255000(99.35%) 0.012629 Save model 10:40:56 00:19:49 left. \n",
      "epoch: 195 train_loss: 0.012557 Test Acc.：248262/255000(97.36%),253356/255000(99.36%) 0.012642 Save model 10:44:05 00:16:30 left. \n",
      "epoch: 196 train_loss: 0.012549 Test Acc.：248248/255000(97.35%),253351/255000(99.35%) 0.01263 Save model 10:47:15 00:13:12 left. \n",
      "epoch: 197 train_loss: 0.012552 Test Acc.：248261/255000(97.36%),253377/255000(99.36%) 0.012635 10:50:22 00:09:54 left. \n",
      "epoch: 198 train_loss: 0.01255 Test Acc.：248246/255000(97.35%),253362/255000(99.36%) 0.012637 10:53:31 00:06:36 left. \n",
      "epoch: 199 train_loss: 0.012548 Test Acc.：248253/255000(97.35%),253343/255000(99.35%) 0.012642 Save model 10:56:41 00:03:17 left. \n",
      "epoch: 200 train_loss: 0.01255 Test Acc.：248272/255000(97.36%),253349/255000(99.35%) 0.012631 10:59:50 00:00:00 left. \n",
      "Test Acc.：174504/255000(68.43%),20000/255000(7.84%) 0.030157 \n",
      "01_1_VC2003_32bit_none 01_2_VC2017_32bit_none 02_1_VC2003_32bit_max 02_2_VC2017_32bit_max 03_1_gcc6.3.0_x86_none 03_2_x86-O0-gcc7.5.0 04_1_gcc6.3.0_x86_O3 04_2_x86-O3-gcc7.5.0 05_1_clang5.0.2_32_none 05_2_x86-O0-Clang10.0.0 06_1_clang5.0.2_32_O3 06_2_x86-O3-Clang10.0.0 07_intel_32_none 08_intel_32bit_max 11_VC2017_64bit_none 12_VC2017_64bit_max 13_1_gcc6.3.0_64bit_none 13_2_x86_64-O0-gcc7.5.0 14_1_gcc6.3.0_64bit_max 14_2_x86_64-O3-gcc7.5.0 15_1_clang5.0.2_64bit_none 15_2_x86_64-O0-Clang10.0.0 16_1_clang5.0.2_64bit_max 16_2_x86_64-O3-Clang10.0.0 17_intel_64_none 18_intel_64bit_max 21_arm-O0-gcc 22_arm-O3-gcc 23_arm-O0-Clang 24_arm-O3-Clang 31_arm64-O0-gcc 32_arm64-O3-gcc 33_arm64-O0-Clang 34_arm64-O3-Clang 41_mips-O0-gcc 42_mips-O3-gcc 43_mips-O0-Clang 44_mips-O3-Clang 51_mips64-O0-gcc 52_mips64-O3-gcc 53_mips64-O0-Clang 54_mips64-O3-Clang 61_powerpc-O0-gcc 62_powerpc-O3-gcc 63_powerpc-O0-Clang 64_powerpc-O3-Clang 71_powerpc64-O0-gcc 72_powerpc64-O3-gcc 73_powerpc64-O0-Clang 74_powerpc64-O3-Clang 80_document \n",
      "01_1_VC2003_32bit_none 4793 197 2 3 0 0 0 0 0 1 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 \n",
      "01_2_VC2017_32bit_none 219 4769 0 2 1 0 1 1 2 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 \n",
      "02_1_VC2003_32bit_max 17 0 4934 24 0 0 1 2 0 1 4 4 0 9 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 \n",
      "02_2_VC2017_32bit_max 7 2 51 4918 0 0 5 1 0 0 2 2 0 8 0 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 \n",
      "03_1_gcc6.3.0_x86_none 0 0 0 0 4870 116 7 3 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "03_2_x86-O0-gcc7.5.0 1 1 0 0 175 4811 0 5 2 0 1 0 0 1 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "04_1_gcc6.3.0_x86_O3 1 0 2 3 3 0 4662 266 1 0 29 23 0 3 0 0 0 0 4 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "04_2_x86-O3-gcc7.5.0 1 0 1 0 0 2 323 4629 2 0 19 21 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "05_1_clang5.0.2_32_none 5 0 0 0 0 1 0 2 4751 234 2 1 0 0 0 0 0 0 0 0 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "05_2_x86-O0-Clang10.0.0 0 0 0 0 1 1 1 4 307 4685 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "06_1_clang5.0.2_32_O3 1 0 2 12 2 0 28 16 0 1 4747 175 0 3 0 0 0 0 2 1 0 0 5 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "06_2_x86-O3-Clang10.0.0 0 0 9 3 0 0 29 25 0 2 183 4741 0 1 0 0 0 0 0 1 0 0 2 3 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "07_intel_32_none 0 1 0 0 2 2 0 0 1 0 0 0 4994 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "08_intel_32bit_max 2 0 6 16 1 0 4 1 2 0 9 1 0 4956 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "11_VC2017_64bit_none 0 0 1 0 0 0 0 0 0 0 1 0 0 0 4988 3 1 2 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 \n",
      "12_VC2017_64bit_max 0 0 0 0 0 0 0 0 0 0 1 0 0 0 2 4975 0 0 6 2 0 1 4 3 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 \n",
      "13_1_gcc6.3.0_64bit_none 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 4458 530 3 0 0 1 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "13_2_x86_64-O0-gcc7.5.0 0 0 0 0 4 3 1 0 1 0 1 0 0 0 0 0 642 4340 1 2 2 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "14_1_gcc6.3.0_64bit_max 0 0 0 1 0 0 7 6 0 0 1 0 0 0 0 0 2 1 4649 256 0 1 42 28 0 3 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 \n",
      "14_2_x86_64-O3-gcc7.5.0 0 0 0 2 0 1 5 6 0 0 6 0 0 0 1 2 0 5 509 4410 0 1 21 29 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "15_1_clang5.0.2_64bit_none 1 0 0 0 0 0 0 0 13 0 0 0 0 0 0 0 0 1 3 0 4669 311 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "15_2_x86_64-O0-Clang10.0.0 1 0 0 0 0 0 0 0 1 5 0 0 0 0 0 0 1 0 0 0 250 4741 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "16_1_clang5.0.2_64bit_max 0 0 0 4 0 1 0 1 0 0 23 0 0 0 0 2 0 0 60 35 1 2 4658 212 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "16_2_x86_64-O3-Clang10.0.0 0 0 2 1 0 0 1 1 0 0 6 2 0 0 0 0 0 2 42 44 0 0 274 4623 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "17_intel_64_none 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 4998 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "18_intel_64bit_max 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 1 1 1 3 0 0 1 1 0 4987 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "21_arm-O0-gcc 0 2 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 4983 2 0 2 2 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 4 \n",
      "22_arm-O3-gcc 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 6 4980 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 6 \n",
      "23_arm-O0-Clang 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 4981 13 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 2 \n",
      "24_arm-O3-Clang 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 4 15 4972 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 \n",
      "31_arm64-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4995 4 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "32_arm64-O3-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 4981 0 14 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "33_arm64-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4999 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "34_arm64-O3-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 26 8 4963 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 \n",
      "41_mips-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4992 0 0 0 7 0 0 1 0 0 0 0 0 0 0 0 0 \n",
      "42_mips-O3-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 4971 0 22 0 4 0 0 0 0 0 0 0 0 0 0 0 \n",
      "43_mips-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 4998 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "44_mips-O3-Clang 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 12 0 4971 0 1 0 15 0 0 0 0 0 0 0 0 0 \n",
      "51_mips64-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 0 0 4991 2 0 0 0 0 0 0 0 0 0 0 0 \n",
      "52_mips64-O3-gcc 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 25 0 2 1 4961 0 8 0 0 0 0 0 0 0 0 0 \n",
      "53_mips64-O0-Clang 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 4998 0 0 0 0 0 0 0 0 0 0 \n",
      "54_mips64-O3-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 9 0 30 0 14 2 4944 0 0 0 0 0 0 0 0 0 \n",
      "61_powerpc-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4996 3 0 0 1 0 0 0 0 \n",
      "62_powerpc-O3-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 4983 1 7 0 6 0 0 0 \n",
      "63_powerpc-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 4999 0 0 0 0 0 0 \n",
      "64_powerpc-O3-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 13 0 4971 0 3 0 10 0 \n",
      "71_powerpc64-O0-gcc 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 4992 3 0 0 1 \n",
      "72_powerpc64-O3-gcc 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 24 0 0 0 4961 1 9 1 \n",
      "73_powerpc64-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 10 0 0 0 4988 2 0 \n",
      "74_powerpc64-O3-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 1 46 0 8 4 4937 0 \n",
      "80_document 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 2 0 1 0 0 0 0 0 1 0 0 4990 \n",
      "no label Num TP FP FN TN R P F1 Acc.\n",
      "0 01_1_VC2003_32bit_none 5000 4793 207 257 4793 0.9491089108910891 0.9586 0.9538308457711442 0.9538308457711443\n",
      "1 01_2_VC2017_32bit_none 5000 4769 231 205 4769 0.9587856855649377 0.9538 0.9562863444956888 0.9562863444956888\n",
      "2 02_1_VC2003_32bit_max 5000 4934 66 78 4934 0.984437350359138 0.9868 0.9856172592888534 0.9856172592888534\n",
      "3 02_2_VC2017_32bit_max 5000 4918 82 72 4918 0.9855711422845691 0.9836 0.9845845845845845 0.9845845845845845\n",
      "4 03_1_gcc6.3.0_x86_none 5000 4870 130 190 4870 0.9624505928853755 0.974 0.9681908548707753 0.9681908548707754\n",
      "5 03_2_x86-O0-gcc7.5.0 5000 4811 189 128 4811 0.9740838226361611 0.9622 0.9681054432035416 0.9681054432035416\n",
      "6 04_1_gcc6.3.0_x86_O3 5000 4662 338 414 4662 0.9184397163120568 0.9324 0.9253672092100039 0.9253672092100039\n",
      "7 04_2_x86-O3-gcc7.5.0 5000 4629 371 340 4629 0.93157576977259 0.9258 0.9286789046042733 0.9286789046042733\n",
      "8 05_1_clang5.0.2_32_none 5000 4751 249 333 4751 0.9345003933910306 0.9502 0.9422848076160254 0.9422848076160254\n",
      "9 05_2_x86-O0-Clang10.0.0 5000 4685 315 246 4685 0.9501115392415331 0.937 0.943510220521599 0.943510220521599\n",
      "10 06_1_clang5.0.2_32_O3 5000 4747 253 291 4747 0.9422389837236999 0.9494 0.9458059374377367 0.9458059374377366\n",
      "11 06_2_x86-O3-Clang10.0.0 5000 4741 259 231 4741 0.9535398230088495 0.9482 0.9508624147613317 0.9508624147613317\n",
      "12 07_intel_32_none 5000 4994 6 1 4994 0.9997997997997998 0.9988 0.9992996498249126 0.9992996498249125\n",
      "13 08_intel_32bit_max 5000 4956 44 30 4956 0.9939831528279182 0.9912 0.992589625475666 0.992589625475666\n",
      "14 11_VC2017_64bit_none 5000 4988 12 3 4988 0.9993989180524945 0.9976 0.9984986487839055 0.9984986487839055\n",
      "15 12_VC2017_64bit_max 5000 4975 25 16 4975 0.996794229613304 0.995 0.9958963066760085 0.9958963066760084\n",
      "16 13_1_gcc6.3.0_64bit_none 5000 4458 542 649 4458 0.8729195222243978 0.8916 0.8821608785989907 0.8821608785989908\n",
      "17 13_2_x86_64-O0-gcc7.5.0 5000 4340 660 546 4340 0.8882521489971347 0.868 0.8780093060894194 0.8780093060894194\n",
      "18 14_1_gcc6.3.0_64bit_max 5000 4649 351 637 4649 0.879493000378358 0.9298 0.9039471125802061 0.9039471125802061\n",
      "19 14_2_x86_64-O3-gcc7.5.0 5000 4410 590 347 4410 0.9270548665125079 0.882 0.9039663831095623 0.9039663831095623\n",
      "20 15_1_clang5.0.2_64bit_none 5000 4669 331 257 4669 0.9478278522127487 0.9338 0.9407616361071932 0.9407616361071932\n",
      "21 15_2_x86_64-O0-Clang10.0.0 5000 4741 259 320 4741 0.9367713890535467 0.9482 0.9424510486035185 0.9424510486035186\n",
      "22 16_1_clang5.0.2_64bit_max 5000 4658 342 359 4658 0.9284432928044648 0.9316 0.9300189677548168 0.9300189677548168\n",
      "23 16_2_x86_64-O3-Clang10.0.0 5000 4623 377 290 4623 0.9409729289639731 0.9246 0.9327146171693734 0.9327146171693735\n",
      "24 17_intel_64_none 5000 4998 2 1 4998 0.9997999599919984 0.9996 0.9996999699969997 0.9996999699969997\n",
      "25 18_intel_64bit_max 5000 4987 13 12 4987 0.9975995199039808 0.9974 0.9974997499749975 0.9974997499749975\n",
      "26 21_arm-O0-gcc 5000 4983 17 10 4983 0.9979971960745043 0.9966 0.9972981086760733 0.9972981086760733\n",
      "27 22_arm-O3-gcc 5000 4980 20 12 4980 0.9975961538461539 0.996 0.9967974379503604 0.9967974379503602\n",
      "28 23_arm-O0-Clang 5000 4981 19 16 4981 0.9967980788473084 0.9962 0.9964989496849055 0.9964989496849055\n",
      "29 24_arm-O3-Clang 5000 4972 28 16 4972 0.9967923015236567 0.9944 0.9955947136563876 0.9955947136563876\n",
      "30 31_arm64-O0-gcc 5000 4995 5 9 4995 0.9982014388489209 0.999 0.9986005597760896 0.9986005597760895\n",
      "31 32_arm64-O3-gcc 5000 4981 19 31 4981 0.9938148443735035 0.9962 0.9950059928086297 0.9950059928086297\n",
      "32 33_arm64-O0-Clang 5000 4999 1 10 4999 0.998003593531643 0.9998 0.9989009891098012 0.9989009891098012\n",
      "33 34_arm64-O3-Clang 5000 4963 37 18 4963 0.9963862678177073 0.9926 0.9944895301072038 0.9944895301072036\n",
      "34 41_mips-O0-gcc 5000 4992 8 11 4992 0.9978013192084749 0.9984 0.9981005698290513 0.9981005698290513\n",
      "35 42_mips-O3-gcc 5000 4971 29 51 4971 0.9898446833930705 0.9942 0.9920175613649971 0.992017561364997\n",
      "36 43_mips-O0-Clang 5000 4998 2 2 4998 0.9996 0.9996 0.9996 0.9996\n",
      "37 44_mips-O3-Clang 5000 4971 29 56 4971 0.9888601551621246 0.9942 0.9915228882018551 0.991522888201855\n",
      "38 51_mips64-O0-gcc 5000 4991 9 9 4991 0.9982 0.9982 0.9982 0.9982\n",
      "39 52_mips64-O3-gcc 5000 4961 39 23 4961 0.9953852327447833 0.9922 0.9937900641025642 0.9937900641025641\n",
      "40 53_mips64-O0-Clang 5000 4998 2 2 4998 0.9996 0.9996 0.9996 0.9996\n",
      "41 54_mips64-O3-Clang 5000 4944 56 26 4944 0.9947686116700202 0.9888 0.9917753259779338 0.9917753259779338\n",
      "42 61_powerpc-O0-gcc 5000 4996 4 4 4996 0.9992 0.9992 0.9992 0.9992\n",
      "43 62_powerpc-O3-gcc 5000 4983 17 46 4983 0.9908530522966793 0.9966 0.9937182171702065 0.9937182171702064\n",
      "44 63_powerpc-O0-Clang 5000 4999 1 12 4999 0.9976052684094991 0.9998 0.9987014284287283 0.9987014284287284\n",
      "45 64_powerpc-O3-Clang 5000 4971 29 55 4971 0.9890569040986869 0.9942 0.9916217833632555 0.9916217833632556\n",
      "46 71_powerpc64-O0-gcc 5000 4992 8 1 4992 0.9997997196074504 0.9984 0.999099369558691 0.999099369558691\n",
      "47 72_powerpc64-O3-gcc 5000 4961 39 22 4961 0.9955849889624724 0.9922 0.9938896123409796 0.9938896123409796\n",
      "48 73_powerpc64-O0-Clang 5000 4988 12 5 4988 0.9989985980372521 0.9976 0.9982988091664164 0.9982988091664164\n",
      "49 74_powerpc64-O3-Clang 5000 4937 63 24 4937 0.9951622656722435 0.9874 0.9912659371549042 0.9912659371549041\n",
      "50 80_document 5000 4990 10 23 4990 0.9954119289846399 0.998 0.9967042844302407 0.9967042844302407\n",
      "  255000 248253 6747 6747 248253 0.9735411764705882 0.9735411764705882 0.9735411764705882 0.9735411764705882\n",
      "3\n",
      "Start MainNet PreTrain\n",
      "epoch: 1 train_loss: 0.018198 00:01:59 06:35:29 left. \n",
      "epoch: 2 train_loss: 0.015423 00:03:58 06:33:10 left. \n",
      "epoch: 3 train_loss: 0.014944 00:05:56 06:29:39 left. \n",
      "epoch: 4 train_loss: 0.014685 00:07:53 06:27:05 left. \n",
      "epoch: 5 train_loss: 0.014535 00:09:53 06:25:36 left. \n",
      "epoch: 6 train_loss: 0.014419 00:11:52 06:23:59 left. \n",
      "epoch: 7 train_loss: 0.014331 00:13:52 06:22:31 left. \n",
      "epoch: 8 train_loss: 0.014265 00:15:51 06:20:33 left. \n",
      "epoch: 9 train_loss: 0.014214 00:17:49 06:18:08 left. \n",
      "epoch: 10 train_loss: 0.014164 00:19:48 06:16:29 left. \n",
      "epoch: 11 train_loss: 0.014117 00:21:47 06:14:22 left. \n",
      "epoch: 12 train_loss: 0.01408 00:23:46 06:12:23 left. \n",
      "epoch: 13 train_loss: 0.014064 00:25:44 06:10:17 left. \n",
      "epoch: 14 train_loss: 0.014031 00:27:44 06:08:37 left. \n",
      "epoch: 15 train_loss: 0.014004 00:29:44 06:06:48 left. \n",
      "epoch: 16 train_loss: 0.013983 00:31:43 06:04:51 left. \n",
      "epoch: 17 train_loss: 0.013962 00:33:42 06:02:47 left. \n",
      "epoch: 18 train_loss: 0.013936 00:35:41 06:00:53 left. \n",
      "epoch: 19 train_loss: 0.013917 00:37:41 05:59:00 left. \n",
      "epoch: 20 train_loss: 0.013898 00:39:40 05:57:03 left. \n",
      "epoch: 21 train_loss: 0.013904 00:41:39 05:55:08 left. \n",
      "epoch: 22 train_loss: 0.013866 00:43:38 05:53:07 left. \n",
      "epoch: 23 train_loss: 0.013856 00:45:36 05:51:02 left. \n",
      "epoch: 24 train_loss: 0.013841 00:47:36 05:49:09 left. \n",
      "epoch: 25 train_loss: 0.013818 00:49:35 05:47:11 left. \n",
      "epoch: 26 train_loss: 0.013814 00:51:34 05:45:11 left. \n",
      "epoch: 27 train_loss: 0.013797 00:53:34 05:43:17 left. \n",
      "epoch: 28 train_loss: 0.013786 00:55:34 05:41:20 left. \n",
      "epoch: 29 train_loss: 0.013775 00:57:32 05:39:15 left. \n",
      "epoch: 30 train_loss: 0.013764 00:59:29 05:37:09 left. \n",
      "epoch: 31 train_loss: 0.013761 01:01:28 05:35:10 left. \n",
      "epoch: 32 train_loss: 0.013741 01:03:27 05:33:11 left. \n",
      "epoch: 33 train_loss: 0.013734 01:05:26 05:31:09 left. \n",
      "epoch: 34 train_loss: 0.013726 01:07:25 05:29:10 left. \n",
      "epoch: 35 train_loss: 0.013725 01:09:23 05:27:08 left. \n",
      "epoch: 36 train_loss: 0.013717 01:11:21 05:25:06 left. \n",
      "epoch: 37 train_loss: 0.013706 01:13:20 05:23:04 left. \n",
      "epoch: 38 train_loss: 0.013706 01:15:20 05:21:09 left. \n",
      "epoch: 39 train_loss: 0.013693 01:17:17 05:19:05 left. \n",
      "epoch: 40 train_loss: 0.013685 01:19:16 05:17:04 left. \n",
      "epoch: 41 train_loss: 0.013672 01:21:15 05:15:08 left. \n",
      "epoch: 42 train_loss: 0.013672 01:23:12 05:13:00 left. \n",
      "epoch: 43 train_loss: 0.013654 01:25:11 05:11:02 left. \n",
      "epoch: 44 train_loss: 0.013651 01:27:11 05:09:06 left. \n",
      "epoch: 45 train_loss: 0.01365 01:29:10 05:07:10 left. \n",
      "epoch: 46 train_loss: 0.013635 01:31:09 05:05:09 left. \n",
      "epoch: 47 train_loss: 0.013635 01:33:08 05:03:12 left. \n",
      "epoch: 48 train_loss: 0.013624 01:35:07 05:01:14 left. \n",
      "epoch: 49 train_loss: 0.013604 01:37:02 04:59:03 left. \n",
      "epoch: 50 train_loss: 0.01361 01:39:00 04:57:01 left. \n",
      "epoch: 51 train_loss: 0.013598 01:40:58 04:55:01 left. \n",
      "epoch: 52 train_loss: 0.013599 01:42:57 04:53:03 left. \n",
      "epoch: 53 train_loss: 0.013586 01:44:57 04:51:06 left. \n",
      "epoch: 54 train_loss: 0.013579 01:46:57 04:49:10 left. \n",
      "epoch: 55 train_loss: 0.013564 01:48:55 04:47:09 left. \n",
      "epoch: 56 train_loss: 0.013561 01:50:51 04:45:04 left. \n",
      "epoch: 57 train_loss: 0.013563 01:52:51 04:43:07 left. \n",
      "epoch: 58 train_loss: 0.013548 01:54:50 04:41:09 left. \n",
      "epoch: 59 train_loss: 0.013551 01:56:50 04:39:13 left. \n",
      "epoch: 60 train_loss: 0.013547 01:58:49 04:37:15 left. \n",
      "epoch: 61 train_loss: 0.013533 02:00:49 04:35:18 left. \n",
      "epoch: 62 train_loss: 0.01353 02:02:48 04:33:20 left. \n",
      "epoch: 63 train_loss: 0.013515 02:04:47 04:31:21 left. \n",
      "epoch: 64 train_loss: 0.013507 02:06:44 04:29:19 left. \n",
      "epoch: 65 train_loss: 0.013503 02:08:42 04:27:19 left. \n",
      "epoch: 66 train_loss: 0.013505 02:10:40 04:25:18 left. \n",
      "epoch: 67 train_loss: 0.013496 02:12:37 04:23:15 left. \n",
      "epoch: 68 train_loss: 0.013483 02:14:36 04:21:18 left. \n",
      "epoch: 69 train_loss: 0.013477 02:16:35 04:19:18 left. \n",
      "epoch: 70 train_loss: 0.013476 02:18:30 04:17:14 left. \n",
      "epoch: 71 train_loss: 0.013463 02:20:31 04:15:19 left. \n",
      "epoch: 72 train_loss: 0.013455 02:22:29 04:13:19 left. \n",
      "epoch: 73 train_loss: 0.013453 02:24:28 04:11:21 left. \n",
      "epoch: 74 train_loss: 0.013448 02:26:29 04:09:25 left. \n",
      "epoch: 75 train_loss: 0.013439 02:28:29 04:07:28 left. \n",
      "epoch: 76 train_loss: 0.013431 02:30:29 04:05:31 left. \n",
      "epoch: 77 train_loss: 0.013424 02:32:28 04:03:34 left. \n",
      "epoch: 78 train_loss: 0.013421 02:34:27 04:01:35 left. \n",
      "epoch: 79 train_loss: 0.01341 02:36:27 03:59:38 left. \n",
      "epoch: 80 train_loss: 0.013399 02:38:26 03:57:39 left. \n",
      "epoch: 81 train_loss: 0.013397 02:40:26 03:55:42 left. \n",
      "epoch: 82 train_loss: 0.013384 02:42:26 03:53:45 left. \n",
      "epoch: 83 train_loss: 0.01338 02:44:24 03:51:45 left. \n",
      "epoch: 84 train_loss: 0.013377 02:46:21 03:49:43 left. \n",
      "epoch: 85 train_loss: 0.013371 02:48:21 03:47:47 left. \n",
      "epoch: 86 train_loss: 0.013362 02:50:21 03:45:49 left. \n",
      "epoch: 87 train_loss: 0.013356 02:52:19 03:43:49 left. \n",
      "epoch: 88 train_loss: 0.013347 02:54:21 03:41:54 left. \n",
      "epoch: 89 train_loss: 0.013339 02:56:20 03:39:56 left. \n",
      "epoch: 90 train_loss: 0.013326 02:58:20 03:37:58 left. \n",
      "epoch: 91 train_loss: 0.013328 03:00:21 03:36:01 left. \n",
      "epoch: 92 train_loss: 0.013321 03:02:20 03:34:03 left. \n",
      "epoch: 93 train_loss: 0.013315 03:04:19 03:32:04 left. \n",
      "epoch: 94 train_loss: 0.013307 03:06:20 03:30:07 left. \n",
      "epoch: 95 train_loss: 0.013296 03:08:19 03:28:08 left. \n",
      "epoch: 96 train_loss: 0.013289 03:10:18 03:26:10 left. \n",
      "epoch: 97 train_loss: 0.01328 03:12:20 03:24:13 left. \n",
      "epoch: 98 train_loss: 0.013275 03:14:19 03:22:14 left. \n",
      "epoch: 99 train_loss: 0.013268 03:16:17 03:20:15 left. \n",
      "epoch: 100 train_loss: 0.013268 03:18:18 03:18:18 left. \n",
      "epoch: 101 train_loss: 0.013252 03:20:18 03:16:20 left. \n",
      "epoch: 102 train_loss: 0.013243 03:22:17 03:14:21 left. \n",
      "epoch: 103 train_loss: 0.013235 03:24:18 03:12:24 left. \n",
      "epoch: 104 train_loss: 0.013235 03:26:17 03:10:25 left. \n",
      "epoch: 105 train_loss: 0.013223 03:28:18 03:08:27 left. \n",
      "epoch: 106 train_loss: 0.013223 03:30:17 03:06:29 left. \n",
      "epoch: 107 train_loss: 0.013208 03:32:18 03:04:31 left. \n",
      "epoch: 108 train_loss: 0.013202 03:34:18 03:02:33 left. \n",
      "epoch: 109 train_loss: 0.013195 03:36:18 03:00:35 left. \n",
      "epoch: 110 train_loss: 0.013182 03:38:18 02:58:36 left. \n",
      "epoch: 111 train_loss: 0.01318 03:40:18 02:56:38 left. \n",
      "epoch: 112 train_loss: 0.013177 03:42:17 02:54:39 left. \n",
      "epoch: 113 train_loss: 0.013159 03:44:16 02:52:40 left. \n",
      "epoch: 114 train_loss: 0.013158 03:46:16 02:50:42 left. \n",
      "epoch: 115 train_loss: 0.013148 03:48:15 02:48:43 left. \n",
      "epoch: 116 train_loss: 0.013142 03:50:14 02:46:43 left. \n",
      "epoch: 117 train_loss: 0.013135 03:52:15 02:44:45 left. \n",
      "epoch: 118 train_loss: 0.01312 03:54:14 02:42:46 left. \n",
      "epoch: 119 train_loss: 0.013117 03:56:14 02:40:48 left. \n",
      "epoch: 120 train_loss: 0.013114 03:58:14 02:38:49 left. \n",
      "epoch: 121 train_loss: 0.013097 04:00:14 02:36:50 left. \n",
      "epoch: 122 train_loss: 0.013092 04:02:13 02:34:51 left. \n",
      "epoch: 123 train_loss: 0.013087 04:04:14 02:32:53 left. \n",
      "epoch: 124 train_loss: 0.013076 04:06:13 02:30:54 left. \n",
      "epoch: 125 train_loss: 0.013069 04:08:12 02:28:55 left. \n",
      "epoch: 126 train_loss: 0.013066 04:10:13 02:26:57 left. \n",
      "epoch: 127 train_loss: 0.013055 04:12:12 02:24:58 left. \n",
      "epoch: 128 train_loss: 0.013047 04:14:12 02:22:59 left. \n",
      "epoch: 129 train_loss: 0.013036 04:16:12 02:21:01 left. \n",
      "epoch: 130 train_loss: 0.01303 04:18:12 02:19:01 left. \n",
      "epoch: 131 train_loss: 0.013021 04:20:11 02:17:02 left. \n",
      "epoch: 132 train_loss: 0.013015 04:22:11 02:15:03 left. \n",
      "epoch: 133 train_loss: 0.013007 04:24:10 02:13:05 left. \n",
      "epoch: 134 train_loss: 0.012993 04:26:11 02:11:06 left. \n",
      "epoch: 135 train_loss: 0.012989 04:28:11 02:09:07 left. \n",
      "epoch: 136 train_loss: 0.012985 04:30:10 02:07:08 left. \n",
      "epoch: 137 train_loss: 0.012971 04:32:10 02:05:09 left. \n",
      "epoch: 138 train_loss: 0.012966 04:34:09 02:03:10 left. \n",
      "epoch: 139 train_loss: 0.012958 04:36:08 02:01:10 left. \n",
      "epoch: 140 train_loss: 0.01295 04:38:09 01:59:12 left. \n",
      "epoch: 141 train_loss: 0.012939 04:40:08 01:57:13 left. \n",
      "epoch: 142 train_loss: 0.012929 04:42:07 01:55:14 left. \n",
      "epoch: 143 train_loss: 0.012925 04:44:08 01:53:15 left. \n",
      "epoch: 144 train_loss: 0.012919 04:46:09 01:51:16 left. \n",
      "epoch: 145 train_loss: 0.012906 04:48:08 01:49:17 left. \n",
      "epoch: 146 train_loss: 0.012901 04:50:09 01:47:19 left. \n",
      "epoch: 147 train_loss: 0.012892 04:52:09 01:45:20 left. \n",
      "epoch: 148 train_loss: 0.012881 04:54:07 01:43:20 left. \n",
      "epoch: 149 train_loss: 0.01287 04:56:08 01:41:21 left. \n",
      "epoch: 150 train_loss: 0.012866 04:58:08 01:39:22 left. \n",
      "epoch: 151 train_loss: 0.012856 05:00:06 01:37:23 left. \n",
      "epoch: 152 train_loss: 0.012849 05:02:07 01:35:24 left. \n",
      "epoch: 153 train_loss: 0.012837 05:04:07 01:33:25 left. \n",
      "epoch: 154 train_loss: 0.012831 05:06:06 01:31:26 left. \n",
      "epoch: 155 train_loss: 0.012823 05:08:06 01:29:26 left. \n",
      "epoch: 156 train_loss: 0.012812 05:10:05 01:27:27 left. \n",
      "epoch: 157 train_loss: 0.012806 05:12:05 01:25:28 left. \n",
      "epoch: 158 train_loss: 0.012801 05:14:05 01:23:29 left. \n",
      "epoch: 159 train_loss: 0.012787 05:16:05 01:21:30 left. \n",
      "epoch: 160 train_loss: 0.012777 05:18:05 01:19:31 left. \n",
      "epoch: 161 train_loss: 0.012776 05:20:04 01:17:32 left. \n",
      "epoch: 162 train_loss: 0.012763 05:22:03 01:15:32 left. \n",
      "epoch: 163 train_loss: 0.012758 05:24:04 01:13:33 left. \n",
      "epoch: 164 train_loss: 0.012752 05:26:04 01:11:34 left. \n",
      "epoch: 165 train_loss: 0.012739 05:28:00 01:09:34 left. \n",
      "epoch: 166 train_loss: 0.012728 05:30:01 01:07:35 left. \n",
      "epoch: 167 train_loss: 0.012723 05:31:59 01:05:36 left. \n",
      "epoch: 168 train_loss: 0.012715 05:33:56 01:03:36 left. \n",
      "epoch: 169 train_loss: 0.012706 05:35:56 01:01:37 left. \n",
      "epoch: 170 train_loss: 0.012698 05:37:55 00:59:37 left. \n",
      "epoch: 171 train_loss: 0.012689 05:39:52 00:57:38 left. \n",
      "epoch: 172 train_loss: 0.012682 05:41:52 00:55:39 left. \n",
      "epoch: 173 train_loss: 0.012677 05:43:49 00:53:39 left. \n",
      "epoch: 174 train_loss: 0.012667 05:45:50 00:51:40 left. \n",
      "epoch: 175 train_loss: 0.012661 05:47:50 00:49:41 left. \n",
      "epoch: 176 train_loss: 0.012653 05:49:49 00:47:42 left. \n",
      "epoch: 177 train_loss: 0.012643 05:51:50 00:45:43 left. \n",
      "epoch: 178 train_loss: 0.01264 05:53:49 00:43:43 left. \n",
      "epoch: 179 train_loss: 0.012632 05:55:48 00:41:44 left. \n",
      "epoch: 180 train_loss: 0.012627 05:57:49 00:39:45 left. \n",
      "epoch: 181 train_loss: 0.01262 05:59:47 00:37:46 left. \n",
      "epoch: 182 train_loss: 0.01261 06:01:46 00:35:46 left. \n",
      "epoch: 183 train_loss: 0.012605 06:03:47 00:33:47 left. \n",
      "epoch: 184 train_loss: 0.012603 06:05:44 00:31:48 left. \n",
      "epoch: 185 train_loss: 0.012593 06:07:44 00:29:49 left. \n",
      "epoch: 186 train_loss: 0.012591 06:09:45 00:27:49 left. \n",
      "epoch: 187 train_loss: 0.012584 06:11:43 00:25:50 left. \n",
      "epoch: 188 train_loss: 0.01258 06:13:42 00:23:51 left. \n",
      "epoch: 189 train_loss: 0.012574 06:15:42 00:21:51 left. \n",
      "epoch: 190 train_loss: 0.012572 06:17:41 00:19:52 left. \n",
      "epoch: 191 train_loss: 0.012568 06:19:42 00:17:53 left. \n",
      "epoch: 192 train_loss: 0.012561 06:21:41 00:15:54 left. \n",
      "epoch: 193 train_loss: 0.012559 06:23:37 00:13:54 left. \n",
      "epoch: 194 train_loss: 0.012556 06:25:38 00:11:55 left. \n",
      "epoch: 195 train_loss: 0.012555 06:27:37 00:09:56 left. \n",
      "epoch: 196 train_loss: 0.012551 06:29:36 00:07:57 left. \n",
      "epoch: 197 train_loss: 0.01255 06:31:36 00:05:57 left. \n",
      "epoch: 198 train_loss: 0.012551 06:33:36 00:03:58 left. \n",
      "epoch: 199 train_loss: 0.01255 06:35:35 00:01:59 left. \n",
      "epoch: 200 train_loss: 0.012549 06:37:35 00:00:00 left. \n",
      "Start SubNet warmup\n",
      "epoch: 1 train_loss: 0.013661 Test Acc.：245708/255000(96.36%),251961/255000(98.81%) 0.013277 00:03:05 10:14:21 left. \n",
      "epoch: 2 train_loss: 0.013161 Test Acc.：246131/255000(96.52%),252306/255000(98.94%) 0.013166 00:06:11 10:12:52 left. \n",
      "epoch: 3 train_loss: 0.01309 Test Acc.：246632/255000(96.72%),253018/255000(99.22%) 0.013051 00:09:18 10:11:28 left. \n",
      "epoch: 4 train_loss: 0.013094 Test Acc.：246632/255000(96.72%),253026/255000(99.23%) 0.013054 00:12:28 10:10:52 left. \n",
      "epoch: 5 train_loss: 0.013043 Test Acc.：246627/255000(96.72%),252824/255000(99.15%) 0.013056 00:15:34 10:07:13 left. \n",
      "epoch: 6 train_loss: 0.013035 Test Acc.：246879/255000(96.82%),253230/255000(99.31%) 0.013001 00:18:42 10:05:08 left. \n",
      "epoch: 7 train_loss: 0.013002 Test Acc.：246624/255000(96.72%),253054/255000(99.24%) 0.013055 00:21:52 10:02:57 left. \n",
      "epoch: 8 train_loss: 0.013002 Test Acc.：246927/255000(96.83%),253278/255000(99.32%) 0.012962 00:25:06 10:02:32 left. \n",
      "epoch: 9 train_loss: 0.012987 Test Acc.：246568/255000(96.69%),252830/255000(99.15%) 0.013076 00:28:13 09:59:04 left. \n",
      "epoch: 10 train_loss: 0.012986 Test Acc.：246532/255000(96.68%),252693/255000(99.10%) 0.013062 00:31:20 09:55:34 left. \n",
      "epoch: 11 train_loss: 0.012978 Test Acc.：246928/255000(96.83%),253251/255000(99.31%) 0.012978 00:34:28 09:52:22 left. \n",
      "epoch: 12 train_loss: 0.012978 Test Acc.：246833/255000(96.80%),253181/255000(99.29%) 0.012995 00:37:39 09:49:58 left. \n",
      "epoch: 13 train_loss: 0.012982 Test Acc.：246802/255000(96.79%),253128/255000(99.27%) 0.012976 00:40:44 09:46:00 left. \n",
      "epoch: 14 train_loss: 0.012977 Test Acc.：246920/255000(96.83%),253281/255000(99.33%) 0.012973 00:43:55 09:43:35 left. \n",
      "epoch: 15 train_loss: 0.012976 Test Acc.：246932/255000(96.84%),253179/255000(99.29%) 0.012962 00:47:06 09:40:58 left. \n",
      "epoch: 16 train_loss: 0.012964 Test Acc.：246987/255000(96.86%),253425/255000(99.38%) 0.012932 00:50:18 09:38:30 left. \n",
      "epoch: 17 train_loss: 0.012983 Test Acc.：246759/255000(96.77%),253096/255000(99.25%) 0.01301 00:53:25 09:35:06 left. \n",
      "epoch: 18 train_loss: 0.01296 Test Acc.：246356/255000(96.61%),252568/255000(99.05%) 0.013055 00:56:32 09:31:41 left. \n",
      "epoch: 19 train_loss: 0.012962 Test Acc.：247048/255000(96.88%),253426/255000(99.38%) 0.012928 00:59:42 09:28:48 left. \n",
      "epoch: 20 train_loss: 0.01296 Test Acc.：247064/255000(96.89%),253417/255000(99.38%) 0.012927 01:02:53 09:26:03 left. \n",
      "epoch: 21 train_loss: 0.01296 Test Acc.：246972/255000(96.85%),253299/255000(99.33%) 0.012952 01:06:02 09:22:56 left. \n",
      "epoch: 22 train_loss: 0.012945 Test Acc.：246961/255000(96.85%),253230/255000(99.31%) 0.012961 01:09:12 09:20:01 left. \n",
      "epoch: 23 train_loss: 0.01294 Test Acc.：246942/255000(96.84%),253284/255000(99.33%) 0.012954 01:12:26 09:17:27 left. \n",
      "epoch: 24 train_loss: 0.012956 Test Acc.：246708/255000(96.75%),253033/255000(99.23%) 0.013004 01:15:37 09:14:35 left. \n",
      "epoch: 25 train_loss: 0.012939 Test Acc.：246821/255000(96.79%),253177/255000(99.29%) 0.012988 01:18:49 09:11:46 left. \n",
      "epoch: 26 train_loss: 0.012932 Test Acc.：247248/255000(96.96%),253625/255000(99.46%) 0.012878 01:22:01 09:08:52 left. \n",
      "epoch: 27 train_loss: 0.012943 Test Acc.：247054/255000(96.88%),253424/255000(99.38%) 0.012925 01:25:08 09:05:34 left. \n",
      "epoch: 28 train_loss: 0.012926 Test Acc.：246983/255000(96.86%),253330/255000(99.35%) 0.012939 01:28:12 09:01:52 left. \n",
      "epoch: 29 train_loss: 0.012946 Test Acc.：247168/255000(96.93%),253592/255000(99.45%) 0.012915 01:31:24 08:58:58 left. \n",
      "epoch: 30 train_loss: 0.012929 Test Acc.：247131/255000(96.91%),253518/255000(99.42%) 0.012908 01:34:31 08:55:36 left. \n",
      "epoch: 31 train_loss: 0.012933 Test Acc.：247056/255000(96.88%),253454/255000(99.39%) 0.012924 01:37:41 08:52:32 left. \n",
      "epoch: 32 train_loss: 0.012942 Test Acc.：247152/255000(96.92%),253574/255000(99.44%) 0.012889 01:40:52 08:49:38 left. \n",
      "epoch: 33 train_loss: 0.012915 Test Acc.：247060/255000(96.89%),253474/255000(99.40%) 0.012922 01:44:01 08:46:27 left. \n",
      "epoch: 34 train_loss: 0.012949 Test Acc.：246949/255000(96.84%),253317/255000(99.34%) 0.012951 01:47:11 08:43:19 left. \n",
      "epoch: 35 train_loss: 0.012919 Test Acc.：247159/255000(96.93%),253552/255000(99.43%) 0.012927 01:50:21 08:40:15 left. \n",
      "epoch: 36 train_loss: 0.012922 Test Acc.：247171/255000(96.93%),253527/255000(99.42%) 0.012899 01:53:30 08:37:06 left. \n",
      "epoch: 37 train_loss: 0.012915 Test Acc.：247101/255000(96.90%),253442/255000(99.39%) 0.012892 01:56:38 08:33:51 left. \n",
      "epoch: 38 train_loss: 0.012938 Test Acc.：246894/255000(96.82%),253242/255000(99.31%) 0.012949 01:59:47 08:30:42 left. \n",
      "epoch: 39 train_loss: 0.012937 Test Acc.：247104/255000(96.90%),253446/255000(99.39%) 0.01293 02:02:57 08:27:37 left. \n",
      "epoch: 40 train_loss: 0.012916 Test Acc.：247064/255000(96.89%),253412/255000(99.38%) 0.012948 02:06:04 08:24:18 left. \n",
      "epoch: 41 train_loss: 0.012912 Test Acc.：247155/255000(96.92%),253532/255000(99.42%) 0.01291 02:09:14 08:21:10 left. \n",
      "epoch: 42 train_loss: 0.012899 Test Acc.：247288/255000(96.98%),253695/255000(99.49%) 0.012871 02:12:25 08:18:11 left. \n",
      "epoch: 43 train_loss: 0.012907 Test Acc.：247128/255000(96.91%),253497/255000(99.41%) 0.012909 02:15:36 08:15:08 left. \n",
      "epoch: 44 train_loss: 0.012898 Test Acc.：247177/255000(96.93%),253600/255000(99.45%) 0.0129 02:18:43 08:11:52 left. \n",
      "epoch: 45 train_loss: 0.012893 Test Acc.：247115/255000(96.91%),253474/255000(99.40%) 0.012917 02:21:53 08:08:43 left. \n",
      "epoch: 46 train_loss: 0.012919 Test Acc.：247188/255000(96.94%),253572/255000(99.44%) 0.012883 02:25:03 08:05:37 left. \n",
      "epoch: 47 train_loss: 0.012899 Test Acc.：247130/255000(96.91%),253504/255000(99.41%) 0.012913 02:28:15 08:02:39 left. \n",
      "epoch: 48 train_loss: 0.012905 Test Acc.：247141/255000(96.92%),253561/255000(99.44%) 0.012902 02:31:28 07:59:39 left. \n",
      "epoch: 49 train_loss: 0.012883 Test Acc.：247094/255000(96.90%),253412/255000(99.38%) 0.012941 02:34:36 07:56:27 left. \n",
      "epoch: 50 train_loss: 0.012893 Test Acc.：247301/255000(96.98%),253730/255000(99.50%) 0.01286 02:37:49 07:53:29 left. \n",
      "epoch: 51 train_loss: 0.012877 Test Acc.：247099/255000(96.90%),253456/255000(99.39%) 0.012907 02:40:58 07:50:17 left. \n",
      "epoch: 52 train_loss: 0.012884 Test Acc.：247320/255000(96.99%),253706/255000(99.49%) 0.012865 02:44:08 07:47:11 left. \n",
      "epoch: 53 train_loss: 0.012898 Test Acc.：247139/255000(96.92%),253559/255000(99.43%) 0.012906 02:47:17 07:44:01 left. \n",
      "epoch: 54 train_loss: 0.012892 Test Acc.：247252/255000(96.96%),253664/255000(99.48%) 0.012879 02:50:28 07:40:55 left. \n",
      "epoch: 55 train_loss: 0.012876 Test Acc.：247360/255000(97.00%),253790/255000(99.53%) 0.01286 02:53:36 07:37:40 left. \n",
      "epoch: 56 train_loss: 0.012882 Test Acc.：247342/255000(97.00%),253777/255000(99.52%) 0.012851 02:56:45 07:34:32 left. \n",
      "epoch: 57 train_loss: 0.012885 Test Acc.：247326/255000(96.99%),253704/255000(99.49%) 0.012864 02:59:56 07:31:24 left. \n",
      "epoch: 58 train_loss: 0.012876 Test Acc.：247408/255000(97.02%),253819/255000(99.54%) 0.012853 03:03:06 07:28:17 left. \n",
      "epoch: 59 train_loss: 0.012867 Test Acc.：247218/255000(96.95%),253673/255000(99.48%) 0.012884 03:06:16 07:25:10 left. \n",
      "epoch: 60 train_loss: 0.012868 Test Acc.：247096/255000(96.90%),253462/255000(99.40%) 0.012913 03:09:23 07:21:55 left. \n",
      "epoch: 61 train_loss: 0.012897 Test Acc.：247151/255000(96.92%),253510/255000(99.42%) 0.012935 03:12:35 07:18:51 left. \n",
      "epoch: 62 train_loss: 0.012866 Test Acc.：247229/255000(96.95%),253505/255000(99.41%) 0.012899 03:15:41 07:15:35 left. \n",
      "epoch: 63 train_loss: 0.012862 Test Acc.：247119/255000(96.91%),253465/255000(99.40%) 0.012906 03:18:51 07:12:25 left. \n",
      "epoch: 64 train_loss: 0.012861 Test Acc.：247337/255000(96.99%),253712/255000(99.49%) 0.012869 03:21:59 07:09:13 left. \n",
      "epoch: 65 train_loss: 0.012878 Test Acc.：247201/255000(96.94%),253587/255000(99.45%) 0.012917 03:25:04 07:05:55 left. \n",
      "epoch: 66 train_loss: 0.012871 Test Acc.：247390/255000(97.02%),253847/255000(99.55%) 0.012828 03:28:10 07:02:39 left. \n",
      "epoch: 67 train_loss: 0.01287 Test Acc.：247283/255000(96.97%),253664/255000(99.48%) 0.01287 03:31:21 06:59:33 left. \n",
      "epoch: 68 train_loss: 0.012859 Test Acc.：247407/255000(97.02%),253777/255000(99.52%) 0.012844 03:34:31 06:56:25 left. \n",
      "epoch: 69 train_loss: 0.012852 Test Acc.：247391/255000(97.02%),253770/255000(99.52%) 0.012848 03:37:45 06:53:25 left. \n",
      "epoch: 70 train_loss: 0.012856 Test Acc.：247292/255000(96.98%),253703/255000(99.49%) 0.012873 03:40:56 06:50:18 left. \n",
      "epoch: 71 train_loss: 0.012848 Test Acc.：247367/255000(97.01%),253827/255000(99.54%) 0.012854 03:44:09 06:47:17 left. \n",
      "epoch: 72 train_loss: 0.012847 Test Acc.：247280/255000(96.97%),253578/255000(99.44%) 0.012861 03:47:17 06:44:03 left. \n",
      "epoch: 73 train_loss: 0.01285 Test Acc.：247376/255000(97.01%),253779/255000(99.52%) 0.012863 03:50:25 06:40:53 left. \n",
      "epoch: 74 train_loss: 0.012851 Test Acc.：247150/255000(96.92%),253444/255000(99.39%) 0.012889 03:53:33 06:37:41 left. \n",
      "epoch: 75 train_loss: 0.012844 Test Acc.：247410/255000(97.02%),253801/255000(99.53%) 0.012847 03:56:42 06:34:30 left. \n",
      "epoch: 76 train_loss: 0.012866 Test Acc.：247121/255000(96.91%),253496/255000(99.41%) 0.012906 03:59:53 06:31:23 left. \n",
      "epoch: 77 train_loss: 0.012836 Test Acc.：247266/255000(96.97%),253629/255000(99.46%) 0.012888 04:03:04 06:28:16 left. \n",
      "epoch: 78 train_loss: 0.012844 Test Acc.：247223/255000(96.95%),253614/255000(99.46%) 0.012886 04:06:13 06:25:06 left. \n",
      "epoch: 79 train_loss: 0.012863 Test Acc.：247258/255000(96.96%),253677/255000(99.48%) 0.012889 04:09:21 06:21:55 left. \n",
      "epoch: 80 train_loss: 0.012844 Test Acc.：247365/255000(97.01%),253774/255000(99.52%) 0.012847 04:12:33 06:18:49 left. \n",
      "epoch: 81 train_loss: 0.012824 Test Acc.：247334/255000(96.99%),253769/255000(99.52%) 0.012867 04:15:42 06:15:40 left. \n",
      "epoch: 82 train_loss: 0.012827 Test Acc.：247454/255000(97.04%),253845/255000(99.55%) 0.012837 04:18:51 06:12:29 left. \n",
      "epoch: 83 train_loss: 0.012832 Test Acc.：247361/255000(97.00%),253799/255000(99.53%) 0.01286 04:21:58 06:09:18 left. \n",
      "epoch: 84 train_loss: 0.012832 Test Acc.：247124/255000(96.91%),253458/255000(99.40%) 0.01292 04:25:12 06:06:14 left. \n",
      "epoch: 85 train_loss: 0.012838 Test Acc.：247144/255000(96.92%),253422/255000(99.38%) 0.012899 04:28:19 06:03:01 left. \n",
      "epoch: 86 train_loss: 0.012824 Test Acc.：247301/255000(96.98%),253692/255000(99.49%) 0.012862 04:31:29 05:59:53 left. \n",
      "epoch: 87 train_loss: 0.012822 Test Acc.：247359/255000(97.00%),253730/255000(99.50%) 0.012855 04:34:39 05:56:45 left. \n",
      "epoch: 88 train_loss: 0.012831 Test Acc.：247231/255000(96.95%),253473/255000(99.40%) 0.012869 04:37:49 05:53:36 left. \n",
      "epoch: 89 train_loss: 0.012844 Test Acc.：247445/255000(97.04%),253878/255000(99.56%) 0.012838 04:40:56 05:50:23 left. \n",
      "epoch: 90 train_loss: 0.012825 Test Acc.：247053/255000(96.88%),253520/255000(99.42%) 0.01289 04:44:05 05:47:13 left. \n",
      "epoch: 91 train_loss: 0.01282 Test Acc.：247453/255000(97.04%),253903/255000(99.57%) 0.012821 04:47:13 05:44:01 left. \n",
      "epoch: 92 train_loss: 0.012817 Test Acc.：247442/255000(97.04%),253866/255000(99.56%) 0.012822 04:50:18 05:40:48 left. \n",
      "epoch: 93 train_loss: 0.012805 Test Acc.：247468/255000(97.05%),253884/255000(99.56%) 0.012828 04:53:25 05:37:36 left. \n",
      "epoch: 94 train_loss: 0.012834 Test Acc.：247084/255000(96.90%),253381/255000(99.37%) 0.012925 04:56:39 05:34:32 left. \n",
      "epoch: 95 train_loss: 0.01283 Test Acc.：247343/255000(97.00%),253747/255000(99.51%) 0.012863 04:59:49 05:31:22 left. \n",
      "epoch: 96 train_loss: 0.01282 Test Acc.：247561/255000(97.08%),253984/255000(99.60%) 0.012802 05:02:59 05:28:14 left. \n",
      "epoch: 97 train_loss: 0.012831 Test Acc.：247369/255000(97.01%),253774/255000(99.52%) 0.012857 05:06:09 05:25:05 left. \n",
      "epoch: 98 train_loss: 0.012815 Test Acc.：247466/255000(97.05%),253828/255000(99.54%) 0.012832 05:09:17 05:21:55 left. \n",
      "epoch: 99 train_loss: 0.012815 Test Acc.：247491/255000(97.06%),253910/255000(99.57%) 0.012824 05:12:27 05:18:46 left. \n",
      "epoch: 100 train_loss: 0.012789 Test Acc.：247464/255000(97.04%),253854/255000(99.55%) 0.012823 05:15:35 05:15:35 left. \n",
      "epoch: 101 train_loss: 0.012797 Test Acc.：247410/255000(97.02%),253819/255000(99.54%) 0.012841 05:18:44 05:12:25 left. \n",
      "epoch: 102 train_loss: 0.0128 Test Acc.：247514/255000(97.06%),253947/255000(99.59%) 0.012829 05:21:48 05:09:11 left. \n",
      "epoch: 103 train_loss: 0.012798 Test Acc.：247489/255000(97.05%),253930/255000(99.58%) 0.01282 05:24:57 05:06:01 left. \n",
      "epoch: 104 train_loss: 0.012791 Test Acc.：247419/255000(97.03%),253859/255000(99.55%) 0.01283 05:28:05 05:02:51 left. \n",
      "epoch: 105 train_loss: 0.012777 Test Acc.：247523/255000(97.07%),253941/255000(99.58%) 0.012824 05:31:12 04:59:40 left. \n",
      "epoch: 106 train_loss: 0.012777 Test Acc.：247456/255000(97.04%),253893/255000(99.57%) 0.012823 05:34:23 04:56:32 left. \n",
      "epoch: 107 train_loss: 0.012795 Test Acc.：247468/255000(97.05%),253903/255000(99.57%) 0.012831 05:37:29 04:53:19 left. \n",
      "epoch: 108 train_loss: 0.012806 Test Acc.：247479/255000(97.05%),253960/255000(99.59%) 0.012813 05:40:38 04:50:10 left. \n",
      "epoch: 109 train_loss: 0.012783 Test Acc.：247361/255000(97.00%),253762/255000(99.51%) 0.012853 05:43:47 04:47:01 left. \n",
      "epoch: 110 train_loss: 0.012768 Test Acc.：247510/255000(97.06%),253981/255000(99.60%) 0.0128 05:46:55 04:43:50 left. \n",
      "epoch: 111 train_loss: 0.012765 Test Acc.：247476/255000(97.05%),253915/255000(99.57%) 0.012816 05:50:06 04:40:43 left. \n",
      "epoch: 112 train_loss: 0.012769 Test Acc.：247548/255000(97.08%),253954/255000(99.59%) 0.012805 05:53:16 04:37:34 left. \n",
      "epoch: 113 train_loss: 0.012767 Test Acc.：247311/255000(96.98%),253696/255000(99.49%) 0.012856 05:56:25 04:34:25 left. \n",
      "epoch: 114 train_loss: 0.012766 Test Acc.：247584/255000(97.09%),254021/255000(99.62%) 0.012796 05:59:33 04:31:14 left. \n",
      "epoch: 115 train_loss: 0.012757 Test Acc.：247568/255000(97.09%),254033/255000(99.62%) 0.012807 06:02:40 04:28:03 left. \n",
      "epoch: 116 train_loss: 0.012765 Test Acc.：247605/255000(97.10%),254026/255000(99.62%) 0.012795 06:05:48 04:24:53 left. \n",
      "epoch: 117 train_loss: 0.01276 Test Acc.：247514/255000(97.06%),253920/255000(99.58%) 0.012816 06:08:56 04:21:43 left. \n",
      "epoch: 118 train_loss: 0.01277 Test Acc.：247479/255000(97.05%),253898/255000(99.57%) 0.012819 06:11:55 04:18:27 left. \n",
      "epoch: 119 train_loss: 0.012749 Test Acc.：247593/255000(97.10%),254030/255000(99.62%) 0.012795 06:14:34 04:14:57 left. \n",
      "epoch: 120 train_loss: 0.012745 Test Acc.：247609/255000(97.10%),254075/255000(99.64%) 0.012794 06:17:17 04:11:31 left. \n",
      "epoch: 121 train_loss: 0.012756 Test Acc.：247587/255000(97.09%),254007/255000(99.61%) 0.012808 06:20:23 04:08:21 left. \n",
      "epoch: 122 train_loss: 0.012744 Test Acc.：247594/255000(97.10%),254043/255000(99.62%) 0.01278 06:23:30 04:05:11 left. \n",
      "epoch: 123 train_loss: 0.01274 Test Acc.：247499/255000(97.06%),253914/255000(99.57%) 0.012815 06:26:34 04:01:59 left. \n",
      "epoch: 124 train_loss: 0.012741 Test Acc.：247639/255000(97.11%),254085/255000(99.64%) 0.012787 06:29:39 03:58:49 left. \n",
      "epoch: 125 train_loss: 0.012755 Test Acc.：247654/255000(97.12%),254084/255000(99.64%) 0.012791 06:32:48 03:55:40 left. \n",
      "epoch: 126 train_loss: 0.012741 Test Acc.：247545/255000(97.08%),253979/255000(99.60%) 0.012815 06:35:55 03:52:31 left. \n",
      "epoch: 127 train_loss: 0.012736 Test Acc.：247595/255000(97.10%),254030/255000(99.62%) 0.012799 06:39:01 03:49:21 left. \n",
      "epoch: 128 train_loss: 0.012742 Test Acc.：247535/255000(97.07%),254015/255000(99.61%) 0.012806 06:42:07 03:46:11 left. \n",
      "epoch: 129 train_loss: 0.012729 Test Acc.：247587/255000(97.09%),254015/255000(99.61%) 0.01282 06:45:18 03:43:04 left. \n",
      "epoch: 130 train_loss: 0.012725 Test Acc.：247449/255000(97.04%),253883/255000(99.56%) 0.012815 06:48:24 03:39:54 left. \n",
      "epoch: 131 train_loss: 0.012728 Test Acc.：247556/255000(97.08%),253965/255000(99.59%) 0.012788 06:51:35 03:36:47 left. \n",
      "epoch: 132 train_loss: 0.012719 Test Acc.：247646/255000(97.12%),254115/255000(99.65%) 0.012766 06:54:40 03:33:37 left. \n",
      "epoch: 133 train_loss: 0.01272 Test Acc.：247616/255000(97.10%),254034/255000(99.62%) 0.012801 06:57:48 03:30:28 left. \n",
      "epoch: 134 train_loss: 0.012725 Test Acc.：247586/255000(97.09%),254026/255000(99.62%) 0.012818 07:01:00 03:27:21 left. \n",
      "epoch: 135 train_loss: 0.01272 Test Acc.：247697/255000(97.14%),254164/255000(99.67%) 0.012764 07:04:10 03:24:13 left. \n",
      "epoch: 136 train_loss: 0.012719 Test Acc.：247656/255000(97.12%),254072/255000(99.64%) 0.012808 07:07:16 03:21:04 left. \n",
      "epoch: 137 train_loss: 0.012713 Test Acc.：247627/255000(97.11%),254062/255000(99.63%) 0.012785 07:10:18 03:17:52 left. \n",
      "epoch: 138 train_loss: 0.012711 Test Acc.：247667/255000(97.12%),254134/255000(99.66%) 0.012776 07:13:28 03:14:44 left. \n",
      "epoch: 139 train_loss: 0.012701 Test Acc.：247618/255000(97.11%),254037/255000(99.62%) 0.01279 07:16:36 03:11:36 left. \n",
      "epoch: 140 train_loss: 0.012713 Test Acc.：247559/255000(97.08%),254050/255000(99.63%) 0.012808 07:19:47 03:08:29 left. \n",
      "epoch: 141 train_loss: 0.012703 Test Acc.：247603/255000(97.10%),254007/255000(99.61%) 0.012781 07:23:01 03:05:22 left. \n",
      "epoch: 142 train_loss: 0.012705 Test Acc.：247651/255000(97.12%),254130/255000(99.66%) 0.012766 07:26:07 03:02:13 left. \n",
      "epoch: 143 train_loss: 0.012698 Test Acc.：247675/255000(97.13%),254075/255000(99.64%) 0.012772 07:29:10 02:59:02 left. \n",
      "epoch: 144 train_loss: 0.012695 Test Acc.：247662/255000(97.12%),254089/255000(99.64%) 0.012776 07:32:14 02:55:52 left. \n",
      "epoch: 145 train_loss: 0.012691 Test Acc.：247676/255000(97.13%),254180/255000(99.68%) 0.012778 07:35:25 02:52:44 left. \n",
      "epoch: 146 train_loss: 0.012691 Test Acc.：247605/255000(97.10%),253932/255000(99.58%) 0.012789 07:38:38 02:49:37 left. \n",
      "epoch: 147 train_loss: 0.012692 Test Acc.：247706/255000(97.14%),254131/255000(99.66%) 0.012768 07:41:46 02:46:29 left. \n",
      "epoch: 148 train_loss: 0.012682 Test Acc.：247726/255000(97.15%),254157/255000(99.67%) 0.01276 07:44:57 02:43:21 left. \n",
      "epoch: 149 train_loss: 0.012681 Test Acc.：247720/255000(97.15%),254137/255000(99.66%) 0.012765 07:48:04 02:40:12 left. \n",
      "epoch: 150 train_loss: 0.012683 Test Acc.：247714/255000(97.14%),254165/255000(99.67%) 0.012758 07:51:12 02:37:04 left. \n",
      "epoch: 151 train_loss: 0.01268 Test Acc.：247690/255000(97.13%),254171/255000(99.67%) 0.01277 07:54:20 02:33:55 left. \n",
      "epoch: 152 train_loss: 0.012678 Test Acc.：247691/255000(97.13%),254172/255000(99.68%) 0.01278 07:57:33 02:30:48 left. \n",
      "epoch: 153 train_loss: 0.012677 Test Acc.：247669/255000(97.13%),254153/255000(99.67%) 0.012775 08:00:39 02:27:39 left. \n",
      "epoch: 154 train_loss: 0.012672 Test Acc.：247761/255000(97.16%),254197/255000(99.69%) 0.012757 08:03:47 02:24:30 left. \n",
      "epoch: 155 train_loss: 0.012669 Test Acc.：247742/255000(97.15%),254198/255000(99.69%) 0.012764 08:06:59 02:21:23 left. \n",
      "epoch: 156 train_loss: 0.012667 Test Acc.：247675/255000(97.13%),254155/255000(99.67%) 0.012769 08:10:06 02:18:14 left. \n",
      "epoch: 157 train_loss: 0.01267 Test Acc.：247686/255000(97.13%),254129/255000(99.66%) 0.012757 08:13:14 02:15:05 left. \n",
      "epoch: 158 train_loss: 0.012662 Test Acc.：247584/255000(97.09%),254040/255000(99.62%) 0.012781 08:16:22 02:11:56 left. \n",
      "epoch: 159 train_loss: 0.012664 Test Acc.：247681/255000(97.13%),254179/255000(99.68%) 0.01276 08:19:31 02:08:48 left. \n",
      "epoch: 160 train_loss: 0.012662 Test Acc.：247762/255000(97.16%),254193/255000(99.68%) 0.012769 08:22:41 02:05:40 left. \n",
      "epoch: 161 train_loss: 0.012658 Test Acc.：247718/255000(97.14%),254220/255000(99.69%) 0.012762 08:25:51 02:02:32 left. \n",
      "epoch: 162 train_loss: 0.012654 Test Acc.：247645/255000(97.12%),254126/255000(99.66%) 0.01278 08:29:01 01:59:23 left. \n",
      "epoch: 163 train_loss: 0.012659 Test Acc.：247684/255000(97.13%),254108/255000(99.65%) 0.01277 08:32:12 01:56:15 left. \n",
      "epoch: 164 train_loss: 0.012653 Test Acc.：247766/255000(97.16%),254218/255000(99.69%) 0.01277 08:35:17 01:53:06 left. \n",
      "epoch: 165 train_loss: 0.012651 Test Acc.：247778/255000(97.17%),254237/255000(99.70%) 0.012756 08:38:26 01:49:58 left. \n",
      "epoch: 166 train_loss: 0.012648 Test Acc.：247693/255000(97.13%),254098/255000(99.65%) 0.012758 08:41:35 01:46:49 left. \n",
      "epoch: 167 train_loss: 0.012645 Test Acc.：247770/255000(97.16%),254215/255000(99.69%) 0.012756 08:44:44 01:43:41 left. \n",
      "epoch: 168 train_loss: 0.012647 Test Acc.：247708/255000(97.14%),254148/255000(99.67%) 0.012758 08:47:51 01:40:32 left. \n",
      "epoch: 169 train_loss: 0.012644 Test Acc.：247750/255000(97.16%),254179/255000(99.68%) 0.012748 08:50:59 01:37:24 left. \n",
      "epoch: 170 train_loss: 0.012644 Test Acc.：247709/255000(97.14%),254127/255000(99.66%) 0.012769 08:54:11 01:34:16 left. \n",
      "epoch: 171 train_loss: 0.012639 Test Acc.：247702/255000(97.14%),254149/255000(99.67%) 0.012764 08:57:14 01:31:06 left. \n",
      "epoch: 172 train_loss: 0.012636 Test Acc.：247717/255000(97.14%),254184/255000(99.68%) 0.012754 09:00:23 01:27:58 left. \n",
      "epoch: 173 train_loss: 0.012639 Test Acc.：247769/255000(97.16%),254206/255000(99.69%) 0.012757 09:03:32 01:24:49 left. \n",
      "epoch: 174 train_loss: 0.012635 Test Acc.：247702/255000(97.14%),254203/255000(99.69%) 0.012767 09:06:40 01:21:41 left. \n",
      "epoch: 175 train_loss: 0.012639 Test Acc.：247819/255000(97.18%),254252/255000(99.71%) 0.012753 09:09:48 01:18:32 left. \n",
      "epoch: 176 train_loss: 0.01263 Test Acc.：247710/255000(97.14%),254169/255000(99.67%) 0.012767 09:12:55 01:15:23 left. \n",
      "epoch: 177 train_loss: 0.012631 Test Acc.：247779/255000(97.17%),254235/255000(99.70%) 0.012749 09:16:07 01:12:15 left. \n",
      "epoch: 178 train_loss: 0.01263 Test Acc.：247751/255000(97.16%),254204/255000(99.69%) 0.012742 09:19:16 01:09:07 left. \n",
      "epoch: 179 train_loss: 0.01263 Test Acc.：247784/255000(97.17%),254219/255000(99.69%) 0.012757 09:22:26 01:05:59 left. \n",
      "epoch: 180 train_loss: 0.012627 Test Acc.：247757/255000(97.16%),254209/255000(99.69%) 0.012748 09:25:32 01:02:50 left. \n",
      "epoch: 181 train_loss: 0.012625 Test Acc.：247751/255000(97.16%),254209/255000(99.69%) 0.012748 09:28:40 00:59:41 left. \n",
      "epoch: 182 train_loss: 0.012624 Test Acc.：247794/255000(97.17%),254222/255000(99.69%) 0.012747 09:31:47 00:56:33 left. \n",
      "epoch: 183 train_loss: 0.012622 Test Acc.：247769/255000(97.16%),254227/255000(99.70%) 0.012752 09:34:56 00:53:24 left. \n",
      "epoch: 184 train_loss: 0.012623 Test Acc.：247786/255000(97.17%),254202/255000(99.69%) 0.012749 09:38:04 00:50:16 left. \n",
      "epoch: 185 train_loss: 0.01262 Test Acc.：247761/255000(97.16%),254239/255000(99.70%) 0.012756 09:41:14 00:47:07 left. \n",
      "epoch: 186 train_loss: 0.01262 Test Acc.：247765/255000(97.16%),254220/255000(99.69%) 0.012753 09:44:22 00:43:59 left. \n",
      "epoch: 187 train_loss: 0.012617 Test Acc.：247794/255000(97.17%),254230/255000(99.70%) 0.012752 09:47:32 00:40:50 left. \n",
      "epoch: 188 train_loss: 0.012617 Test Acc.：247767/255000(97.16%),254217/255000(99.69%) 0.012743 09:50:39 00:37:42 left. \n",
      "epoch: 189 train_loss: 0.012621 Test Acc.：247769/255000(97.16%),254241/255000(99.70%) 0.01275 09:53:48 00:34:33 left. \n",
      "epoch: 190 train_loss: 0.012616 Test Acc.：247764/255000(97.16%),254234/255000(99.70%) 0.012749 09:56:55 00:31:25 left. \n",
      "epoch: 191 train_loss: 0.012613 Test Acc.：247782/255000(97.17%),254242/255000(99.70%) 0.01274 10:00:07 00:28:16 left. \n",
      "epoch: 192 train_loss: 0.012616 Test Acc.：247790/255000(97.17%),254240/255000(99.70%) 0.012741 10:03:17 00:25:08 left. \n",
      "epoch: 193 train_loss: 0.012619 Test Acc.：247774/255000(97.17%),254234/255000(99.70%) 0.01275 10:06:22 00:21:59 left. \n",
      "epoch: 194 train_loss: 0.012616 Test Acc.：247784/255000(97.17%),254228/255000(99.70%) 0.012749 10:09:29 00:18:51 left. \n",
      "epoch: 195 train_loss: 0.012617 Test Acc.：247793/255000(97.17%),254248/255000(99.71%) 0.012744 10:12:37 00:15:42 left. \n",
      "epoch: 196 train_loss: 0.012618 Test Acc.：247784/255000(97.17%),254235/255000(99.70%) 0.012742 10:15:46 00:12:34 left. \n",
      "epoch: 197 train_loss: 0.012613 Test Acc.：247770/255000(97.16%),254240/255000(99.70%) 0.012744 10:18:52 00:09:25 left. \n",
      "epoch: 198 train_loss: 0.012619 Test Acc.：247796/255000(97.17%),254241/255000(99.70%) 0.012741 10:22:02 00:06:16 left. \n",
      "epoch: 199 train_loss: 0.012614 Test Acc.：247802/255000(97.18%),254246/255000(99.70%) 0.012745 10:25:12 00:03:08 left. \n",
      "epoch: 200 train_loss: 0.012614 Test Acc.：247794/255000(97.17%),254246/255000(99.70%) 0.01275 10:28:19 00:00:00 left. \n",
      "Test Acc.：247794/255000(97.17%),254246/255000(99.70%) 0.01275 \n",
      "epoch: 1 train_loss: 0.014593 Test Acc.：242567/255000(95.12%),253463/255000(99.40%) 0.013849 Save model 00:03:19 11:02:04 left. \n",
      "epoch: 2 train_loss: 0.01396 Test Acc.：242828/255000(95.23%),253417/255000(99.38%) 0.013818 Save model 00:06:37 10:56:41 left. \n",
      "epoch: 3 train_loss: 0.013869 Test Acc.：243193/255000(95.37%),253609/255000(99.45%) 0.013643 Save model 00:09:58 10:55:04 left. \n",
      "epoch: 4 train_loss: 0.013863 Test Acc.：243273/255000(95.40%),253362/255000(99.36%) 0.013697 Save model 00:13:15 10:49:25 left. \n",
      "epoch: 5 train_loss: 0.013832 Test Acc.：243108/255000(95.34%),253365/255000(99.36%) 0.013588 Save model 00:16:36 10:48:00 left. \n",
      "epoch: 6 train_loss: 0.013835 Test Acc.：243172/255000(95.36%),253164/255000(99.28%) 0.013634 00:19:57 10:45:33 left. \n",
      "epoch: 7 train_loss: 0.013833 Test Acc.：243637/255000(95.54%),253451/255000(99.39%) 0.013668 00:23:18 10:42:44 left. \n",
      "epoch: 8 train_loss: 0.013828 Test Acc.：243225/255000(95.38%),253408/255000(99.38%) 0.013698 Save model 00:26:36 10:38:39 left. \n",
      "epoch: 9 train_loss: 0.013812 Test Acc.：243884/255000(95.64%),253352/255000(99.35%) 0.013515 Save model 00:29:56 10:35:21 left. \n",
      "epoch: 10 train_loss: 0.013821 Test Acc.：243578/255000(95.52%),253264/255000(99.32%) 0.01356 00:33:14 10:31:37 left. \n",
      "epoch: 11 train_loss: 0.013807 Test Acc.：243344/255000(95.43%),253010/255000(99.22%) 0.013573 Save model 00:36:31 10:27:31 left. \n",
      "epoch: 12 train_loss: 0.013818 Test Acc.：243027/255000(95.30%),253237/255000(99.31%) 0.013695 00:39:50 10:24:06 left. \n",
      "epoch: 13 train_loss: 0.013799 Test Acc.：243435/255000(95.46%),252987/255000(99.21%) 0.013637 Save model 00:43:09 10:20:54 left. \n",
      "epoch: 14 train_loss: 0.013801 Test Acc.：243570/255000(95.52%),253137/255000(99.27%) 0.013572 00:46:27 10:17:16 left. \n",
      "epoch: 15 train_loss: 0.013797 Test Acc.：243489/255000(95.49%),253154/255000(99.28%) 0.013689 Save model 00:49:48 10:14:13 left. \n",
      "epoch: 16 train_loss: 0.013784 Test Acc.：243282/255000(95.40%),252930/255000(99.19%) 0.013638 Save model 00:53:07 10:10:53 left. \n",
      "epoch: 17 train_loss: 0.013779 Test Acc.：243306/255000(95.41%),253144/255000(99.27%) 0.013562 Save model 00:56:23 10:06:59 left. \n",
      "epoch: 18 train_loss: 0.013782 Test Acc.：243683/255000(95.56%),252913/255000(99.18%) 0.013575 00:59:37 10:02:54 left. \n",
      "epoch: 19 train_loss: 0.01379 Test Acc.：240097/255000(94.16%),243876/255000(95.64%) 0.014304 01:03:00 10:00:14 left. \n",
      "epoch: 20 train_loss: 0.013779 Test Acc.：243754/255000(95.59%),253014/255000(99.22%) 0.01351 Save model 01:06:22 09:57:19 left. \n",
      "epoch: 21 train_loss: 0.013763 Test Acc.：243344/255000(95.43%),253380/255000(99.36%) 0.013599 Save model 01:09:41 09:54:05 left. \n",
      "epoch: 22 train_loss: 0.013782 Test Acc.：243342/255000(95.43%),253145/255000(99.27%) 0.013671 01:13:01 09:50:47 left. \n",
      "epoch: 23 train_loss: 0.013764 Test Acc.：243205/255000(95.37%),253029/255000(99.23%) 0.013584 01:16:21 09:47:41 left. \n",
      "epoch: 24 train_loss: 0.013768 Test Acc.：243805/255000(95.61%),253069/255000(99.24%) 0.013538 01:19:40 09:44:16 left. \n",
      "epoch: 25 train_loss: 0.013757 Test Acc.：243553/255000(95.51%),252872/255000(99.17%) 0.013547 Save model 01:22:57 09:40:45 left. \n",
      "epoch: 26 train_loss: 0.013762 Test Acc.：243486/255000(95.48%),253046/255000(99.23%) 0.013549 01:26:14 09:37:08 left. \n",
      "epoch: 27 train_loss: 0.013743 Test Acc.：243403/255000(95.45%),253067/255000(99.24%) 0.013535 Save model 01:29:36 09:34:08 left. \n",
      "epoch: 28 train_loss: 0.01374 Test Acc.：243834/255000(95.62%),253072/255000(99.24%) 0.013479 Save model 01:32:54 09:30:43 left. \n",
      "epoch: 29 train_loss: 0.013759 Test Acc.：243679/255000(95.56%),253001/255000(99.22%) 0.013577 01:36:14 09:27:30 left. \n",
      "epoch: 30 train_loss: 0.013757 Test Acc.：243468/255000(95.48%),252964/255000(99.20%) 0.013587 01:39:37 09:24:34 left. \n",
      "epoch: 31 train_loss: 0.01375 Test Acc.：243733/255000(95.58%),252900/255000(99.18%) 0.013588 01:42:59 09:21:27 left. \n",
      "epoch: 32 train_loss: 0.013716 Test Acc.：243658/255000(95.55%),252954/255000(99.20%) 0.013543 Save model 01:46:21 09:18:24 left. \n",
      "epoch: 33 train_loss: 0.013714 Test Acc.：243607/255000(95.53%),253057/255000(99.24%) 0.013498 Save model 01:49:46 09:15:31 left. \n",
      "epoch: 34 train_loss: 0.013724 Test Acc.：243447/255000(95.47%),253042/255000(99.23%) 0.013573 01:53:07 09:12:18 left. \n",
      "epoch: 35 train_loss: 0.01371 Test Acc.：244041/255000(95.70%),253211/255000(99.30%) 0.013508 Save model 01:56:33 09:09:30 left. \n",
      "epoch: 36 train_loss: 0.013701 Test Acc.：244077/255000(95.72%),253267/255000(99.32%) 0.013462 Save model 01:59:57 09:06:28 left. \n",
      "epoch: 37 train_loss: 0.01372 Test Acc.：243482/255000(95.48%),252989/255000(99.21%) 0.01359 02:03:19 09:03:18 left. \n",
      "epoch: 38 train_loss: 0.0137 Test Acc.：243614/255000(95.53%),252928/255000(99.19%) 0.013551 Save model 02:06:42 09:00:11 left. \n",
      "epoch: 39 train_loss: 0.013682 Test Acc.：243872/255000(95.64%),253233/255000(99.31%) 0.013534 Save model 02:10:01 08:56:48 left. \n",
      "epoch: 40 train_loss: 0.013678 Test Acc.：243918/255000(95.65%),253261/255000(99.32%) 0.013518 Save model 02:13:15 08:53:03 left. \n",
      "epoch: 41 train_loss: 0.013681 Test Acc.：244025/255000(95.70%),252980/255000(99.21%) 0.013531 02:16:37 08:49:51 left. \n",
      "epoch: 42 train_loss: 0.013678 Test Acc.：243864/255000(95.63%),253122/255000(99.26%) 0.013604 Save model 02:19:56 08:46:27 left. \n",
      "epoch: 43 train_loss: 0.013674 Test Acc.：244240/255000(95.78%),253166/255000(99.28%) 0.013427 Save model 02:23:10 08:42:45 left. \n",
      "epoch: 44 train_loss: 0.013662 Test Acc.：243790/255000(95.60%),253041/255000(99.23%) 0.013522 Save model 02:26:30 08:39:26 left. \n",
      "epoch: 45 train_loss: 0.013664 Test Acc.：243754/255000(95.59%),253225/255000(99.30%) 0.01354 02:29:50 08:36:06 left. \n",
      "epoch: 46 train_loss: 0.013671 Test Acc.：243796/255000(95.61%),252927/255000(99.19%) 0.013509 02:33:09 08:32:43 left. \n",
      "epoch: 47 train_loss: 0.013651 Test Acc.：244186/255000(95.76%),253326/255000(99.34%) 0.013448 Save model 02:36:28 08:29:21 left. \n",
      "epoch: 48 train_loss: 0.013641 Test Acc.：243544/255000(95.51%),253085/255000(99.25%) 0.013506 Save model 02:39:43 08:25:48 left. \n",
      "epoch: 49 train_loss: 0.01364 Test Acc.：244307/255000(95.81%),253264/255000(99.32%) 0.013485 Save model 02:43:12 08:22:57 left. \n",
      "epoch: 50 train_loss: 0.01364 Test Acc.：244152/255000(95.75%),253253/255000(99.31%) 0.013525 02:46:34 08:19:42 left. \n",
      "epoch: 51 train_loss: 0.013646 Test Acc.：243822/255000(95.62%),253012/255000(99.22%) 0.01351 02:49:55 08:16:26 left. \n",
      "epoch: 52 train_loss: 0.013625 Test Acc.：244256/255000(95.79%),253114/255000(99.26%) 0.013417 Save model 02:53:16 08:13:10 left. \n",
      "epoch: 53 train_loss: 0.013644 Test Acc.：243492/255000(95.49%),253050/255000(99.24%) 0.013492 02:56:35 08:09:48 left. \n",
      "epoch: 54 train_loss: 0.013606 Test Acc.：244050/255000(95.71%),253077/255000(99.25%) 0.013474 Save model 02:59:56 08:06:29 left. \n",
      "epoch: 55 train_loss: 0.01361 Test Acc.：244417/255000(95.85%),252899/255000(99.18%) 0.013381 03:03:21 08:03:22 left. \n",
      "epoch: 56 train_loss: 0.013623 Test Acc.：243823/255000(95.62%),252305/255000(98.94%) 0.013518 03:06:43 08:00:08 left. \n",
      "epoch: 57 train_loss: 0.013599 Test Acc.：244383/255000(95.84%),253203/255000(99.30%) 0.013436 Save model 03:10:02 07:56:46 left. \n",
      "epoch: 58 train_loss: 0.013595 Test Acc.：244093/255000(95.72%),252493/255000(99.02%) 0.013433 Save model 03:13:21 07:53:24 left. \n",
      "epoch: 59 train_loss: 0.013584 Test Acc.：244376/255000(95.83%),253383/255000(99.37%) 0.013399 Save model 03:16:39 07:49:58 left. \n",
      "epoch: 60 train_loss: 0.013587 Test Acc.：244298/255000(95.80%),253201/255000(99.29%) 0.013411 03:19:53 07:46:25 left. \n",
      "epoch: 61 train_loss: 0.013585 Test Acc.：244484/255000(95.88%),253066/255000(99.24%) 0.013372 03:23:15 07:43:08 left. \n",
      "epoch: 62 train_loss: 0.013619 Test Acc.：244486/255000(95.88%),252859/255000(99.16%) 0.013382 03:26:39 07:39:58 left. \n",
      "epoch: 63 train_loss: 0.01359 Test Acc.：244317/255000(95.81%),252814/255000(99.14%) 0.013378 03:29:55 07:36:31 left. \n",
      "epoch: 64 train_loss: 0.013571 Test Acc.：244668/255000(95.95%),252940/255000(99.19%) 0.013378 Save model 03:33:17 07:33:14 left. \n",
      "epoch: 65 train_loss: 0.013562 Test Acc.：244045/255000(95.70%),253219/255000(99.30%) 0.013448 Save model 03:36:45 07:30:10 left. \n",
      "epoch: 66 train_loss: 0.013554 Test Acc.：243911/255000(95.65%),252679/255000(99.09%) 0.013461 Save model 03:40:03 07:26:46 left. \n",
      "epoch: 67 train_loss: 0.01355 Test Acc.：244304/255000(95.81%),253241/255000(99.31%) 0.013421 Save model 03:43:24 07:23:28 left. \n",
      "epoch: 68 train_loss: 0.01354 Test Acc.：244478/255000(95.87%),252953/255000(99.20%) 0.013475 Save model 03:46:40 07:20:00 left. \n",
      "epoch: 69 train_loss: 0.013528 Test Acc.：244473/255000(95.87%),253167/255000(99.28%) 0.013388 Save model 03:50:00 07:16:40 left. \n",
      "epoch: 70 train_loss: 0.013518 Test Acc.：244643/255000(95.94%),253122/255000(99.26%) 0.013372 Save model 03:53:18 07:13:18 left. \n",
      "epoch: 71 train_loss: 0.013521 Test Acc.：244453/255000(95.86%),251334/255000(98.56%) 0.013391 03:56:40 07:10:00 left. \n",
      "epoch: 72 train_loss: 0.013517 Test Acc.：244579/255000(95.91%),253209/255000(99.30%) 0.013406 Save model 03:59:59 07:06:39 left. \n",
      "epoch: 73 train_loss: 0.013504 Test Acc.：244155/255000(95.75%),252842/255000(99.15%) 0.013408 Save model 04:03:13 07:03:09 left. \n",
      "epoch: 74 train_loss: 0.013501 Test Acc.：244575/255000(95.91%),253078/255000(99.25%) 0.013412 Save model 04:06:37 06:59:56 left. \n",
      "epoch: 75 train_loss: 0.013492 Test Acc.：244776/255000(95.99%),253102/255000(99.26%) 0.013286 Save model 04:09:54 06:56:31 left. \n",
      "epoch: 76 train_loss: 0.01349 Test Acc.：244531/255000(95.89%),253202/255000(99.29%) 0.01337 Save model 04:13:14 06:53:10 left. \n",
      "epoch: 77 train_loss: 0.013481 Test Acc.：244615/255000(95.93%),253238/255000(99.31%) 0.01338 Save model 04:16:36 06:49:53 left. \n",
      "epoch: 78 train_loss: 0.013469 Test Acc.：244809/255000(96.00%),253298/255000(99.33%) 0.013335 Save model 04:19:54 06:46:31 left. \n",
      "epoch: 79 train_loss: 0.013464 Test Acc.：244740/255000(95.98%),253322/255000(99.34%) 0.013352 Save model 04:23:13 06:43:10 left. \n",
      "epoch: 80 train_loss: 0.013464 Test Acc.：243988/255000(95.68%),253073/255000(99.24%) 0.013458 04:26:35 06:39:52 left. \n",
      "epoch: 81 train_loss: 0.013445 Test Acc.：244752/255000(95.98%),252992/255000(99.21%) 0.013239 Save model 04:29:55 06:36:33 left. \n",
      "epoch: 82 train_loss: 0.013447 Test Acc.：244971/255000(96.07%),253212/255000(99.30%) 0.013343 04:33:14 06:33:11 left. \n",
      "epoch: 83 train_loss: 0.013437 Test Acc.：244494/255000(95.88%),253104/255000(99.26%) 0.013299 Save model 04:36:34 06:29:52 left. \n",
      "epoch: 84 train_loss: 0.013441 Test Acc.：245061/255000(96.10%),253318/255000(99.34%) 0.013275 04:39:57 06:26:36 left. \n",
      "epoch: 85 train_loss: 0.013423 Test Acc.：245147/255000(96.14%),253152/255000(99.28%) 0.013239 Save model 04:43:21 06:23:21 left. \n",
      "epoch: 86 train_loss: 0.01343 Test Acc.：245006/255000(96.08%),253286/255000(99.33%) 0.013301 04:46:47 06:20:09 left. \n",
      "epoch: 87 train_loss: 0.013405 Test Acc.：245116/255000(96.12%),253220/255000(99.30%) 0.013229 Save model 04:50:16 06:17:00 left. \n",
      "epoch: 88 train_loss: 0.013406 Test Acc.：245007/255000(96.08%),253221/255000(99.30%) 0.013256 04:53:36 06:13:41 left. \n",
      "epoch: 89 train_loss: 0.01339 Test Acc.：244615/255000(95.93%),253194/255000(99.29%) 0.013367 Save model 04:56:54 06:10:18 left. \n",
      "epoch: 90 train_loss: 0.013391 Test Acc.：244796/255000(96.00%),253081/255000(99.25%) 0.013323 05:00:12 06:06:55 left. \n",
      "epoch: 91 train_loss: 0.013382 Test Acc.：244787/255000(95.99%),253382/255000(99.37%) 0.013293 Save model 05:03:35 06:03:38 left. \n",
      "epoch: 92 train_loss: 0.013377 Test Acc.：245113/255000(96.12%),253241/255000(99.31%) 0.013279 Save model 05:06:56 06:00:19 left. \n",
      "epoch: 93 train_loss: 0.013368 Test Acc.：244767/255000(95.99%),253354/255000(99.35%) 0.013287 Save model 05:10:18 05:57:01 left. \n",
      "epoch: 94 train_loss: 0.01336 Test Acc.：245161/255000(96.14%),253362/255000(99.36%) 0.013265 Save model 05:13:28 05:53:29 left. \n",
      "epoch: 95 train_loss: 0.013355 Test Acc.：245013/255000(96.08%),253475/255000(99.40%) 0.013265 Save model 05:16:48 05:50:09 left. \n",
      "epoch: 96 train_loss: 0.013346 Test Acc.：245424/255000(96.24%),253147/255000(99.27%) 0.013202 Save model 05:20:06 05:46:47 left. \n",
      "epoch: 97 train_loss: 0.013337 Test Acc.：245268/255000(96.18%),253291/255000(99.33%) 0.013189 Save model 05:23:28 05:43:29 left. \n",
      "epoch: 98 train_loss: 0.013327 Test Acc.：244894/255000(96.04%),253231/255000(99.31%) 0.013277 Save model 05:26:47 05:40:08 left. \n",
      "epoch: 99 train_loss: 0.013317 Test Acc.：245354/255000(96.22%),253414/255000(99.38%) 0.013206 Save model 05:30:11 05:36:51 left. \n",
      "epoch: 100 train_loss: 0.013317 Test Acc.：245554/255000(96.30%),253308/255000(99.34%) 0.013183 Save model 05:33:32 05:33:32 left. \n",
      "epoch: 101 train_loss: 0.013307 Test Acc.：245425/255000(96.25%),253336/255000(99.35%) 0.013162 Save model 05:36:48 05:30:08 left. \n",
      "epoch: 102 train_loss: 0.013302 Test Acc.：245629/255000(96.33%),253326/255000(99.34%) 0.01319 Save model 05:40:11 05:26:51 left. \n",
      "epoch: 103 train_loss: 0.013285 Test Acc.：245443/255000(96.25%),252569/255000(99.05%) 0.013171 Save model 05:43:29 05:23:29 left. \n",
      "epoch: 104 train_loss: 0.013284 Test Acc.：245517/255000(96.28%),252974/255000(99.21%) 0.013166 Save model 05:46:54 05:20:13 left. \n",
      "epoch: 105 train_loss: 0.013274 Test Acc.：245338/255000(96.21%),253319/255000(99.34%) 0.013208 Save model 05:50:19 05:16:57 left. \n",
      "epoch: 106 train_loss: 0.013267 Test Acc.：245744/255000(96.37%),253491/255000(99.41%) 0.01315 Save model 05:53:31 05:13:29 left. \n",
      "epoch: 107 train_loss: 0.01326 Test Acc.：245670/255000(96.34%),253445/255000(99.39%) 0.013143 Save model 05:56:51 05:10:09 left. \n",
      "epoch: 108 train_loss: 0.013251 Test Acc.：245315/255000(96.20%),253348/255000(99.35%) 0.013176 Save model 06:00:19 05:06:56 left. \n",
      "epoch: 109 train_loss: 0.013242 Test Acc.：245624/255000(96.32%),253323/255000(99.34%) 0.013166 Save model 06:03:43 05:03:39 left. \n",
      "epoch: 110 train_loss: 0.013228 Test Acc.：245731/255000(96.37%),253243/255000(99.31%) 0.0131 Save model 06:07:06 05:00:21 left. \n",
      "epoch: 111 train_loss: 0.013227 Test Acc.：245684/255000(96.35%),253303/255000(99.33%) 0.013111 Save model 06:10:31 04:57:05 left. \n",
      "epoch: 112 train_loss: 0.013218 Test Acc.：245695/255000(96.35%),253381/255000(99.37%) 0.013153 Save model 06:13:51 04:53:44 left. \n",
      "epoch: 113 train_loss: 0.013214 Test Acc.：245718/255000(96.36%),253377/255000(99.36%) 0.013109 Save model 06:17:09 04:50:22 left. \n",
      "epoch: 114 train_loss: 0.013201 Test Acc.：245611/255000(96.32%),252829/255000(99.15%) 0.013118 Save model 06:20:31 04:47:03 left. \n",
      "epoch: 115 train_loss: 0.013191 Test Acc.：245934/255000(96.44%),253364/255000(99.36%) 0.01304 Save model 06:23:54 04:43:45 left. \n",
      "epoch: 116 train_loss: 0.013183 Test Acc.：245979/255000(96.46%),253487/255000(99.41%) 0.013084 Save model 06:27:19 04:40:28 left. \n",
      "epoch: 117 train_loss: 0.013177 Test Acc.：245633/255000(96.33%),253355/255000(99.35%) 0.013122 Save model 06:30:33 04:37:03 left. \n",
      "epoch: 118 train_loss: 0.013169 Test Acc.：245646/255000(96.33%),253328/255000(99.34%) 0.013098 Save model 06:33:55 04:33:44 left. \n",
      "epoch: 119 train_loss: 0.013158 Test Acc.：245885/255000(96.43%),253437/255000(99.39%) 0.01309 Save model 06:37:15 04:30:24 left. \n",
      "epoch: 120 train_loss: 0.013149 Test Acc.：246123/255000(96.52%),253413/255000(99.38%) 0.013044 Save model 06:40:31 04:27:00 left. \n",
      "epoch: 121 train_loss: 0.013148 Test Acc.：246094/255000(96.51%),252200/255000(98.90%) 0.01303 Save model 06:43:49 04:23:39 left. \n",
      "epoch: 122 train_loss: 0.013156 Test Acc.：246103/255000(96.51%),252935/255000(99.19%) 0.013062 06:47:10 04:20:19 left. \n",
      "epoch: 123 train_loss: 0.013132 Test Acc.：246019/255000(96.48%),253311/255000(99.34%) 0.013072 Save model 06:50:30 04:16:58 left. \n",
      "epoch: 124 train_loss: 0.013117 Test Acc.：245984/255000(96.46%),253295/255000(99.33%) 0.013055 Save model 06:53:46 04:13:36 left. \n",
      "epoch: 125 train_loss: 0.013109 Test Acc.：246042/255000(96.49%),253319/255000(99.34%) 0.013041 Save model 06:57:08 04:10:16 left. \n",
      "epoch: 126 train_loss: 0.013103 Test Acc.：246259/255000(96.57%),253291/255000(99.33%) 0.013042 Save model 07:00:33 04:06:59 left. \n",
      "epoch: 127 train_loss: 0.013088 Test Acc.：245995/255000(96.47%),252658/255000(99.08%) 0.013029 Save model 07:03:57 04:03:41 left. \n",
      "epoch: 128 train_loss: 0.013078 Test Acc.：246125/255000(96.52%),252862/255000(99.16%) 0.013009 Save model 07:07:17 04:00:21 left. \n",
      "epoch: 129 train_loss: 0.013069 Test Acc.：246179/255000(96.54%),252532/255000(99.03%) 0.013046 Save model 07:10:40 03:57:02 left. \n",
      "epoch: 130 train_loss: 0.013063 Test Acc.：246301/255000(96.59%),252917/255000(99.18%) 0.013005 Save model 07:14:01 03:53:42 left. \n",
      "epoch: 131 train_loss: 0.01305 Test Acc.：246110/255000(96.51%),253199/255000(99.29%) 0.012997 Save model 07:17:18 03:50:20 left. \n",
      "epoch: 132 train_loss: 0.01304 Test Acc.：246437/255000(96.64%),253168/255000(99.28%) 0.012965 Save model 07:20:30 03:46:55 left. \n",
      "epoch: 133 train_loss: 0.013031 Test Acc.：246312/255000(96.59%),252970/255000(99.20%) 0.012967 Save model 07:23:47 03:43:33 left. \n",
      "epoch: 134 train_loss: 0.01302 Test Acc.：246201/255000(96.55%),252849/255000(99.16%) 0.012982 Save model 07:27:10 03:40:15 left. \n",
      "epoch: 135 train_loss: 0.013013 Test Acc.：246383/255000(96.62%),252685/255000(99.09%) 0.012941 Save model 07:30:30 03:36:54 left. \n",
      "epoch: 136 train_loss: 0.013 Test Acc.：246572/255000(96.69%),253049/255000(99.23%) 0.012969 Save model 07:33:51 03:33:34 left. \n",
      "epoch: 137 train_loss: 0.012986 Test Acc.：246365/255000(96.61%),253042/255000(99.23%) 0.012936 Save model 07:37:12 03:30:14 left. \n",
      "epoch: 138 train_loss: 0.012983 Test Acc.：246506/255000(96.67%),253155/255000(99.28%) 0.012949 Save model 07:40:27 03:26:52 left. \n",
      "epoch: 139 train_loss: 0.012972 Test Acc.：246468/255000(96.65%),252925/255000(99.19%) 0.012939 Save model 07:43:44 03:23:30 left. \n",
      "epoch: 140 train_loss: 0.012965 Test Acc.：246534/255000(96.68%),252928/255000(99.19%) 0.01293 Save model 07:47:03 03:20:09 left. \n",
      "epoch: 141 train_loss: 0.012948 Test Acc.：246674/255000(96.73%),252896/255000(99.17%) 0.012919 Save model 07:50:22 03:16:49 left. \n",
      "epoch: 142 train_loss: 0.012948 Test Acc.：246650/255000(96.73%),252958/255000(99.20%) 0.012898 07:53:40 03:13:28 left. \n",
      "epoch: 143 train_loss: 0.012936 Test Acc.：246808/255000(96.79%),252971/255000(99.20%) 0.012857 Save model 07:57:04 03:10:09 left. \n",
      "epoch: 144 train_loss: 0.012921 Test Acc.：246674/255000(96.73%),253167/255000(99.28%) 0.012911 Save model 08:00:24 03:06:49 left. \n",
      "epoch: 145 train_loss: 0.01292 Test Acc.：246584/255000(96.70%),252317/255000(98.95%) 0.012931 Save model 08:03:23 03:03:21 left. \n",
      "epoch: 146 train_loss: 0.01291 Test Acc.：246952/255000(96.84%),252983/255000(99.21%) 0.012879 Save model 08:06:49 03:00:03 left. \n",
      "epoch: 147 train_loss: 0.012893 Test Acc.：246781/255000(96.78%),252602/255000(99.06%) 0.012865 Save model 08:10:09 02:56:43 left. \n",
      "epoch: 148 train_loss: 0.012885 Test Acc.：246932/255000(96.84%),252522/255000(99.03%) 0.01285 Save model 08:13:28 02:53:22 left. \n",
      "epoch: 149 train_loss: 0.012877 Test Acc.：246997/255000(96.86%),252796/255000(99.14%) 0.012847 Save model 08:16:46 02:50:02 left. \n",
      "epoch: 150 train_loss: 0.012859 Test Acc.：246956/255000(96.85%),252973/255000(99.21%) 0.012852 Save model 08:20:07 02:46:42 left. \n",
      "epoch: 151 train_loss: 0.012851 Test Acc.：247009/255000(96.87%),253062/255000(99.24%) 0.012843 Save model 08:23:32 02:43:24 left. \n",
      "epoch: 152 train_loss: 0.012846 Test Acc.：247018/255000(96.87%),252995/255000(99.21%) 0.01282 Save model 08:26:54 02:40:04 left. \n",
      "epoch: 153 train_loss: 0.012832 Test Acc.：247136/255000(96.92%),252851/255000(99.16%) 0.012793 Save model 08:30:16 02:36:44 left. \n",
      "epoch: 154 train_loss: 0.012826 Test Acc.：246999/255000(96.86%),252691/255000(99.09%) 0.012819 Save model 08:33:34 02:33:24 left. \n",
      "epoch: 155 train_loss: 0.012818 Test Acc.：247113/255000(96.91%),252461/255000(99.00%) 0.012809 Save model 08:36:52 02:30:03 left. \n",
      "epoch: 156 train_loss: 0.012807 Test Acc.：247079/255000(96.89%),252674/255000(99.09%) 0.012823 Save model 08:40:10 02:26:42 left. \n",
      "epoch: 157 train_loss: 0.012796 Test Acc.：247172/255000(96.93%),252768/255000(99.12%) 0.01281 Save model 08:43:32 02:23:23 left. \n",
      "epoch: 158 train_loss: 0.012786 Test Acc.：247232/255000(96.95%),252481/255000(99.01%) 0.012794 Save model 08:46:58 02:20:04 left. \n",
      "epoch: 159 train_loss: 0.012778 Test Acc.：247302/255000(96.98%),252696/255000(99.10%) 0.012778 Save model 08:50:23 02:16:45 left. \n",
      "epoch: 160 train_loss: 0.012769 Test Acc.：247404/255000(97.02%),252484/255000(99.01%) 0.012753 Save model 08:53:45 02:13:26 left. \n",
      "epoch: 161 train_loss: 0.012759 Test Acc.：247234/255000(96.95%),251700/255000(98.71%) 0.012773 Save model 08:56:59 02:10:04 left. \n",
      "epoch: 162 train_loss: 0.012753 Test Acc.：247413/255000(97.02%),252538/255000(99.03%) 0.012748 Save model 09:00:18 02:06:44 left. \n",
      "epoch: 163 train_loss: 0.012741 Test Acc.：247291/255000(96.98%),252505/255000(99.02%) 0.012777 Save model 09:03:37 02:03:23 left. \n",
      "epoch: 164 train_loss: 0.012732 Test Acc.：247502/255000(97.06%),252500/255000(99.02%) 0.012739 Save model 09:06:55 02:00:03 left. \n",
      "epoch: 165 train_loss: 0.012723 Test Acc.：247443/255000(97.04%),252385/255000(98.97%) 0.012742 Save model 09:10:16 01:56:43 left. \n",
      "epoch: 166 train_loss: 0.01271 Test Acc.：247427/255000(97.03%),252421/255000(98.99%) 0.012727 Save model 09:13:34 01:53:22 left. \n",
      "epoch: 167 train_loss: 0.012704 Test Acc.：247436/255000(97.03%),251155/255000(98.49%) 0.012734 Save model 09:16:54 01:50:02 left. \n",
      "epoch: 168 train_loss: 0.012696 Test Acc.：247572/255000(97.09%),252552/255000(99.04%) 0.012706 Save model 09:20:13 01:46:42 left. \n",
      "epoch: 169 train_loss: 0.012686 Test Acc.：247578/255000(97.09%),251972/255000(98.81%) 0.01271 Save model 09:23:33 01:43:22 left. \n",
      "epoch: 170 train_loss: 0.012673 Test Acc.：247659/255000(97.12%),252240/255000(98.92%) 0.012702 Save model 09:26:50 01:40:01 left. \n",
      "epoch: 171 train_loss: 0.01267 Test Acc.：247629/255000(97.11%),252284/255000(98.93%) 0.012702 Save model 09:30:09 01:36:41 left. \n",
      "epoch: 172 train_loss: 0.012656 Test Acc.：247536/255000(97.07%),252158/255000(98.89%) 0.012695 Save model 09:33:30 01:33:21 left. \n",
      "epoch: 173 train_loss: 0.012649 Test Acc.：247586/255000(97.09%),252601/255000(99.06%) 0.012697 Save model 09:36:51 01:30:01 left. \n",
      "epoch: 174 train_loss: 0.01264 Test Acc.：247617/255000(97.10%),252549/255000(99.04%) 0.012687 Save model 09:40:12 01:26:41 left. \n",
      "epoch: 175 train_loss: 0.012636 Test Acc.：247732/255000(97.15%),251906/255000(98.79%) 0.012688 Save model 09:43:33 01:23:21 left. \n",
      "epoch: 176 train_loss: 0.012629 Test Acc.：247679/255000(97.13%),252009/255000(98.83%) 0.012687 Save model 09:46:51 01:20:01 left. \n",
      "epoch: 177 train_loss: 0.012618 Test Acc.：247735/255000(97.15%),251978/255000(98.81%) 0.012677 Save model 09:50:11 01:16:41 left. \n",
      "epoch: 178 train_loss: 0.012609 Test Acc.：247787/255000(97.17%),251606/255000(98.67%) 0.012674 Save model 09:53:29 01:13:21 left. \n",
      "epoch: 179 train_loss: 0.012604 Test Acc.：247774/255000(97.17%),252264/255000(98.93%) 0.012662 Save model 09:56:46 01:10:00 left. \n",
      "epoch: 180 train_loss: 0.012594 Test Acc.：247844/255000(97.19%),251558/255000(98.65%) 0.012663 Save model 10:00:05 01:06:40 left. \n",
      "epoch: 181 train_loss: 0.012589 Test Acc.：247886/255000(97.21%),251929/255000(98.80%) 0.012652 Save model 10:03:20 01:03:20 left. \n",
      "epoch: 182 train_loss: 0.012586 Test Acc.：247905/255000(97.22%),252070/255000(98.85%) 0.012636 Save model 10:06:40 01:00:00 left. \n",
      "epoch: 183 train_loss: 0.012576 Test Acc.：247895/255000(97.21%),252347/255000(98.96%) 0.012639 Save model 10:10:01 00:56:40 left. \n",
      "epoch: 184 train_loss: 0.01257 Test Acc.：247906/255000(97.22%),251480/255000(98.62%) 0.012642 Save model 10:13:21 00:53:20 left. \n",
      "epoch: 185 train_loss: 0.01256 Test Acc.：247988/255000(97.25%),251941/255000(98.80%) 0.012633 Save model 10:16:43 00:50:00 left. \n",
      "epoch: 186 train_loss: 0.012556 Test Acc.：247984/255000(97.25%),251939/255000(98.80%) 0.012632 Save model 10:20:05 00:46:40 left. \n",
      "epoch: 187 train_loss: 0.012551 Test Acc.：248025/255000(97.26%),251967/255000(98.81%) 0.012627 Save model 10:23:23 00:43:20 left. \n",
      "epoch: 188 train_loss: 0.012547 Test Acc.：247991/255000(97.25%),251817/255000(98.75%) 0.012624 Save model 10:26:39 00:39:59 left. \n",
      "epoch: 189 train_loss: 0.012543 Test Acc.：247961/255000(97.24%),251744/255000(98.72%) 0.012619 Save model 10:29:57 00:36:39 left. \n",
      "epoch: 190 train_loss: 0.012533 Test Acc.：248027/255000(97.27%),251791/255000(98.74%) 0.012614 Save model 10:33:13 00:33:19 left. \n",
      "epoch: 191 train_loss: 0.01253 Test Acc.：247999/255000(97.25%),252063/255000(98.85%) 0.012613 Save model 10:36:29 00:29:59 left. \n",
      "epoch: 192 train_loss: 0.012526 Test Acc.：248063/255000(97.28%),251676/255000(98.70%) 0.012622 Save model 10:39:45 00:26:39 left. \n",
      "epoch: 193 train_loss: 0.012524 Test Acc.：248047/255000(97.27%),252017/255000(98.83%) 0.012611 Save model 10:43:13 00:23:19 left. \n",
      "epoch: 194 train_loss: 0.012523 Test Acc.：248052/255000(97.28%),251762/255000(98.73%) 0.01261 Save model 10:46:33 00:19:59 left. \n",
      "epoch: 195 train_loss: 0.012523 Test Acc.：248057/255000(97.28%),251928/255000(98.80%) 0.012613 10:49:54 00:16:39 left. \n",
      "epoch: 196 train_loss: 0.012517 Test Acc.：248076/255000(97.28%),251770/255000(98.73%) 0.012616 Save model 10:53:13 00:13:19 left. \n",
      "epoch: 197 train_loss: 0.012514 Test Acc.：248005/255000(97.26%),251831/255000(98.76%) 0.012622 Save model 10:56:35 00:09:59 left. \n",
      "epoch: 198 train_loss: 0.012513 Test Acc.：248023/255000(97.26%),251824/255000(98.75%) 0.012604 Save model 10:59:46 00:06:39 left. \n",
      "epoch: 199 train_loss: 0.012512 Test Acc.：248033/255000(97.27%),251821/255000(98.75%) 0.012611 Save model 11:03:05 00:03:19 left. \n",
      "epoch: 200 train_loss: 0.01251 Test Acc.：248053/255000(97.28%),251863/255000(98.77%) 0.012606 Save model 11:06:27 00:00:00 left. \n",
      "Test Acc.：196512/255000(77.06%),20549/255000(8.06%) 0.02467 \n",
      "01_1_VC2003_32bit_none 01_2_VC2017_32bit_none 02_1_VC2003_32bit_max 02_2_VC2017_32bit_max 03_1_gcc6.3.0_x86_none 03_2_x86-O0-gcc7.5.0 04_1_gcc6.3.0_x86_O3 04_2_x86-O3-gcc7.5.0 05_1_clang5.0.2_32_none 05_2_x86-O0-Clang10.0.0 06_1_clang5.0.2_32_O3 06_2_x86-O3-Clang10.0.0 07_intel_32_none 08_intel_32bit_max 11_VC2017_64bit_none 12_VC2017_64bit_max 13_1_gcc6.3.0_64bit_none 13_2_x86_64-O0-gcc7.5.0 14_1_gcc6.3.0_64bit_max 14_2_x86_64-O3-gcc7.5.0 15_1_clang5.0.2_64bit_none 15_2_x86_64-O0-Clang10.0.0 16_1_clang5.0.2_64bit_max 16_2_x86_64-O3-Clang10.0.0 17_intel_64_none 18_intel_64bit_max 21_arm-O0-gcc 22_arm-O3-gcc 23_arm-O0-Clang 24_arm-O3-Clang 31_arm64-O0-gcc 32_arm64-O3-gcc 33_arm64-O0-Clang 34_arm64-O3-Clang 41_mips-O0-gcc 42_mips-O3-gcc 43_mips-O0-Clang 44_mips-O3-Clang 51_mips64-O0-gcc 52_mips64-O3-gcc 53_mips64-O0-Clang 54_mips64-O3-Clang 61_powerpc-O0-gcc 62_powerpc-O3-gcc 63_powerpc-O0-Clang 64_powerpc-O3-Clang 71_powerpc64-O0-gcc 72_powerpc64-O3-gcc 73_powerpc64-O0-Clang 74_powerpc64-O3-Clang 80_document \n",
      "01_1_VC2003_32bit_none 4789 192 5 2 0 0 0 0 4 0 4 0 0 1 0 0 0 0 0 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "01_2_VC2017_32bit_none 242 4747 1 2 0 2 0 1 2 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "02_1_VC2003_32bit_max 15 0 4944 20 0 1 3 1 0 0 4 0 0 7 0 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 \n",
      "02_2_VC2017_32bit_max 4 4 41 4921 0 1 3 0 0 0 6 0 0 11 0 0 0 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 \n",
      "03_1_gcc6.3.0_x86_none 0 0 0 1 4874 111 4 2 1 1 3 1 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "03_2_x86-O0-gcc7.5.0 0 0 0 0 152 4837 1 5 1 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "04_1_gcc6.3.0_x86_O3 0 0 3 2 4 0 4657 262 1 2 26 34 0 3 0 0 0 0 4 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "04_2_x86-O3-gcc7.5.0 0 0 1 3 2 6 351 4587 1 2 21 21 0 1 0 0 0 0 2 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "05_1_clang5.0.2_32_none 1 2 0 0 1 0 1 1 4735 255 0 0 0 1 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "05_2_x86-O0-Clang10.0.0 1 0 0 1 1 0 0 2 304 4683 1 2 0 0 0 0 0 0 0 0 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "06_1_clang5.0.2_32_O3 0 0 0 11 0 1 24 22 2 0 4727 188 0 4 1 0 0 0 4 0 1 1 11 2 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "06_2_x86-O3-Clang10.0.0 0 0 5 0 0 0 20 23 0 1 190 4752 0 2 0 0 0 0 1 0 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "07_intel_32_none 0 0 0 0 0 1 0 0 0 0 0 0 4999 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "08_intel_32bit_max 0 2 10 16 0 0 3 0 1 3 11 0 1 4949 0 0 0 0 0 2 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "11_VC2017_64bit_none 1 0 0 1 0 0 0 0 0 0 0 0 0 1 4989 2 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 \n",
      "12_VC2017_64bit_max 0 1 5 5 1 0 0 0 0 0 0 0 0 0 2 4966 0 1 1 4 1 0 4 0 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 \n",
      "13_1_gcc6.3.0_64bit_none 1 0 0 1 2 1 0 0 0 0 0 0 0 0 0 0 4454 533 2 2 1 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "13_2_x86_64-O0-gcc7.5.0 0 0 0 0 3 4 0 0 1 0 0 0 0 0 0 0 689 4296 1 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "14_1_gcc6.3.0_64bit_max 0 0 0 0 0 0 11 3 0 0 6 0 0 0 0 1 6 0 4617 267 0 4 37 44 0 3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "14_2_x86_64-O3-gcc7.5.0 0 1 0 0 0 0 2 4 1 1 3 0 0 1 0 2 2 1 486 4422 1 1 33 35 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "15_1_clang5.0.2_64bit_none 0 0 0 0 0 0 0 0 9 0 0 0 0 0 0 1 2 0 1 0 4703 284 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "15_2_x86_64-O0-Clang10.0.0 0 0 0 0 0 0 0 0 1 5 0 0 0 0 0 0 4 5 1 1 241 4742 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "16_1_clang5.0.2_64bit_max 1 0 0 3 0 0 2 1 0 0 16 1 0 0 1 3 0 1 66 28 2 0 4624 245 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "16_2_x86_64-O3-Clang10.0.0 0 0 0 0 0 1 0 0 0 0 4 4 0 1 1 1 2 0 50 39 0 0 287 4607 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "17_intel_64_none 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 4998 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "18_intel_64bit_max 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 1 3 0 0 3 0 0 4990 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "21_arm-O0-gcc 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4978 6 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 13 \n",
      "22_arm-O3-gcc 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 4 4983 3 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 5 \n",
      "23_arm-O0-Clang 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 4985 7 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 \n",
      "24_arm-O3-Clang 0 0 0 0 0 0 1 2 0 0 0 0 0 2 0 1 0 0 1 0 0 0 0 0 0 0 1 2 18 4966 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 \n",
      "31_arm64-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4997 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "32_arm64-O3-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 4986 0 13 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "33_arm64-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4993 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 \n",
      "34_arm64-O3-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 39 14 4945 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 \n",
      "41_mips-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 4991 2 0 0 1 3 0 1 0 0 0 0 0 0 0 0 1 \n",
      "42_mips-O3-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 4980 0 13 0 6 0 0 0 0 0 0 0 0 0 0 0 \n",
      "43_mips-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 4996 1 0 0 1 0 0 0 0 0 0 0 0 0 0 \n",
      "44_mips-O3-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 22 1 4958 0 1 0 18 0 0 0 0 0 0 0 0 0 \n",
      "51_mips64-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 1 0 4992 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "52_mips64-O3-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 32 0 4 2 4952 0 10 0 0 0 0 0 0 0 0 0 \n",
      "53_mips64-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 4 3 4992 0 0 0 0 0 0 0 0 0 0 \n",
      "54_mips64-O3-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 0 39 0 24 0 4929 0 0 0 0 0 0 0 0 0 \n",
      "61_powerpc-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4997 2 1 0 0 0 0 0 0 \n",
      "62_powerpc-O3-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 4977 0 5 0 11 0 4 0 \n",
      "63_powerpc-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 4996 1 0 0 2 0 0 \n",
      "64_powerpc-O3-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 16 4 4965 0 0 0 15 0 \n",
      "71_powerpc64-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 1 0 0 4991 1 0 1 1 \n",
      "72_powerpc64-O3-gcc 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 24 0 4 3 4953 2 6 3 \n",
      "73_powerpc64-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 4990 2 0 \n",
      "74_powerpc64-O3-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 30 0 4 1 4964 0 \n",
      "80_document 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 4 2 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 4988 \n",
      "no label Num TP FP FN TN R P F1 Acc.\n",
      "0 01_1_VC2003_32bit_none 5000 4789 211 267 4789 0.9471914556962026 0.9578 0.9524661893396978 0.9524661893396977\n",
      "1 01_2_VC2017_32bit_none 5000 4747 253 202 4747 0.9591836734693877 0.9494 0.95426676047844 0.9542667604784401\n",
      "2 02_1_VC2003_32bit_max 5000 4944 56 71 4944 0.9858424725822532 0.9888 0.9873190214677983 0.9873190214677983\n",
      "3 02_2_VC2017_32bit_max 5000 4921 79 69 4921 0.9861723446893788 0.9842 0.9851851851851853 0.9851851851851852\n",
      "4 03_1_gcc6.3.0_x86_none 5000 4874 126 166 4874 0.9670634920634921 0.9748 0.9709163346613546 0.9709163346613546\n",
      "5 03_2_x86-O0-gcc7.5.0 5000 4837 163 129 4837 0.9740233588401128 0.9674 0.9707003812964078 0.9707003812964078\n",
      "6 04_1_gcc6.3.0_x86_O3 5000 4657 343 427 4657 0.9160110149488592 0.9314 0.9236414121380404 0.9236414121380404\n",
      "7 04_2_x86-O3-gcc7.5.0 5000 4587 413 332 4587 0.9325066070339499 0.9174 0.9248916221393285 0.9248916221393285\n",
      "8 05_1_clang5.0.2_32_none 5000 4735 265 330 4735 0.9348469891411648 0.947 0.9408842523596621 0.9408842523596622\n",
      "9 05_2_x86-O0-Clang10.0.0 5000 4683 317 271 4683 0.9452967299152201 0.9366 0.9409282700421941 0.9409282700421941\n",
      "10 06_1_clang5.0.2_32_O3 5000 4727 273 299 4727 0.9405093513728611 0.9454 0.9429483343307401 0.9429483343307401\n",
      "11 06_2_x86-O3-Clang10.0.0 5000 4752 248 251 4752 0.9498301019388367 0.9504 0.9501149655103469 0.9501149655103469\n",
      "12 07_intel_32_none 5000 4999 1 1 4999 0.9998 0.9998 0.9998 0.9998\n",
      "13 08_intel_32bit_max 5000 4949 51 37 4949 0.992579221821099 0.9898 0.9911876627278188 0.991187662727819\n",
      "14 11_VC2017_64bit_none 5000 4989 11 6 4989 0.9987987987987988 0.9978 0.9982991495747875 0.9982991495747874\n",
      "15 12_VC2017_64bit_max 5000 4966 34 15 4966 0.996988556514756 0.9932 0.995090672277327 0.995090672277327\n",
      "16 13_1_gcc6.3.0_64bit_none 5000 4454 546 706 4454 0.8631782945736434 0.8908 0.8767716535433071 0.8767716535433071\n",
      "17 13_2_x86_64-O0-gcc7.5.0 5000 4296 704 547 4296 0.8870534792483997 0.8592 0.8729046022554099 0.8729046022554099\n",
      "18 14_1_gcc6.3.0_64bit_max 5000 4617 383 624 4617 0.8809387521465369 0.9234 0.9016697588126159 0.9016697588126159\n",
      "19 14_2_x86_64-O3-gcc7.5.0 5000 4422 578 354 4422 0.9258793969849246 0.8844 0.9046644844517185 0.9046644844517185\n",
      "20 15_1_clang5.0.2_64bit_none 5000 4703 297 253 4703 0.9489507667473769 0.9406 0.9447569304941743 0.9447569304941744\n",
      "21 15_2_x86_64-O0-Clang10.0.0 5000 4742 258 301 4742 0.9403133055720801 0.9484 0.9443393408344121 0.9443393408344121\n",
      "22 16_1_clang5.0.2_64bit_max 5000 4624 376 382 4624 0.923691570115861 0.9248 0.924245452728363 0.924245452728363\n",
      "23 16_2_x86_64-O3-Clang10.0.0 5000 4607 393 335 4607 0.9322136786726022 0.9214 0.9267752967209816 0.9267752967209817\n",
      "24 17_intel_64_none 5000 4998 2 1 4998 0.9997999599919984 0.9996 0.9996999699969997 0.9996999699969997\n",
      "25 18_intel_64bit_max 5000 4990 10 22 4990 0.9956105347166799 0.998 0.9968038353975229 0.9968038353975229\n",
      "26 21_arm-O0-gcc 5000 4978 22 10 4978 0.9979951884522855 0.9956 0.9967961553864637 0.9967961553864637\n",
      "27 22_arm-O3-gcc 5000 4983 17 11 4983 0.9977973568281938 0.9966 0.9971983189913949 0.9971983189913949\n",
      "28 23_arm-O0-Clang 5000 4985 15 21 4985 0.9958050339592489 0.997 0.9964021587047772 0.9964021587047771\n",
      "29 24_arm-O3-Clang 5000 4966 34 7 4966 0.9985923989543535 0.9932 0.9958889000300812 0.9958889000300812\n",
      "30 31_arm64-O0-gcc 5000 4997 3 1 4997 0.9997999199679872 0.9994 0.9995999199839969 0.9995999199839968\n",
      "31 32_arm64-O3-gcc 5000 4986 14 44 4986 0.9912524850894632 0.9972 0.9942173479561315 0.9942173479561316\n",
      "32 33_arm64-O0-Clang 5000 4993 7 14 4993 0.9972039145196725 0.9986 0.9979014689717198 0.9979014689717198\n",
      "33 34_arm64-O3-Clang 5000 4945 55 23 4945 0.9953703703703703 0.989 0.9921749598715891 0.9921749598715891\n",
      "34 41_mips-O0-gcc 5000 4991 9 9 4991 0.9982 0.9982 0.9982 0.9982\n",
      "35 42_mips-O3-gcc 5000 4980 20 65 4980 0.9871159563924677 0.996 0.9915380786460926 0.9915380786460926\n",
      "36 43_mips-O0-Clang 5000 4996 4 3 4996 0.9993998799759952 0.9992 0.9992999299929992 0.9992999299929993\n",
      "37 44_mips-O3-Clang 5000 4958 42 62 4958 0.9876494023904383 0.9916 0.989620758483034 0.9896207584830339\n",
      "38 51_mips64-O0-gcc 5000 4992 8 7 4992 0.9985997199439888 0.9984 0.9984998499849985 0.9984998499849985\n",
      "39 52_mips64-O3-gcc 5000 4952 48 37 4952 0.9925836841050311 0.9904 0.991490639703674 0.991490639703674\n",
      "40 53_mips64-O0-Clang 5000 4992 8 2 4992 0.9995995194233079 0.9984 0.9989993996397838 0.9989993996397839\n",
      "41 54_mips64-O3-Clang 5000 4929 71 33 4929 0.9933494558645707 0.9858 0.9895603292511544 0.9895603292511543\n",
      "42 61_powerpc-O0-gcc 5000 4997 3 6 4997 0.998800719568259 0.9994 0.9991002699190243 0.9991002699190243\n",
      "43 62_powerpc-O3-gcc 5000 4977 23 43 4977 0.9914342629482071 0.9954 0.9934131736526944 0.9934131736526947\n",
      "44 63_powerpc-O0-Clang 5000 4996 4 13 4996 0.997404671591136 0.9992 0.9983015286242382 0.9983015286242382\n",
      "45 64_powerpc-O3-Clang 5000 4965 35 43 4965 0.9914137380191693 0.993 0.9922062350119903 0.9922062350119905\n",
      "46 71_powerpc64-O0-gcc 5000 4991 9 3 4991 0.999399279134962 0.9982 0.9987992795677406 0.9987992795677406\n",
      "47 72_powerpc64-O3-gcc 5000 4953 47 19 4953 0.9961786001609011 0.9906 0.99338146811071 0.99338146811071\n",
      "48 73_powerpc64-O0-Clang 5000 4990 10 5 4990 0.998998998998999 0.998 0.9984992496248123 0.9984992496248124\n",
      "49 74_powerpc64-O3-Clang 5000 4964 36 29 4964 0.9941918686160625 0.9928 0.9934954468127689 0.9934954468127689\n",
      "50 80_document 5000 4988 12 39 4988 0.9922418937736225 0.9976 0.9949137329211131 0.994913732921113\n",
      "  255000 248053 6947 6947 248053 0.9727568627450981 0.9727568627450981 0.9727568627450981 0.9727568627450981\n",
      "4\n",
      "Start MainNet PreTrain\n",
      "epoch: 1 train_loss: 0.0178 00:01:59 06:37:44 left. \n",
      "epoch: 2 train_loss: 0.015112 00:03:58 06:33:59 left. \n",
      "epoch: 3 train_loss: 0.014614 00:05:57 06:31:37 left. \n",
      "epoch: 4 train_loss: 0.014403 00:07:57 06:29:38 left. \n",
      "epoch: 5 train_loss: 0.014248 00:09:58 06:29:14 left. \n",
      "epoch: 6 train_loss: 0.014146 00:11:59 06:27:35 left. \n",
      "epoch: 7 train_loss: 0.014067 00:13:58 06:25:27 left. \n",
      "epoch: 8 train_loss: 0.014011 00:15:58 06:23:33 left. \n",
      "epoch: 9 train_loss: 0.013968 00:17:58 06:21:28 left. \n",
      "epoch: 10 train_loss: 0.013916 00:19:59 06:19:59 left. \n",
      "epoch: 11 train_loss: 0.013895 00:21:58 06:17:38 left. \n",
      "epoch: 12 train_loss: 0.013863 00:23:59 06:15:45 left. \n",
      "epoch: 13 train_loss: 0.013842 00:25:58 06:13:39 left. \n",
      "epoch: 14 train_loss: 0.013833 00:27:58 06:11:37 left. \n",
      "epoch: 15 train_loss: 0.013819 00:29:58 06:09:47 left. \n",
      "epoch: 16 train_loss: 0.013809 00:31:58 06:07:41 left. \n",
      "epoch: 17 train_loss: 0.013797 00:33:58 06:05:46 left. \n",
      "epoch: 18 train_loss: 0.013778 00:35:55 06:03:18 left. \n",
      "epoch: 19 train_loss: 0.013765 00:37:54 06:01:10 left. \n",
      "epoch: 20 train_loss: 0.013753 00:39:54 05:59:09 left. \n",
      "epoch: 21 train_loss: 0.013741 00:41:53 05:57:05 left. \n",
      "epoch: 22 train_loss: 0.013732 00:43:54 05:55:17 left. \n",
      "epoch: 23 train_loss: 0.01372 00:45:54 05:53:16 left. \n",
      "epoch: 24 train_loss: 0.013714 00:47:53 05:51:15 left. \n",
      "epoch: 25 train_loss: 0.013713 00:49:51 05:49:03 left. \n",
      "epoch: 26 train_loss: 0.013688 00:51:50 05:46:54 left. \n",
      "epoch: 27 train_loss: 0.013685 00:53:48 05:44:45 left. \n",
      "epoch: 28 train_loss: 0.013675 00:55:46 05:42:37 left. \n",
      "epoch: 29 train_loss: 0.013665 00:57:46 05:40:41 left. \n",
      "epoch: 30 train_loss: 0.013659 00:59:44 05:38:32 left. \n",
      "epoch: 31 train_loss: 0.013657 01:01:41 05:36:18 left. \n",
      "epoch: 32 train_loss: 0.013649 01:03:40 05:34:16 left. \n",
      "epoch: 33 train_loss: 0.013639 01:05:40 05:32:18 left. \n",
      "epoch: 34 train_loss: 0.01363 01:07:39 05:30:19 left. \n",
      "epoch: 35 train_loss: 0.013622 01:09:37 05:28:12 left. \n",
      "epoch: 36 train_loss: 0.013616 01:11:30 05:25:47 left. \n",
      "epoch: 37 train_loss: 0.013606 01:13:30 05:23:48 left. \n",
      "epoch: 38 train_loss: 0.013595 01:15:30 05:21:55 left. \n",
      "epoch: 39 train_loss: 0.013598 01:17:29 05:19:52 left. \n",
      "epoch: 40 train_loss: 0.013584 01:19:28 05:17:54 left. \n",
      "epoch: 41 train_loss: 0.01358 01:21:26 05:15:51 left. \n",
      "epoch: 42 train_loss: 0.013575 01:23:27 05:13:56 left. \n",
      "epoch: 43 train_loss: 0.01358 01:25:26 05:11:57 left. \n",
      "epoch: 44 train_loss: 0.013564 01:27:25 05:09:57 left. \n",
      "epoch: 45 train_loss: 0.01356 01:29:26 05:08:03 left. \n",
      "epoch: 46 train_loss: 0.013546 01:31:25 05:06:03 left. \n",
      "epoch: 47 train_loss: 0.013534 01:33:24 05:04:03 left. \n",
      "epoch: 48 train_loss: 0.013533 01:35:23 05:02:05 left. \n",
      "epoch: 49 train_loss: 0.013524 01:37:23 05:00:08 left. \n",
      "epoch: 50 train_loss: 0.013521 01:39:22 04:58:08 left. \n",
      "epoch: 51 train_loss: 0.013512 01:41:21 04:56:07 left. \n",
      "epoch: 52 train_loss: 0.013506 01:43:22 04:54:13 left. \n",
      "epoch: 53 train_loss: 0.013503 01:45:19 04:52:08 left. \n",
      "epoch: 54 train_loss: 0.013499 01:47:20 04:50:13 left. \n",
      "epoch: 55 train_loss: 0.013496 01:49:20 04:48:15 left. \n",
      "epoch: 56 train_loss: 0.013486 01:51:21 04:46:21 left. \n",
      "epoch: 57 train_loss: 0.013478 01:53:20 04:44:20 left. \n",
      "epoch: 58 train_loss: 0.013467 01:55:19 04:42:20 left. \n",
      "epoch: 59 train_loss: 0.013465 01:57:19 04:40:22 left. \n",
      "epoch: 60 train_loss: 0.013454 01:59:18 04:38:24 left. \n",
      "epoch: 61 train_loss: 0.013453 02:01:21 04:36:32 left. \n",
      "epoch: 62 train_loss: 0.013454 02:03:19 04:34:30 left. \n",
      "epoch: 63 train_loss: 0.013434 02:05:21 04:32:36 left. \n",
      "epoch: 64 train_loss: 0.013439 02:07:20 04:30:37 left. \n",
      "epoch: 65 train_loss: 0.013423 02:09:20 04:28:37 left. \n",
      "epoch: 66 train_loss: 0.013418 02:11:20 04:26:39 left. \n",
      "epoch: 67 train_loss: 0.01342 02:13:20 04:24:41 left. \n",
      "epoch: 68 train_loss: 0.013405 02:15:21 04:22:44 left. \n",
      "epoch: 69 train_loss: 0.0134 02:17:18 04:20:42 left. \n",
      "epoch: 70 train_loss: 0.013394 02:19:17 04:18:40 left. \n",
      "epoch: 71 train_loss: 0.013395 02:21:15 04:16:39 left. \n",
      "epoch: 72 train_loss: 0.013382 02:23:15 04:14:40 left. \n",
      "epoch: 73 train_loss: 0.013371 02:25:16 04:12:43 left. \n",
      "epoch: 74 train_loss: 0.013373 02:27:15 04:10:43 left. \n",
      "epoch: 75 train_loss: 0.013362 02:29:14 04:08:44 left. \n",
      "epoch: 76 train_loss: 0.013364 02:31:12 04:06:42 left. \n",
      "epoch: 77 train_loss: 0.013345 02:33:11 04:04:42 left. \n",
      "epoch: 78 train_loss: 0.013343 02:35:10 04:02:43 left. \n",
      "epoch: 79 train_loss: 0.013335 02:37:09 04:00:42 left. \n",
      "epoch: 80 train_loss: 0.01333 02:39:09 03:58:44 left. \n",
      "epoch: 81 train_loss: 0.013325 02:41:08 03:56:44 left. \n",
      "epoch: 82 train_loss: 0.013316 02:43:09 03:54:47 left. \n",
      "epoch: 83 train_loss: 0.013313 02:45:08 03:52:47 left. \n",
      "epoch: 84 train_loss: 0.013305 02:47:10 03:50:51 left. \n",
      "epoch: 85 train_loss: 0.013301 02:49:10 03:48:52 left. \n",
      "epoch: 86 train_loss: 0.013289 02:51:08 03:46:51 left. \n",
      "epoch: 87 train_loss: 0.013281 02:53:07 03:44:52 left. \n",
      "epoch: 88 train_loss: 0.013272 02:55:07 03:42:52 left. \n",
      "epoch: 89 train_loss: 0.013272 02:57:04 03:40:50 left. \n",
      "epoch: 90 train_loss: 0.013258 02:59:02 03:38:49 left. \n",
      "epoch: 91 train_loss: 0.013254 03:01:03 03:36:52 left. \n",
      "epoch: 92 train_loss: 0.013253 03:03:01 03:34:51 left. \n",
      "epoch: 93 train_loss: 0.013239 03:05:02 03:32:54 left. \n",
      "epoch: 94 train_loss: 0.013238 03:07:04 03:30:56 left. \n",
      "epoch: 95 train_loss: 0.013235 03:09:03 03:28:57 left. \n",
      "epoch: 96 train_loss: 0.013215 03:11:02 03:26:57 left. \n",
      "epoch: 97 train_loss: 0.013216 03:13:00 03:24:56 left. \n",
      "epoch: 98 train_loss: 0.013208 03:15:02 03:22:59 left. \n",
      "epoch: 99 train_loss: 0.013198 03:17:00 03:20:59 left. \n",
      "epoch: 100 train_loss: 0.013192 03:19:01 03:19:01 left. \n",
      "epoch: 101 train_loss: 0.013191 03:20:58 03:16:59 left. \n",
      "epoch: 102 train_loss: 0.013178 03:22:55 03:14:58 left. \n",
      "epoch: 103 train_loss: 0.013172 03:24:56 03:12:59 left. \n",
      "epoch: 104 train_loss: 0.013162 03:26:56 03:11:01 left. \n",
      "epoch: 105 train_loss: 0.013158 03:28:55 03:09:02 left. \n",
      "epoch: 106 train_loss: 0.013152 03:30:55 03:07:02 left. \n",
      "epoch: 107 train_loss: 0.013138 03:32:55 03:05:03 left. \n",
      "epoch: 108 train_loss: 0.013135 03:34:55 03:03:04 left. \n",
      "epoch: 109 train_loss: 0.013125 03:36:53 03:01:04 left. \n",
      "epoch: 110 train_loss: 0.013116 03:38:53 02:59:05 left. \n",
      "epoch: 111 train_loss: 0.013117 03:40:52 02:57:05 left. \n",
      "epoch: 112 train_loss: 0.01311 03:42:52 02:55:06 left. \n",
      "epoch: 113 train_loss: 0.013104 03:44:51 02:53:07 left. \n",
      "epoch: 114 train_loss: 0.013095 03:46:52 02:51:08 left. \n",
      "epoch: 115 train_loss: 0.013081 03:48:51 02:49:09 left. \n",
      "epoch: 116 train_loss: 0.013073 03:50:52 02:47:11 left. \n",
      "epoch: 117 train_loss: 0.013072 03:52:53 02:45:13 left. \n",
      "epoch: 118 train_loss: 0.013061 03:54:53 02:43:13 left. \n",
      "epoch: 119 train_loss: 0.013057 03:56:55 02:41:15 left. \n",
      "epoch: 120 train_loss: 0.013045 03:58:54 02:39:16 left. \n",
      "epoch: 121 train_loss: 0.01304 04:00:52 02:37:15 left. \n",
      "epoch: 122 train_loss: 0.013031 04:02:52 02:35:16 left. \n",
      "epoch: 123 train_loss: 0.013027 04:04:49 02:33:16 left. \n",
      "epoch: 124 train_loss: 0.013017 04:06:50 02:31:17 left. \n",
      "epoch: 125 train_loss: 0.013017 04:08:49 02:29:17 left. \n",
      "epoch: 126 train_loss: 0.013003 04:10:50 02:27:18 left. \n",
      "epoch: 127 train_loss: 0.012991 04:12:48 02:25:19 left. \n",
      "epoch: 128 train_loss: 0.012984 04:14:48 02:23:19 left. \n",
      "epoch: 129 train_loss: 0.012981 04:16:49 02:21:21 left. \n",
      "epoch: 130 train_loss: 0.012976 04:18:49 02:19:22 left. \n",
      "epoch: 131 train_loss: 0.012964 04:20:51 02:17:23 left. \n",
      "epoch: 132 train_loss: 0.012954 04:22:49 02:15:23 left. \n",
      "epoch: 133 train_loss: 0.012945 04:24:49 02:13:24 left. \n",
      "epoch: 134 train_loss: 0.012939 04:26:45 02:11:23 left. \n",
      "epoch: 135 train_loss: 0.012931 04:28:44 02:09:23 left. \n",
      "epoch: 136 train_loss: 0.012927 04:30:42 02:07:23 left. \n",
      "epoch: 137 train_loss: 0.012915 04:32:41 02:05:23 left. \n",
      "epoch: 138 train_loss: 0.012912 04:34:40 02:03:24 left. \n",
      "epoch: 139 train_loss: 0.012901 04:36:38 02:01:24 left. \n",
      "epoch: 140 train_loss: 0.012897 04:38:36 01:59:24 left. \n",
      "epoch: 141 train_loss: 0.012885 04:40:36 01:57:25 left. \n",
      "epoch: 142 train_loss: 0.012878 04:42:36 01:55:25 left. \n",
      "epoch: 143 train_loss: 0.012869 04:44:36 01:53:26 left. \n",
      "epoch: 144 train_loss: 0.01286 04:46:37 01:51:27 left. \n",
      "epoch: 145 train_loss: 0.012855 04:48:34 01:49:27 left. \n",
      "epoch: 146 train_loss: 0.012844 04:50:32 01:47:27 left. \n",
      "epoch: 147 train_loss: 0.012836 04:52:29 01:45:27 left. \n",
      "epoch: 148 train_loss: 0.012832 04:54:27 01:43:27 left. \n",
      "epoch: 149 train_loss: 0.012823 04:56:27 01:41:28 left. \n",
      "epoch: 150 train_loss: 0.012811 04:58:25 01:39:28 left. \n",
      "epoch: 151 train_loss: 0.012809 05:00:24 01:37:28 left. \n",
      "epoch: 152 train_loss: 0.012799 05:02:23 01:35:29 left. \n",
      "epoch: 153 train_loss: 0.01279 05:04:21 01:33:29 left. \n",
      "epoch: 154 train_loss: 0.012779 05:06:21 01:31:30 left. \n",
      "epoch: 155 train_loss: 0.01277 05:08:21 01:29:31 left. \n",
      "epoch: 156 train_loss: 0.012765 05:10:18 01:27:31 left. \n",
      "epoch: 157 train_loss: 0.012755 05:12:16 01:25:31 left. \n",
      "epoch: 158 train_loss: 0.012752 05:14:15 01:23:32 left. \n",
      "epoch: 159 train_loss: 0.012743 05:16:15 01:21:33 left. \n",
      "epoch: 160 train_loss: 0.012731 05:18:15 01:19:33 left. \n",
      "epoch: 161 train_loss: 0.012723 05:20:15 01:17:34 left. \n",
      "epoch: 162 train_loss: 0.012719 05:22:15 01:15:35 left. \n",
      "epoch: 163 train_loss: 0.012706 05:24:15 01:13:36 left. \n",
      "epoch: 164 train_loss: 0.0127 05:26:14 01:11:36 left. \n",
      "epoch: 165 train_loss: 0.012691 05:28:09 01:09:36 left. \n",
      "epoch: 166 train_loss: 0.012687 05:30:05 01:07:36 left. \n",
      "epoch: 167 train_loss: 0.012679 05:32:05 01:05:37 left. \n",
      "epoch: 168 train_loss: 0.012665 05:34:04 01:03:37 left. \n",
      "epoch: 169 train_loss: 0.012659 05:36:05 01:01:39 left. \n",
      "epoch: 170 train_loss: 0.012653 05:38:04 00:59:39 left. \n",
      "epoch: 171 train_loss: 0.012646 05:40:03 00:57:40 left. \n",
      "epoch: 172 train_loss: 0.012639 05:42:03 00:55:41 left. \n",
      "epoch: 173 train_loss: 0.01263 05:44:00 00:53:41 left. \n",
      "epoch: 174 train_loss: 0.012625 05:45:59 00:51:42 left. \n",
      "epoch: 175 train_loss: 0.012614 05:47:56 00:49:42 left. \n",
      "epoch: 176 train_loss: 0.01261 05:49:55 00:47:43 left. \n",
      "epoch: 177 train_loss: 0.012602 05:51:54 00:45:43 left. \n",
      "epoch: 178 train_loss: 0.012599 05:53:54 00:43:44 left. \n",
      "epoch: 179 train_loss: 0.012589 05:55:53 00:41:45 left. \n",
      "epoch: 180 train_loss: 0.012583 05:57:53 00:39:45 left. \n",
      "epoch: 181 train_loss: 0.012577 05:59:53 00:37:46 left. \n",
      "epoch: 182 train_loss: 0.01257 06:01:53 00:35:47 left. \n",
      "epoch: 183 train_loss: 0.012565 06:03:53 00:33:48 left. \n",
      "epoch: 184 train_loss: 0.012561 06:05:54 00:31:49 left. \n",
      "epoch: 185 train_loss: 0.012554 06:07:54 00:29:49 left. \n",
      "epoch: 186 train_loss: 0.012547 06:09:53 00:27:50 left. \n",
      "epoch: 187 train_loss: 0.012543 06:11:53 00:25:51 left. \n",
      "epoch: 188 train_loss: 0.01254 06:13:54 00:23:51 left. \n",
      "epoch: 189 train_loss: 0.012532 06:15:53 00:21:52 left. \n",
      "epoch: 190 train_loss: 0.012529 06:17:55 00:19:53 left. \n",
      "epoch: 191 train_loss: 0.012529 06:19:55 00:17:54 left. \n",
      "epoch: 192 train_loss: 0.012522 06:21:56 00:15:54 left. \n",
      "epoch: 193 train_loss: 0.012516 06:23:56 00:13:55 left. \n",
      "epoch: 194 train_loss: 0.012515 06:25:55 00:11:56 left. \n",
      "epoch: 195 train_loss: 0.012513 06:27:56 00:09:56 left. \n",
      "epoch: 196 train_loss: 0.012511 06:29:56 00:07:57 left. \n",
      "epoch: 197 train_loss: 0.012516 06:31:55 00:05:58 left. \n",
      "epoch: 198 train_loss: 0.012513 06:33:54 00:03:58 left. \n",
      "epoch: 199 train_loss: 0.012507 06:35:53 00:01:59 left. \n",
      "epoch: 200 train_loss: 0.012511 06:37:53 00:00:00 left. \n",
      "Start SubNet warmup\n",
      "epoch: 1 train_loss: 0.01356 Test Acc.：245636/255000(96.33%),251555/255000(98.65%) 0.013286 00:03:04 10:10:23 left. \n",
      "epoch: 2 train_loss: 0.013104 Test Acc.：246367/255000(96.61%),252253/255000(98.92%) 0.013042 00:06:10 10:10:50 left. \n",
      "epoch: 3 train_loss: 0.013034 Test Acc.：246605/255000(96.71%),252713/255000(99.10%) 0.012985 00:09:14 10:07:20 left. \n",
      "epoch: 4 train_loss: 0.01302 Test Acc.：246240/255000(96.56%),251957/255000(98.81%) 0.013042 00:12:20 10:04:38 left. \n",
      "epoch: 5 train_loss: 0.012982 Test Acc.：247062/255000(96.89%),253248/255000(99.31%) 0.0129 00:15:28 10:03:50 left. \n",
      "epoch: 6 train_loss: 0.012963 Test Acc.：247078/255000(96.89%),253250/255000(99.31%) 0.012902 00:18:40 10:03:45 left. \n",
      "epoch: 7 train_loss: 0.012946 Test Acc.：247065/255000(96.89%),253303/255000(99.33%) 0.012915 00:21:48 10:01:28 left. \n",
      "epoch: 8 train_loss: 0.012946 Test Acc.：247156/255000(96.92%),253317/255000(99.34%) 0.012877 00:24:53 09:57:22 left. \n",
      "epoch: 9 train_loss: 0.012923 Test Acc.：247010/255000(96.87%),253154/255000(99.28%) 0.012883 00:28:05 09:56:04 left. \n",
      "epoch: 10 train_loss: 0.012913 Test Acc.：247206/255000(96.94%),253413/255000(99.38%) 0.012861 00:31:16 09:54:08 left. \n",
      "epoch: 11 train_loss: 0.012918 Test Acc.：247209/255000(96.94%),253451/255000(99.39%) 0.012832 00:34:25 09:51:32 left. \n",
      "epoch: 12 train_loss: 0.012907 Test Acc.：247366/255000(97.01%),253619/255000(99.46%) 0.012832 00:37:31 09:47:54 left. \n",
      "epoch: 13 train_loss: 0.012914 Test Acc.：247045/255000(96.88%),253196/255000(99.29%) 0.012881 00:40:33 09:43:31 left. \n",
      "epoch: 14 train_loss: 0.012896 Test Acc.：247489/255000(97.05%),253722/255000(99.50%) 0.012809 00:43:40 09:40:14 left. \n",
      "epoch: 15 train_loss: 0.012901 Test Acc.：247040/255000(96.88%),253188/255000(99.29%) 0.012902 00:46:49 09:37:32 left. \n",
      "epoch: 16 train_loss: 0.012894 Test Acc.：247405/255000(97.02%),253632/255000(99.46%) 0.012814 00:50:00 09:35:10 left. \n",
      "epoch: 17 train_loss: 0.012886 Test Acc.：247338/255000(97.00%),253630/255000(99.46%) 0.012829 00:53:09 09:32:11 left. \n",
      "epoch: 18 train_loss: 0.012895 Test Acc.：247438/255000(97.03%),253697/255000(99.49%) 0.012809 00:56:16 09:29:01 left. \n",
      "epoch: 19 train_loss: 0.01289 Test Acc.：247152/255000(96.92%),253329/255000(99.34%) 0.01289 00:59:22 09:25:36 left. \n",
      "epoch: 20 train_loss: 0.012882 Test Acc.：246874/255000(96.81%),253051/255000(99.24%) 0.012915 01:02:28 09:22:19 left. \n",
      "epoch: 21 train_loss: 0.012892 Test Acc.：247247/255000(96.96%),253502/255000(99.41%) 0.012859 01:05:37 09:19:19 left. \n",
      "epoch: 22 train_loss: 0.012905 Test Acc.：247080/255000(96.89%),253325/255000(99.34%) 0.012902 01:08:44 09:16:07 left. \n",
      "epoch: 23 train_loss: 0.012906 Test Acc.：247276/255000(96.97%),253480/255000(99.40%) 0.012854 01:11:50 09:12:53 left. \n",
      "epoch: 24 train_loss: 0.012901 Test Acc.：247061/255000(96.89%),253326/255000(99.34%) 0.012888 01:15:01 09:10:09 left. \n",
      "epoch: 25 train_loss: 0.012881 Test Acc.：247324/255000(96.99%),253530/255000(99.42%) 0.012826 01:18:13 09:07:36 left. \n",
      "epoch: 26 train_loss: 0.012879 Test Acc.：247368/255000(97.01%),253624/255000(99.46%) 0.012828 01:21:22 09:04:33 left. \n",
      "epoch: 27 train_loss: 0.012887 Test Acc.：247187/255000(96.94%),253370/255000(99.36%) 0.012889 01:24:32 09:01:43 left. \n",
      "epoch: 28 train_loss: 0.012877 Test Acc.：247307/255000(96.98%),253492/255000(99.41%) 0.012822 01:27:41 08:58:41 left. \n",
      "epoch: 29 train_loss: 0.012913 Test Acc.：246591/255000(96.70%),252556/255000(99.04%) 0.013005 01:30:45 08:55:09 left. \n",
      "epoch: 30 train_loss: 0.012894 Test Acc.：246825/255000(96.79%),253051/255000(99.24%) 0.012945 01:33:55 08:52:17 left. \n",
      "epoch: 31 train_loss: 0.012892 Test Acc.：246878/255000(96.81%),253154/255000(99.28%) 0.012945 01:37:05 08:49:16 left. \n",
      "epoch: 32 train_loss: 0.012883 Test Acc.：247272/255000(96.97%),253544/255000(99.43%) 0.012841 01:40:14 08:46:14 left. \n",
      "epoch: 33 train_loss: 0.012885 Test Acc.：247334/255000(96.99%),253624/255000(99.46%) 0.012833 01:43:23 08:43:11 left. \n",
      "epoch: 34 train_loss: 0.012872 Test Acc.：247290/255000(96.98%),253522/255000(99.42%) 0.012845 01:46:35 08:40:23 left. \n",
      "epoch: 35 train_loss: 0.012874 Test Acc.：247482/255000(97.05%),253766/255000(99.52%) 0.012798 01:49:42 08:37:10 left. \n",
      "epoch: 36 train_loss: 0.012889 Test Acc.：247307/255000(96.98%),253594/255000(99.45%) 0.012839 01:52:53 08:34:17 left. \n",
      "epoch: 37 train_loss: 0.012881 Test Acc.：247170/255000(96.93%),253369/255000(99.36%) 0.012872 01:56:03 08:31:18 left. \n",
      "epoch: 38 train_loss: 0.012893 Test Acc.：246982/255000(96.86%),253133/255000(99.27%) 0.01292 01:59:09 08:27:57 left. \n",
      "epoch: 39 train_loss: 0.012871 Test Acc.：247286/255000(96.97%),253500/255000(99.41%) 0.012844 02:02:19 08:24:59 left. \n",
      "epoch: 40 train_loss: 0.012873 Test Acc.：247360/255000(97.00%),253583/255000(99.44%) 0.0128 02:05:32 08:22:08 left. \n",
      "epoch: 41 train_loss: 0.012865 Test Acc.：247344/255000(97.00%),253582/255000(99.44%) 0.012829 02:08:39 08:18:54 left. \n",
      "epoch: 42 train_loss: 0.012861 Test Acc.：247301/255000(96.98%),253472/255000(99.40%) 0.012863 02:11:30 08:14:44 left. \n",
      "epoch: 43 train_loss: 0.012862 Test Acc.：247312/255000(96.99%),253589/255000(99.45%) 0.012841 02:14:28 08:10:58 left. \n",
      "epoch: 44 train_loss: 0.012861 Test Acc.：247408/255000(97.02%),253686/255000(99.48%) 0.012826 02:17:41 08:08:11 left. \n",
      "epoch: 45 train_loss: 0.012871 Test Acc.：247469/255000(97.05%),253752/255000(99.51%) 0.01281 02:20:48 08:04:58 left. \n",
      "epoch: 46 train_loss: 0.012851 Test Acc.：247274/255000(96.97%),253376/255000(99.36%) 0.012855 02:23:55 08:01:49 left. \n",
      "epoch: 47 train_loss: 0.01287 Test Acc.：247229/255000(96.95%),253387/255000(99.37%) 0.012852 02:27:06 07:58:53 left. \n",
      "epoch: 48 train_loss: 0.012843 Test Acc.：247439/255000(97.03%),253657/255000(99.47%) 0.012807 02:30:19 07:56:03 left. \n",
      "epoch: 49 train_loss: 0.012846 Test Acc.：246677/255000(96.74%),252658/255000(99.08%) 0.012981 02:33:25 07:52:49 left. \n",
      "epoch: 50 train_loss: 0.012849 Test Acc.：247429/255000(97.03%),253658/255000(99.47%) 0.012825 02:36:37 07:49:52 left. \n",
      "epoch: 51 train_loss: 0.012829 Test Acc.：247492/255000(97.06%),253803/255000(99.53%) 0.012803 02:39:45 07:46:43 left. \n",
      "epoch: 52 train_loss: 0.012859 Test Acc.：247447/255000(97.04%),253733/255000(99.50%) 0.012794 02:42:48 07:43:22 left. \n",
      "epoch: 53 train_loss: 0.012839 Test Acc.：247615/255000(97.10%),253923/255000(99.58%) 0.012767 02:45:55 07:40:13 left. \n",
      "epoch: 54 train_loss: 0.012843 Test Acc.：247280/255000(96.97%),253488/255000(99.41%) 0.012841 02:49:03 07:37:05 left. \n",
      "epoch: 55 train_loss: 0.012835 Test Acc.：247420/255000(97.03%),253665/255000(99.48%) 0.01282 02:52:14 07:34:06 left. \n",
      "epoch: 56 train_loss: 0.012831 Test Acc.：247176/255000(96.93%),253315/255000(99.34%) 0.012867 02:55:22 07:30:57 left. \n",
      "epoch: 57 train_loss: 0.012838 Test Acc.：247482/255000(97.05%),253767/255000(99.52%) 0.012811 02:58:33 07:27:57 left. \n",
      "epoch: 58 train_loss: 0.012829 Test Acc.：247334/255000(96.99%),253517/255000(99.42%) 0.012827 03:01:38 07:24:42 left. \n",
      "epoch: 59 train_loss: 0.01282 Test Acc.：247473/255000(97.05%),253706/255000(99.49%) 0.012789 03:04:46 07:21:35 left. \n",
      "epoch: 60 train_loss: 0.012833 Test Acc.：246934/255000(96.84%),253067/255000(99.24%) 0.012926 03:07:55 07:18:29 left. \n",
      "epoch: 61 train_loss: 0.012844 Test Acc.：247129/255000(96.91%),253340/255000(99.35%) 0.012867 03:11:03 07:15:22 left. \n",
      "epoch: 62 train_loss: 0.012836 Test Acc.：247094/255000(96.90%),253278/255000(99.32%) 0.012881 03:14:15 07:12:23 left. \n",
      "epoch: 63 train_loss: 0.012844 Test Acc.：247377/255000(97.01%),253703/255000(99.49%) 0.012823 03:17:24 07:09:16 left. \n",
      "epoch: 64 train_loss: 0.012827 Test Acc.：247402/255000(97.02%),253700/255000(99.49%) 0.012808 03:20:28 07:06:00 left. \n",
      "epoch: 65 train_loss: 0.012827 Test Acc.：247555/255000(97.08%),253719/255000(99.50%) 0.012787 03:23:32 07:02:44 left. \n",
      "epoch: 66 train_loss: 0.012828 Test Acc.：247366/255000(97.01%),253579/255000(99.44%) 0.012839 03:26:44 06:59:44 left. \n",
      "epoch: 67 train_loss: 0.012825 Test Acc.：247400/255000(97.02%),253733/255000(99.50%) 0.012811 03:29:49 06:56:31 left. \n",
      "epoch: 68 train_loss: 0.012815 Test Acc.：247555/255000(97.08%),253819/255000(99.54%) 0.012782 03:32:58 06:53:24 left. \n",
      "epoch: 69 train_loss: 0.012812 Test Acc.：247007/255000(96.87%),253300/255000(99.33%) 0.012882 03:36:05 06:50:16 left. \n",
      "epoch: 70 train_loss: 0.012813 Test Acc.：247406/255000(97.02%),253657/255000(99.47%) 0.012849 03:39:14 06:47:10 left. \n",
      "epoch: 71 train_loss: 0.012837 Test Acc.：247481/255000(97.05%),253736/255000(99.50%) 0.012796 03:42:21 06:44:00 left. \n",
      "epoch: 72 train_loss: 0.012818 Test Acc.：247524/255000(97.07%),253810/255000(99.53%) 0.012785 03:45:31 06:40:56 left. \n",
      "epoch: 73 train_loss: 0.01281 Test Acc.：247544/255000(97.08%),253804/255000(99.53%) 0.012802 03:48:37 06:37:44 left. \n",
      "epoch: 74 train_loss: 0.012817 Test Acc.：247378/255000(97.01%),253758/255000(99.51%) 0.01281 03:51:40 06:34:27 left. \n",
      "epoch: 75 train_loss: 0.012822 Test Acc.：247710/255000(97.14%),253951/255000(99.59%) 0.01275 03:54:49 06:31:22 left. \n",
      "epoch: 76 train_loss: 0.012803 Test Acc.：247225/255000(96.95%),253429/255000(99.38%) 0.012865 03:57:58 06:28:17 left. \n",
      "epoch: 77 train_loss: 0.012809 Test Acc.：247592/255000(97.09%),253864/255000(99.55%) 0.012773 04:01:06 06:25:09 left. \n",
      "epoch: 78 train_loss: 0.012798 Test Acc.：246835/255000(96.80%),252884/255000(99.17%) 0.012986 04:04:14 06:22:01 left. \n",
      "epoch: 79 train_loss: 0.012792 Test Acc.：247699/255000(97.14%),253954/255000(99.59%) 0.012765 04:07:21 06:18:52 left. \n",
      "epoch: 80 train_loss: 0.0128 Test Acc.：247549/255000(97.08%),253779/255000(99.52%) 0.012772 04:10:29 06:15:44 left. \n",
      "epoch: 81 train_loss: 0.012792 Test Acc.：247559/255000(97.08%),253844/255000(99.55%) 0.012793 04:13:38 06:12:38 left. \n",
      "epoch: 82 train_loss: 0.012793 Test Acc.：247337/255000(96.99%),253619/255000(99.46%) 0.012835 04:16:49 06:09:33 left. \n",
      "epoch: 83 train_loss: 0.012797 Test Acc.：247505/255000(97.06%),253757/255000(99.51%) 0.012785 04:19:58 06:06:27 left. \n",
      "epoch: 84 train_loss: 0.012803 Test Acc.：247580/255000(97.09%),253751/255000(99.51%) 0.012782 04:23:00 06:03:12 left. \n",
      "epoch: 85 train_loss: 0.01279 Test Acc.：247709/255000(97.14%),253960/255000(99.59%) 0.012751 04:26:09 06:00:05 left. \n",
      "epoch: 86 train_loss: 0.012781 Test Acc.：247675/255000(97.13%),253984/255000(99.60%) 0.012745 04:29:22 05:57:05 left. \n",
      "epoch: 87 train_loss: 0.012793 Test Acc.：246940/255000(96.84%),253188/255000(99.29%) 0.012948 04:32:26 05:53:51 left. \n",
      "epoch: 88 train_loss: 0.012802 Test Acc.：247378/255000(97.01%),253661/255000(99.47%) 0.01282 04:35:38 05:50:49 left. \n",
      "epoch: 89 train_loss: 0.012781 Test Acc.：247624/255000(97.11%),253882/255000(99.56%) 0.012787 04:38:46 05:47:40 left. \n",
      "epoch: 90 train_loss: 0.012785 Test Acc.：247405/255000(97.02%),253641/255000(99.47%) 0.012822 04:41:52 05:44:30 left. \n",
      "epoch: 91 train_loss: 0.012786 Test Acc.：247574/255000(97.09%),253809/255000(99.53%) 0.012774 04:45:03 05:41:26 left. \n",
      "epoch: 92 train_loss: 0.012771 Test Acc.：247625/255000(97.11%),253917/255000(99.58%) 0.01277 04:48:12 05:38:19 left. \n",
      "epoch: 93 train_loss: 0.012765 Test Acc.：247621/255000(97.11%),253938/255000(99.58%) 0.012755 04:51:21 05:35:13 left. \n",
      "epoch: 94 train_loss: 0.012772 Test Acc.：247623/255000(97.11%),253904/255000(99.57%) 0.012754 04:54:29 05:32:05 left. \n",
      "epoch: 95 train_loss: 0.012773 Test Acc.：247534/255000(97.07%),253837/255000(99.54%) 0.01278 04:57:37 05:28:57 left. \n",
      "epoch: 96 train_loss: 0.012785 Test Acc.：247618/255000(97.11%),253915/255000(99.57%) 0.012762 05:00:45 05:25:49 left. \n",
      "epoch: 97 train_loss: 0.01276 Test Acc.：247525/255000(97.07%),253813/255000(99.53%) 0.012799 05:03:51 05:22:39 left. \n",
      "epoch: 98 train_loss: 0.012768 Test Acc.：247482/255000(97.05%),253723/255000(99.50%) 0.01279 05:06:58 05:19:29 left. \n",
      "epoch: 99 train_loss: 0.012772 Test Acc.：247483/255000(97.05%),253835/255000(99.54%) 0.012774 05:10:04 05:16:19 left. \n",
      "epoch: 100 train_loss: 0.012766 Test Acc.：247619/255000(97.11%),253900/255000(99.57%) 0.012767 05:13:06 05:13:06 left. \n",
      "epoch: 101 train_loss: 0.012759 Test Acc.：247661/255000(97.12%),253910/255000(99.57%) 0.012765 05:16:12 05:09:56 left. \n",
      "epoch: 102 train_loss: 0.01277 Test Acc.：247577/255000(97.09%),253818/255000(99.54%) 0.012782 05:19:20 05:06:49 left. \n",
      "epoch: 103 train_loss: 0.012766 Test Acc.：247313/255000(96.99%),253545/255000(99.43%) 0.012806 05:22:29 05:03:42 left. \n",
      "epoch: 104 train_loss: 0.012764 Test Acc.：247500/255000(97.06%),253815/255000(99.54%) 0.012771 05:25:37 05:00:34 left. \n",
      "epoch: 105 train_loss: 0.012766 Test Acc.：247508/255000(97.06%),253699/255000(99.49%) 0.012785 05:28:42 04:57:24 left. \n",
      "epoch: 106 train_loss: 0.01275 Test Acc.：247626/255000(97.11%),253865/255000(99.55%) 0.01277 05:31:47 04:54:13 left. \n",
      "epoch: 107 train_loss: 0.012776 Test Acc.：247535/255000(97.07%),253750/255000(99.51%) 0.012788 05:34:59 04:51:09 left. \n",
      "epoch: 108 train_loss: 0.012766 Test Acc.：247606/255000(97.10%),253875/255000(99.56%) 0.01276 05:38:05 04:48:00 left. \n",
      "epoch: 109 train_loss: 0.012746 Test Acc.：247644/255000(97.12%),253984/255000(99.60%) 0.012747 05:41:11 04:44:51 left. \n",
      "epoch: 110 train_loss: 0.012737 Test Acc.：247411/255000(97.02%),253615/255000(99.46%) 0.012818 05:44:18 04:41:42 left. \n",
      "epoch: 111 train_loss: 0.012738 Test Acc.：247720/255000(97.15%),254011/255000(99.61%) 0.01275 05:47:30 04:38:38 left. \n",
      "epoch: 112 train_loss: 0.012734 Test Acc.：247659/255000(97.12%),253943/255000(99.59%) 0.012751 05:50:37 04:35:29 left. \n",
      "epoch: 113 train_loss: 0.01274 Test Acc.：247640/255000(97.11%),253967/255000(99.59%) 0.012776 05:53:48 04:32:24 left. \n",
      "epoch: 114 train_loss: 0.012732 Test Acc.：247607/255000(97.10%),253894/255000(99.57%) 0.012763 05:56:57 04:29:16 left. \n",
      "epoch: 115 train_loss: 0.012749 Test Acc.：247731/255000(97.15%),253988/255000(99.60%) 0.012739 06:00:03 04:26:07 left. \n",
      "epoch: 116 train_loss: 0.012731 Test Acc.：247699/255000(97.14%),254016/255000(99.61%) 0.012758 06:03:10 04:22:59 left. \n",
      "epoch: 117 train_loss: 0.012728 Test Acc.：247587/255000(97.09%),253817/255000(99.54%) 0.01276 06:06:16 04:19:50 left. \n",
      "epoch: 118 train_loss: 0.012725 Test Acc.：247741/255000(97.15%),254010/255000(99.61%) 0.012742 06:09:24 04:16:42 left. \n",
      "epoch: 119 train_loss: 0.012732 Test Acc.：247782/255000(97.17%),254117/255000(99.65%) 0.01272 06:12:31 04:13:33 left. \n",
      "epoch: 120 train_loss: 0.012725 Test Acc.：247793/255000(97.17%),254132/255000(99.66%) 0.012713 06:15:44 04:10:29 left. \n",
      "epoch: 121 train_loss: 0.012728 Test Acc.：247750/255000(97.16%),254055/255000(99.63%) 0.012735 06:18:52 04:07:21 left. \n",
      "epoch: 122 train_loss: 0.012713 Test Acc.：247702/255000(97.14%),254006/255000(99.61%) 0.012745 06:21:57 04:04:12 left. \n",
      "epoch: 123 train_loss: 0.012708 Test Acc.：247811/255000(97.18%),254085/255000(99.64%) 0.012727 06:25:06 04:01:05 left. \n",
      "epoch: 124 train_loss: 0.012711 Test Acc.：247439/255000(97.03%),253663/255000(99.48%) 0.012786 06:28:12 03:57:56 left. \n",
      "epoch: 125 train_loss: 0.012708 Test Acc.：247803/255000(97.18%),254118/255000(99.65%) 0.012729 06:31:20 03:54:48 left. \n",
      "epoch: 126 train_loss: 0.012701 Test Acc.：247789/255000(97.17%),254048/255000(99.63%) 0.012726 06:34:31 03:51:42 left. \n",
      "epoch: 127 train_loss: 0.012708 Test Acc.：247771/255000(97.17%),254057/255000(99.63%) 0.012726 06:37:43 03:48:37 left. \n",
      "epoch: 128 train_loss: 0.012701 Test Acc.：247742/255000(97.15%),254046/255000(99.63%) 0.012735 06:40:52 03:45:29 left. \n",
      "epoch: 129 train_loss: 0.0127 Test Acc.：247762/255000(97.16%),254097/255000(99.65%) 0.012733 06:44:00 03:42:21 left. \n",
      "epoch: 130 train_loss: 0.012694 Test Acc.：247732/255000(97.15%),254051/255000(99.63%) 0.012748 06:47:06 03:39:12 left. \n",
      "epoch: 131 train_loss: 0.012705 Test Acc.：247760/255000(97.16%),254080/255000(99.64%) 0.012721 06:50:15 03:36:05 left. \n",
      "epoch: 132 train_loss: 0.012695 Test Acc.：247791/255000(97.17%),254115/255000(99.65%) 0.012739 06:53:23 03:32:57 left. \n",
      "epoch: 133 train_loss: 0.012698 Test Acc.：247741/255000(97.15%),254073/255000(99.64%) 0.012718 06:56:31 03:29:49 left. \n",
      "epoch: 134 train_loss: 0.01269 Test Acc.：247784/255000(97.17%),254089/255000(99.64%) 0.012714 06:59:40 03:26:42 left. \n",
      "epoch: 135 train_loss: 0.012684 Test Acc.：247791/255000(97.17%),254159/255000(99.67%) 0.012723 07:02:48 03:23:34 left. \n",
      "epoch: 136 train_loss: 0.012684 Test Acc.：247786/255000(97.17%),254105/255000(99.65%) 0.012727 07:05:55 03:20:26 left. \n",
      "epoch: 137 train_loss: 0.012682 Test Acc.：247865/255000(97.20%),254186/255000(99.68%) 0.012715 07:09:00 03:17:17 left. \n",
      "epoch: 138 train_loss: 0.012679 Test Acc.：247777/255000(97.17%),254079/255000(99.64%) 0.012728 07:12:09 03:14:09 left. \n",
      "epoch: 139 train_loss: 0.012671 Test Acc.：247795/255000(97.17%),254141/255000(99.66%) 0.01272 07:15:14 03:11:00 left. \n",
      "epoch: 140 train_loss: 0.012675 Test Acc.：247823/255000(97.19%),254145/255000(99.66%) 0.012706 07:18:23 03:07:52 left. \n",
      "epoch: 141 train_loss: 0.012678 Test Acc.：247834/255000(97.19%),254135/255000(99.66%) 0.012712 07:21:33 03:04:45 left. \n",
      "epoch: 142 train_loss: 0.012666 Test Acc.：247768/255000(97.16%),254097/255000(99.65%) 0.012728 07:24:42 03:01:38 left. \n",
      "epoch: 143 train_loss: 0.012676 Test Acc.：247780/255000(97.17%),254140/255000(99.66%) 0.012726 07:27:51 02:58:31 left. \n",
      "epoch: 144 train_loss: 0.012668 Test Acc.：247804/255000(97.18%),254117/255000(99.65%) 0.012719 07:30:57 02:55:22 left. \n",
      "epoch: 145 train_loss: 0.012673 Test Acc.：247771/255000(97.17%),254016/255000(99.61%) 0.012733 07:34:05 02:52:14 left. \n",
      "epoch: 146 train_loss: 0.012665 Test Acc.：247796/255000(97.17%),254162/255000(99.67%) 0.012724 07:37:11 02:49:06 left. \n",
      "epoch: 147 train_loss: 0.012658 Test Acc.：247875/255000(97.21%),254199/255000(99.69%) 0.01271 07:40:20 02:45:58 left. \n",
      "epoch: 148 train_loss: 0.012655 Test Acc.：247817/255000(97.18%),254133/255000(99.66%) 0.012724 07:43:31 02:42:51 left. \n",
      "epoch: 149 train_loss: 0.012661 Test Acc.：247865/255000(97.20%),254185/255000(99.68%) 0.012707 07:46:38 02:39:43 left. \n",
      "epoch: 150 train_loss: 0.012651 Test Acc.：247846/255000(97.19%),254203/255000(99.69%) 0.012715 07:49:44 02:36:34 left. \n",
      "epoch: 151 train_loss: 0.012649 Test Acc.：247896/255000(97.21%),254201/255000(99.69%) 0.012708 07:52:54 02:33:27 left. \n",
      "epoch: 152 train_loss: 0.012652 Test Acc.：247800/255000(97.18%),254192/255000(99.68%) 0.012704 07:55:56 02:30:17 left. \n",
      "epoch: 153 train_loss: 0.012647 Test Acc.：247869/255000(97.20%),254218/255000(99.69%) 0.012703 07:59:04 02:27:10 left. \n",
      "epoch: 154 train_loss: 0.012642 Test Acc.：247874/255000(97.21%),254182/255000(99.68%) 0.012704 08:02:12 02:24:02 left. \n",
      "epoch: 155 train_loss: 0.012639 Test Acc.：247829/255000(97.19%),254165/255000(99.67%) 0.012717 08:05:22 02:20:54 left. \n",
      "epoch: 156 train_loss: 0.012636 Test Acc.：247787/255000(97.17%),254136/255000(99.66%) 0.012713 08:08:28 02:17:46 left. \n",
      "epoch: 157 train_loss: 0.012635 Test Acc.：247889/255000(97.21%),254242/255000(99.70%) 0.01271 08:11:38 02:14:39 left. \n",
      "epoch: 158 train_loss: 0.012632 Test Acc.：247871/255000(97.20%),254216/255000(99.69%) 0.012701 08:14:46 02:11:31 left. \n",
      "epoch: 159 train_loss: 0.012632 Test Acc.：247916/255000(97.22%),254255/255000(99.71%) 0.012705 08:17:54 02:08:23 left. \n",
      "epoch: 160 train_loss: 0.012627 Test Acc.：247917/255000(97.22%),254243/255000(99.70%) 0.012705 08:21:03 02:05:15 left. \n",
      "epoch: 161 train_loss: 0.01263 Test Acc.：247878/255000(97.21%),254229/255000(99.70%) 0.012694 08:24:09 02:02:07 left. \n",
      "epoch: 162 train_loss: 0.012625 Test Acc.：247915/255000(97.22%),254229/255000(99.70%) 0.012705 08:27:08 01:58:57 left. \n",
      "epoch: 163 train_loss: 0.012623 Test Acc.：247893/255000(97.21%),254257/255000(99.71%) 0.012689 08:30:14 01:55:49 left. \n",
      "epoch: 164 train_loss: 0.012619 Test Acc.：247874/255000(97.21%),254214/255000(99.69%) 0.012714 08:33:21 01:52:41 left. \n",
      "epoch: 165 train_loss: 0.012616 Test Acc.：247868/255000(97.20%),254182/255000(99.68%) 0.012707 08:36:27 01:49:33 left. \n",
      "epoch: 166 train_loss: 0.012619 Test Acc.：247889/255000(97.21%),254220/255000(99.69%) 0.012717 08:39:33 01:46:25 left. \n",
      "epoch: 167 train_loss: 0.012615 Test Acc.：247904/255000(97.22%),254266/255000(99.71%) 0.012698 08:42:42 01:43:17 left. \n",
      "epoch: 168 train_loss: 0.012608 Test Acc.：247930/255000(97.23%),254265/255000(99.71%) 0.012696 08:45:52 01:40:09 left. \n",
      "epoch: 169 train_loss: 0.012611 Test Acc.：247968/255000(97.24%),254243/255000(99.70%) 0.012698 08:48:58 01:37:01 left. \n",
      "epoch: 170 train_loss: 0.012609 Test Acc.：247918/255000(97.22%),254284/255000(99.72%) 0.012692 08:52:03 01:33:53 left. \n",
      "epoch: 171 train_loss: 0.012609 Test Acc.：247896/255000(97.21%),254243/255000(99.70%) 0.012695 08:55:12 01:30:46 left. \n",
      "epoch: 172 train_loss: 0.012603 Test Acc.：247943/255000(97.23%),254265/255000(99.71%) 0.012704 08:58:17 01:27:37 left. \n",
      "epoch: 173 train_loss: 0.012603 Test Acc.：247954/255000(97.24%),254287/255000(99.72%) 0.012695 09:01:22 01:24:29 left. \n",
      "epoch: 174 train_loss: 0.012604 Test Acc.：247944/255000(97.23%),254272/255000(99.71%) 0.012701 09:04:25 01:21:21 left. \n",
      "epoch: 175 train_loss: 0.0126 Test Acc.：247922/255000(97.22%),254226/255000(99.70%) 0.012706 09:07:30 01:18:12 left. \n",
      "epoch: 176 train_loss: 0.0126 Test Acc.：247950/255000(97.24%),254265/255000(99.71%) 0.012699 09:10:35 01:15:04 left. \n",
      "epoch: 177 train_loss: 0.012597 Test Acc.：247906/255000(97.22%),254248/255000(99.71%) 0.012708 09:13:41 01:11:56 left. \n",
      "epoch: 178 train_loss: 0.012594 Test Acc.：247963/255000(97.24%),254277/255000(99.72%) 0.012711 09:16:42 01:08:48 left. \n",
      "epoch: 179 train_loss: 0.012595 Test Acc.：247932/255000(97.23%),254283/255000(99.72%) 0.0127 09:19:53 01:05:41 left. \n",
      "epoch: 180 train_loss: 0.012593 Test Acc.：247941/255000(97.23%),254283/255000(99.72%) 0.012687 09:23:02 01:02:33 left. \n",
      "epoch: 181 train_loss: 0.01259 Test Acc.：247949/255000(97.23%),254311/255000(99.73%) 0.012704 09:26:08 00:59:25 left. \n",
      "epoch: 182 train_loss: 0.012587 Test Acc.：247954/255000(97.24%),254273/255000(99.71%) 0.012689 09:29:13 00:56:17 left. \n",
      "epoch: 183 train_loss: 0.012594 Test Acc.：247935/255000(97.23%),254257/255000(99.71%) 0.012703 09:32:20 00:53:10 left. \n",
      "epoch: 184 train_loss: 0.012589 Test Acc.：247945/255000(97.23%),254274/255000(99.72%) 0.012686 09:35:29 00:50:02 left. \n",
      "epoch: 185 train_loss: 0.012588 Test Acc.：247931/255000(97.23%),254248/255000(99.71%) 0.012696 09:38:41 00:46:55 left. \n",
      "epoch: 186 train_loss: 0.012587 Test Acc.：247968/255000(97.24%),254304/255000(99.73%) 0.012698 09:41:49 00:43:47 left. \n",
      "epoch: 187 train_loss: 0.012579 Test Acc.：247941/255000(97.23%),254293/255000(99.72%) 0.0127 09:44:55 00:40:39 left. \n",
      "epoch: 188 train_loss: 0.012584 Test Acc.：247931/255000(97.23%),254298/255000(99.72%) 0.012686 09:48:03 00:37:32 left. \n",
      "epoch: 189 train_loss: 0.012583 Test Acc.：247933/255000(97.23%),254253/255000(99.71%) 0.012693 09:51:12 00:34:24 left. \n",
      "epoch: 190 train_loss: 0.012585 Test Acc.：247933/255000(97.23%),254283/255000(99.72%) 0.012695 09:54:19 00:31:16 left. \n",
      "epoch: 191 train_loss: 0.012583 Test Acc.：247964/255000(97.24%),254285/255000(99.72%) 0.012692 09:57:26 00:28:09 left. \n",
      "epoch: 192 train_loss: 0.01258 Test Acc.：247944/255000(97.23%),254294/255000(99.72%) 0.012697 10:00:36 00:25:01 left. \n",
      "epoch: 193 train_loss: 0.012584 Test Acc.：247953/255000(97.24%),254272/255000(99.71%) 0.012694 10:03:43 00:21:53 left. \n",
      "epoch: 194 train_loss: 0.012583 Test Acc.：247938/255000(97.23%),254298/255000(99.72%) 0.012685 10:06:56 00:18:46 left. \n",
      "epoch: 195 train_loss: 0.012583 Test Acc.：247968/255000(97.24%),254303/255000(99.73%) 0.012699 10:10:04 00:15:38 left. \n",
      "epoch: 196 train_loss: 0.012582 Test Acc.：247912/255000(97.22%),254300/255000(99.73%) 0.012684 10:13:11 00:12:30 left. \n",
      "epoch: 197 train_loss: 0.012579 Test Acc.：247961/255000(97.24%),254295/255000(99.72%) 0.012699 10:16:17 00:09:23 left. \n",
      "epoch: 198 train_loss: 0.01258 Test Acc.：247938/255000(97.23%),254294/255000(99.72%) 0.012691 10:19:26 00:06:15 left. \n",
      "epoch: 199 train_loss: 0.012585 Test Acc.：247977/255000(97.25%),254299/255000(99.73%) 0.012699 10:22:36 00:03:07 left. \n",
      "epoch: 200 train_loss: 0.012581 Test Acc.：247964/255000(97.24%),254298/255000(99.72%) 0.012686 10:25:45 00:00:00 left. \n",
      "Test Acc.：247964/255000(97.24%),254298/255000(99.72%) 0.012686 \n",
      "epoch: 1 train_loss: 0.014497 Test Acc.：243090/255000(95.33%),252889/255000(99.17%) 0.013668 Save model 00:03:17 10:55:42 left. \n",
      "epoch: 2 train_loss: 0.013859 Test Acc.：243220/255000(95.38%),252791/255000(99.13%) 0.01365 Save model 00:06:40 11:00:48 left. \n",
      "epoch: 3 train_loss: 0.013803 Test Acc.：243679/255000(95.56%),252964/255000(99.20%) 0.01352 Save model 00:09:58 10:55:25 left. \n",
      "epoch: 4 train_loss: 0.013778 Test Acc.：243811/255000(95.61%),252538/255000(99.03%) 0.013551 Save model 00:13:16 10:50:30 left. \n",
      "epoch: 5 train_loss: 0.013769 Test Acc.：243164/255000(95.36%),252812/255000(99.14%) 0.013654 Save model 00:16:28 10:42:21 left. \n",
      "epoch: 6 train_loss: 0.01377 Test Acc.：243911/255000(95.65%),253168/255000(99.28%) 0.013538 00:19:44 10:38:14 left. \n",
      "epoch: 7 train_loss: 0.013754 Test Acc.：243849/255000(95.63%),253035/255000(99.23%) 0.013483 Save model 00:23:02 10:35:30 left. \n",
      "epoch: 8 train_loss: 0.013751 Test Acc.：243380/255000(95.44%),252940/255000(99.19%) 0.013544 Save model 00:26:27 10:34:49 left. \n",
      "epoch: 9 train_loss: 0.013741 Test Acc.：243203/255000(95.37%),253006/255000(99.22%) 0.013619 Save model 00:29:49 10:32:50 left. \n",
      "epoch: 10 train_loss: 0.013738 Test Acc.：243678/255000(95.56%),252920/255000(99.18%) 0.013517 Save model 00:33:11 10:30:35 left. \n",
      "epoch: 11 train_loss: 0.013738 Test Acc.：243961/255000(95.67%),252566/255000(99.05%) 0.013516 00:36:28 10:26:41 left. \n",
      "epoch: 12 train_loss: 0.013722 Test Acc.：244032/255000(95.70%),253044/255000(99.23%) 0.013535 Save model 00:39:43 10:22:20 left. \n",
      "epoch: 13 train_loss: 0.013734 Test Acc.：244148/255000(95.74%),252937/255000(99.19%) 0.013482 00:43:03 10:19:21 left. \n",
      "epoch: 14 train_loss: 0.013723 Test Acc.：243892/255000(95.64%),252874/255000(99.17%) 0.013448 00:46:25 10:16:49 left. \n",
      "epoch: 15 train_loss: 0.013721 Test Acc.：243331/255000(95.42%),252728/255000(99.11%) 0.013623 Save model 00:49:44 10:13:31 left. \n",
      "epoch: 16 train_loss: 0.013724 Test Acc.：243887/255000(95.64%),252500/255000(99.02%) 0.013493 00:53:04 10:10:19 left. \n",
      "epoch: 17 train_loss: 0.013739 Test Acc.：243917/255000(95.65%),252805/255000(99.14%) 0.013533 00:56:22 10:06:53 left. \n",
      "epoch: 18 train_loss: 0.013713 Test Acc.：243876/255000(95.64%),252788/255000(99.13%) 0.013583 Save model 00:59:39 10:03:13 left. \n",
      "epoch: 19 train_loss: 0.013713 Test Acc.：243012/255000(95.30%),252866/255000(99.16%) 0.013575 Save model 01:03:03 10:00:41 left. \n",
      "epoch: 20 train_loss: 0.013705 Test Acc.：243910/255000(95.65%),252877/255000(99.17%) 0.013478 Save model 01:06:27 09:58:11 left. \n",
      "epoch: 21 train_loss: 0.013705 Test Acc.：243454/255000(95.47%),252956/255000(99.20%) 0.013507 Save model 01:09:52 09:55:31 left. \n",
      "epoch: 22 train_loss: 0.013692 Test Acc.：243286/255000(95.41%),251444/255000(98.61%) 0.01356 Save model 01:13:14 09:52:39 left. \n",
      "epoch: 23 train_loss: 0.013679 Test Acc.：243932/255000(95.66%),252853/255000(99.16%) 0.013483 Save model 01:16:34 09:49:19 left. \n",
      "epoch: 24 train_loss: 0.013723 Test Acc.：243853/255000(95.63%),252922/255000(99.19%) 0.013496 01:19:56 09:46:17 left. \n",
      "epoch: 25 train_loss: 0.013683 Test Acc.：243724/255000(95.58%),252689/255000(99.09%) 0.013479 01:23:18 09:43:08 left. \n",
      "epoch: 26 train_loss: 0.013691 Test Acc.：244192/255000(95.76%),252874/255000(99.17%) 0.013424 01:26:33 09:39:18 left. \n",
      "epoch: 27 train_loss: 0.01367 Test Acc.：244162/255000(95.75%),252844/255000(99.15%) 0.013466 Save model 01:29:51 09:35:45 left. \n",
      "epoch: 28 train_loss: 0.013683 Test Acc.：244024/255000(95.70%),252825/255000(99.15%) 0.013537 01:33:08 09:32:10 left. \n",
      "epoch: 29 train_loss: 0.013676 Test Acc.：243803/255000(95.61%),252681/255000(99.09%) 0.013553 01:36:31 09:29:10 left. \n",
      "epoch: 30 train_loss: 0.013674 Test Acc.：244311/255000(95.81%),252859/255000(99.16%) 0.013502 01:39:49 09:25:42 left. \n",
      "epoch: 31 train_loss: 0.013665 Test Acc.：244193/255000(95.76%),252832/255000(99.15%) 0.013446 Save model 01:43:09 09:22:24 left. \n",
      "epoch: 32 train_loss: 0.013664 Test Acc.：243605/255000(95.53%),252851/255000(99.16%) 0.01354 Save model 01:46:28 09:18:59 left. \n",
      "epoch: 33 train_loss: 0.013668 Test Acc.：243838/255000(95.62%),252691/255000(99.09%) 0.013521 01:49:49 09:15:44 left. \n",
      "epoch: 34 train_loss: 0.013656 Test Acc.：244000/255000(95.69%),252156/255000(98.88%) 0.013447 Save model 01:53:09 09:12:26 left. \n",
      "epoch: 35 train_loss: 0.013653 Test Acc.：243870/255000(95.64%),252711/255000(99.10%) 0.013566 Save model 01:56:28 09:09:06 left. \n",
      "epoch: 36 train_loss: 0.013646 Test Acc.：243818/255000(95.61%),252763/255000(99.12%) 0.013534 Save model 01:59:44 09:05:29 left. \n",
      "epoch: 37 train_loss: 0.013637 Test Acc.：244064/255000(95.71%),252627/255000(99.07%) 0.013474 Save model 02:03:00 09:01:55 left. \n",
      "epoch: 38 train_loss: 0.013637 Test Acc.：243690/255000(95.56%),252932/255000(99.19%) 0.013396 Save model 02:06:19 08:58:32 left. \n",
      "epoch: 39 train_loss: 0.013624 Test Acc.：243873/255000(95.64%),252526/255000(99.03%) 0.01354 Save model 02:09:34 08:54:54 left. \n",
      "epoch: 40 train_loss: 0.013645 Test Acc.：244308/255000(95.81%),252775/255000(99.13%) 0.013459 02:12:55 08:51:41 left. \n",
      "epoch: 41 train_loss: 0.013623 Test Acc.：243696/255000(95.57%),252762/255000(99.12%) 0.013577 Save model 02:16:15 08:48:26 left. \n",
      "epoch: 42 train_loss: 0.013607 Test Acc.：244334/255000(95.82%),252682/255000(99.09%) 0.013414 Save model 02:19:33 08:44:59 left. \n",
      "epoch: 43 train_loss: 0.013612 Test Acc.：244133/255000(95.74%),252723/255000(99.11%) 0.013413 02:22:48 08:41:25 left. \n",
      "epoch: 44 train_loss: 0.013604 Test Acc.：244528/255000(95.89%),252946/255000(99.19%) 0.013413 Save model 02:26:04 08:37:55 left. \n",
      "epoch: 45 train_loss: 0.013595 Test Acc.：244059/255000(95.71%),252761/255000(99.12%) 0.013402 Save model 02:29:23 08:34:35 left. \n",
      "epoch: 46 train_loss: 0.013613 Test Acc.：244451/255000(95.86%),252732/255000(99.11%) 0.013406 02:32:45 08:31:23 left. \n",
      "epoch: 47 train_loss: 0.013584 Test Acc.：244685/255000(95.95%),252891/255000(99.17%) 0.01333 Save model 02:36:06 08:28:09 left. \n",
      "epoch: 48 train_loss: 0.013588 Test Acc.：244209/255000(95.77%),252742/255000(99.11%) 0.013338 02:39:22 08:24:42 left. \n",
      "epoch: 49 train_loss: 0.013584 Test Acc.：244508/255000(95.89%),252932/255000(99.19%) 0.013427 Save model 02:42:42 08:21:23 left. \n",
      "epoch: 50 train_loss: 0.013581 Test Acc.：244421/255000(95.85%),252893/255000(99.17%) 0.013344 Save model 02:46:05 08:18:15 left. \n",
      "epoch: 51 train_loss: 0.013566 Test Acc.：244528/255000(95.89%),252778/255000(99.13%) 0.013399 Save model 02:49:21 08:14:46 left. \n",
      "epoch: 52 train_loss: 0.01357 Test Acc.：244646/255000(95.94%),252861/255000(99.16%) 0.013332 02:52:34 08:11:11 left. \n",
      "epoch: 53 train_loss: 0.013564 Test Acc.：244352/255000(95.82%),252901/255000(99.18%) 0.013437 Save model 02:55:52 08:07:48 left. \n",
      "epoch: 54 train_loss: 0.013577 Test Acc.：244182/255000(95.76%),252605/255000(99.06%) 0.013524 02:59:11 08:04:29 left. \n",
      "epoch: 55 train_loss: 0.013544 Test Acc.：244213/255000(95.77%),252911/255000(99.18%) 0.013348 Save model 03:02:30 08:01:09 left. \n",
      "epoch: 56 train_loss: 0.013546 Test Acc.：244077/255000(95.72%),252633/255000(99.07%) 0.013444 03:05:48 07:57:48 left. \n",
      "epoch: 57 train_loss: 0.013548 Test Acc.：244523/255000(95.89%),252951/255000(99.20%) 0.013354 03:09:05 07:54:24 left. \n",
      "epoch: 58 train_loss: 0.013539 Test Acc.：244527/255000(95.89%),252838/255000(99.15%) 0.013397 Save model 03:12:24 07:51:03 left. \n",
      "epoch: 59 train_loss: 0.013539 Test Acc.：244848/255000(96.02%),252862/255000(99.16%) 0.013343 03:15:45 07:47:48 left. \n",
      "epoch: 60 train_loss: 0.013537 Test Acc.：244521/255000(95.89%),252789/255000(99.13%) 0.013381 Save model 03:19:07 07:44:37 left. \n",
      "epoch: 61 train_loss: 0.013512 Test Acc.：244488/255000(95.88%),252583/255000(99.05%) 0.013331 Save model 03:22:27 07:41:20 left. \n",
      "epoch: 62 train_loss: 0.013524 Test Acc.：245041/255000(96.09%),252934/255000(99.19%) 0.013313 03:25:46 07:38:01 left. \n",
      "epoch: 63 train_loss: 0.013515 Test Acc.：244706/255000(95.96%),252858/255000(99.16%) 0.013323 03:29:06 07:34:43 left. \n",
      "epoch: 64 train_loss: 0.013499 Test Acc.：244700/255000(95.96%),252875/255000(99.17%) 0.013337 Save model 03:32:25 07:31:23 left. \n",
      "epoch: 65 train_loss: 0.013499 Test Acc.：244907/255000(96.04%),252885/255000(99.17%) 0.013276 03:35:38 07:27:52 left. \n",
      "epoch: 66 train_loss: 0.013502 Test Acc.：244670/255000(95.95%),252880/255000(99.17%) 0.013366 03:38:54 07:24:26 left. \n",
      "epoch: 67 train_loss: 0.013488 Test Acc.：244732/255000(95.97%),252696/255000(99.10%) 0.013354 Save model 03:42:10 07:21:02 left. \n",
      "epoch: 68 train_loss: 0.013489 Test Acc.：244769/255000(95.99%),252690/255000(99.09%) 0.01329 03:45:30 07:17:45 left. \n",
      "epoch: 69 train_loss: 0.013467 Test Acc.：244826/255000(96.01%),252933/255000(99.19%) 0.013286 Save model 03:48:46 07:14:19 left. \n",
      "epoch: 70 train_loss: 0.013476 Test Acc.：244770/255000(95.99%),252672/255000(99.09%) 0.013357 03:52:07 07:11:04 left. \n",
      "epoch: 71 train_loss: 0.013468 Test Acc.：244666/255000(95.95%),252178/255000(98.89%) 0.013317 03:55:22 07:07:38 left. \n",
      "epoch: 72 train_loss: 0.013461 Test Acc.：244818/255000(96.01%),252819/255000(99.14%) 0.013288 Save model 03:58:38 07:04:15 left. \n",
      "epoch: 73 train_loss: 0.013447 Test Acc.：244527/255000(95.89%),252935/255000(99.19%) 0.013368 Save model 04:01:55 07:00:53 left. \n",
      "epoch: 74 train_loss: 0.013446 Test Acc.：244592/255000(95.92%),252991/255000(99.21%) 0.01333 Save model 04:05:00 06:57:10 left. \n",
      "epoch: 75 train_loss: 0.013451 Test Acc.：245051/255000(96.10%),252689/255000(99.09%) 0.013248 04:08:14 06:53:44 left. \n",
      "epoch: 76 train_loss: 0.013436 Test Acc.：244600/255000(95.92%),252733/255000(99.11%) 0.013286 Save model 04:11:33 06:50:26 left. \n",
      "epoch: 77 train_loss: 0.013421 Test Acc.：245304/255000(96.20%),252931/255000(99.19%) 0.013237 Save model 04:14:50 06:47:05 left. \n",
      "epoch: 78 train_loss: 0.013419 Test Acc.：244873/255000(96.03%),252927/255000(99.19%) 0.013309 Save model 04:18:09 06:43:46 left. \n",
      "epoch: 79 train_loss: 0.013413 Test Acc.：242990/255000(95.29%),251534/255000(98.64%) 0.013734 Save model 04:21:26 06:40:26 left. \n",
      "epoch: 80 train_loss: 0.013417 Test Acc.：245098/255000(96.12%),252692/255000(99.09%) 0.013269 04:24:43 06:37:04 left. \n",
      "epoch: 81 train_loss: 0.013403 Test Acc.：245082/255000(96.11%),252853/255000(99.16%) 0.013265 Save model 04:28:02 06:33:47 left. \n",
      "epoch: 82 train_loss: 0.013391 Test Acc.：245093/255000(96.11%),252791/255000(99.13%) 0.013262 Save model 04:31:23 06:30:32 left. \n",
      "epoch: 83 train_loss: 0.013383 Test Acc.：245100/255000(96.12%),253020/255000(99.22%) 0.013253 Save model 04:34:40 06:27:11 left. \n",
      "epoch: 84 train_loss: 0.013379 Test Acc.：245342/255000(96.21%),253044/255000(99.23%) 0.013231 Save model 04:37:59 06:23:54 left. \n",
      "epoch: 85 train_loss: 0.013395 Test Acc.：245085/255000(96.11%),252803/255000(99.14%) 0.013249 04:41:16 06:20:33 left. \n",
      "epoch: 86 train_loss: 0.013372 Test Acc.：245332/255000(96.21%),252807/255000(99.14%) 0.013243 Save model 04:44:36 06:17:15 left. \n",
      "epoch: 87 train_loss: 0.013349 Test Acc.：245402/255000(96.24%),252815/255000(99.14%) 0.013226 Save model 04:47:59 06:14:03 left. \n",
      "epoch: 88 train_loss: 0.013358 Test Acc.：245347/255000(96.21%),252812/255000(99.14%) 0.013151 04:51:22 06:10:50 left. \n",
      "epoch: 89 train_loss: 0.013347 Test Acc.：245275/255000(96.19%),252915/255000(99.18%) 0.0132 Save model 04:54:40 06:07:30 left. \n",
      "epoch: 90 train_loss: 0.013346 Test Acc.：245209/255000(96.16%),252937/255000(99.19%) 0.013189 Save model 04:57:58 06:04:11 left. \n",
      "epoch: 91 train_loss: 0.013325 Test Acc.：245298/255000(96.20%),252726/255000(99.11%) 0.013171 Save model 05:01:17 06:00:53 left. \n",
      "epoch: 92 train_loss: 0.013321 Test Acc.：245222/255000(96.17%),252918/255000(99.18%) 0.013179 Save model 05:04:40 05:57:39 left. \n",
      "epoch: 93 train_loss: 0.013312 Test Acc.：245557/255000(96.30%),252994/255000(99.21%) 0.01315 Save model 05:08:03 05:54:25 left. \n",
      "epoch: 94 train_loss: 0.013314 Test Acc.：245437/255000(96.25%),252796/255000(99.14%) 0.013185 05:11:24 05:51:09 left. \n",
      "epoch: 95 train_loss: 0.013298 Test Acc.：245532/255000(96.29%),253017/255000(99.22%) 0.013165 Save model 05:14:44 05:47:52 left. \n",
      "epoch: 96 train_loss: 0.013304 Test Acc.：245466/255000(96.26%),252897/255000(99.18%) 0.013162 05:18:01 05:44:31 left. \n",
      "epoch: 97 train_loss: 0.013287 Test Acc.：245487/255000(96.27%),253119/255000(99.26%) 0.013169 Save model 05:21:21 05:41:13 left. \n",
      "epoch: 98 train_loss: 0.013282 Test Acc.：245248/255000(96.18%),252618/255000(99.07%) 0.013235 Save model 05:24:39 05:37:54 left. \n",
      "epoch: 99 train_loss: 0.013272 Test Acc.：245259/255000(96.18%),252814/255000(99.14%) 0.013204 Save model 05:28:00 05:34:37 left. \n",
      "epoch: 100 train_loss: 0.013275 Test Acc.：245449/255000(96.25%),253109/255000(99.26%) 0.013184 05:31:18 05:31:18 left. \n",
      "epoch: 101 train_loss: 0.013256 Test Acc.：245524/255000(96.28%),252603/255000(99.06%) 0.013242 Save model 05:34:34 05:27:57 left. \n",
      "epoch: 102 train_loss: 0.013249 Test Acc.：245756/255000(96.37%),252978/255000(99.21%) 0.013097 Save model 05:37:53 05:24:38 left. \n",
      "epoch: 103 train_loss: 0.013246 Test Acc.：245651/255000(96.33%),252885/255000(99.17%) 0.013179 Save model 05:41:14 05:21:21 left. \n",
      "epoch: 104 train_loss: 0.013235 Test Acc.：245886/255000(96.43%),253032/255000(99.23%) 0.013082 Save model 05:44:31 05:18:01 left. \n",
      "epoch: 105 train_loss: 0.013232 Test Acc.：245920/255000(96.44%),253063/255000(99.24%) 0.013091 Save model 05:47:47 05:14:40 left. \n",
      "epoch: 106 train_loss: 0.013218 Test Acc.：245600/255000(96.31%),253082/255000(99.25%) 0.01315 Save model 05:51:02 05:11:18 left. \n",
      "epoch: 107 train_loss: 0.013214 Test Acc.：245435/255000(96.25%),253024/255000(99.23%) 0.013133 Save model 05:54:21 05:07:59 left. \n",
      "epoch: 108 train_loss: 0.013205 Test Acc.：245936/255000(96.45%),253126/255000(99.27%) 0.013089 Save model 05:57:42 05:04:42 left. \n",
      "epoch: 109 train_loss: 0.013207 Test Acc.：245998/255000(96.47%),253031/255000(99.23%) 0.013076 06:01:04 05:01:26 left. \n",
      "epoch: 110 train_loss: 0.013195 Test Acc.：246029/255000(96.48%),253078/255000(99.25%) 0.013059 Save model 06:04:21 04:58:06 left. \n",
      "epoch: 111 train_loss: 0.013185 Test Acc.：246193/255000(96.55%),253046/255000(99.23%) 0.013041 Save model 06:07:38 04:54:46 left. \n",
      "epoch: 112 train_loss: 0.013171 Test Acc.：245984/255000(96.46%),253128/255000(99.27%) 0.013065 Save model 06:10:58 04:51:28 left. \n",
      "epoch: 113 train_loss: 0.013166 Test Acc.：246274/255000(96.58%),253133/255000(99.27%) 0.01309 Save model 06:14:18 04:48:11 left. \n",
      "epoch: 114 train_loss: 0.013161 Test Acc.：246029/255000(96.48%),253111/255000(99.26%) 0.013057 Save model 06:17:37 04:44:52 left. \n",
      "epoch: 115 train_loss: 0.013157 Test Acc.：246266/255000(96.57%),253112/255000(99.26%) 0.013039 Save model 06:20:56 04:41:34 left. \n",
      "epoch: 116 train_loss: 0.013145 Test Acc.：246165/255000(96.54%),253069/255000(99.24%) 0.013015 Save model 06:24:16 04:38:15 left. \n",
      "epoch: 117 train_loss: 0.01314 Test Acc.：245927/255000(96.44%),252920/255000(99.18%) 0.013045 Save model 06:27:30 04:34:54 left. \n",
      "epoch: 118 train_loss: 0.013131 Test Acc.：246245/255000(96.57%),253049/255000(99.23%) 0.013028 Save model 06:30:44 04:31:32 left. \n",
      "epoch: 119 train_loss: 0.013125 Test Acc.：246072/255000(96.50%),253052/255000(99.24%) 0.013024 Save model 06:34:01 04:28:12 left. \n",
      "epoch: 120 train_loss: 0.013105 Test Acc.：246024/255000(96.48%),252976/255000(99.21%) 0.013088 Save model 06:37:20 04:24:53 left. \n",
      "epoch: 121 train_loss: 0.013112 Test Acc.：246441/255000(96.64%),253106/255000(99.26%) 0.012974 06:40:40 04:21:35 left. \n",
      "epoch: 122 train_loss: 0.013096 Test Acc.：246391/255000(96.62%),253168/255000(99.28%) 0.013002 Save model 06:43:58 04:18:16 left. \n",
      "epoch: 123 train_loss: 0.013085 Test Acc.：246431/255000(96.64%),252941/255000(99.19%) 0.012971 Save model 06:47:19 04:14:59 left. \n",
      "epoch: 124 train_loss: 0.013082 Test Acc.：246513/255000(96.67%),253100/255000(99.25%) 0.012999 Save model 06:50:38 04:11:41 left. \n",
      "epoch: 125 train_loss: 0.013067 Test Acc.：246355/255000(96.61%),253117/255000(99.26%) 0.01298 Save model 06:54:01 04:08:24 left. \n",
      "epoch: 126 train_loss: 0.013064 Test Acc.：246510/255000(96.67%),252951/255000(99.20%) 0.012966 Save model 06:57:20 04:05:06 left. \n",
      "epoch: 127 train_loss: 0.013056 Test Acc.：246544/255000(96.68%),253165/255000(99.28%) 0.012966 Save model 07:00:40 04:01:48 left. \n",
      "epoch: 128 train_loss: 0.013051 Test Acc.：246609/255000(96.71%),253066/255000(99.24%) 0.012939 Save model 07:04:00 03:58:30 left. \n",
      "epoch: 129 train_loss: 0.013035 Test Acc.：246520/255000(96.67%),253136/255000(99.27%) 0.012965 Save model 07:07:19 03:55:11 left. \n",
      "epoch: 130 train_loss: 0.013032 Test Acc.：246348/255000(96.61%),253164/255000(99.28%) 0.01298 Save model 07:10:38 03:51:52 left. \n",
      "epoch: 131 train_loss: 0.013022 Test Acc.：246610/255000(96.71%),253186/255000(99.29%) 0.012955 Save model 07:13:55 03:48:33 left. \n",
      "epoch: 132 train_loss: 0.013015 Test Acc.：246735/255000(96.76%),253129/255000(99.27%) 0.012913 Save model 07:17:10 03:45:12 left. \n",
      "epoch: 133 train_loss: 0.013005 Test Acc.：246186/255000(96.54%),253093/255000(99.25%) 0.013004 Save model 07:20:34 03:41:56 left. \n",
      "epoch: 134 train_loss: 0.012999 Test Acc.：246847/255000(96.80%),253097/255000(99.25%) 0.012894 Save model 07:23:54 03:38:38 left. \n",
      "epoch: 135 train_loss: 0.012988 Test Acc.：246731/255000(96.76%),253120/255000(99.26%) 0.0129 Save model 07:27:16 03:35:21 left. \n",
      "epoch: 136 train_loss: 0.012987 Test Acc.：246814/255000(96.79%),253131/255000(99.27%) 0.012896 Save model 07:30:35 03:32:02 left. \n",
      "epoch: 137 train_loss: 0.012971 Test Acc.：246790/255000(96.78%),253160/255000(99.28%) 0.012913 Save model 07:33:50 03:28:41 left. \n",
      "epoch: 138 train_loss: 0.012972 Test Acc.：246863/255000(96.81%),253287/255000(99.33%) 0.012898 07:37:11 03:25:24 left. \n",
      "epoch: 139 train_loss: 0.012956 Test Acc.：246870/255000(96.81%),253165/255000(99.28%) 0.01291 Save model 07:40:28 03:22:04 left. \n",
      "epoch: 140 train_loss: 0.012948 Test Acc.：246737/255000(96.76%),253188/255000(99.29%) 0.012916 Save model 07:43:43 03:18:44 left. \n",
      "epoch: 141 train_loss: 0.012944 Test Acc.：246888/255000(96.82%),253222/255000(99.30%) 0.012887 Save model 07:47:01 03:15:25 left. \n",
      "epoch: 142 train_loss: 0.012931 Test Acc.：246767/255000(96.77%),252879/255000(99.17%) 0.012953 Save model 07:50:16 03:12:04 left. \n",
      "epoch: 143 train_loss: 0.012923 Test Acc.：246904/255000(96.83%),253147/255000(99.27%) 0.012895 Save model 07:53:35 03:08:46 left. \n",
      "epoch: 144 train_loss: 0.012907 Test Acc.：246969/255000(96.85%),253213/255000(99.30%) 0.012865 Save model 07:56:52 03:05:26 left. \n",
      "epoch: 145 train_loss: 0.012901 Test Acc.：247029/255000(96.87%),253152/255000(99.28%) 0.012847 Save model 08:00:10 03:02:08 left. \n",
      "epoch: 146 train_loss: 0.012895 Test Acc.：247061/255000(96.89%),253263/255000(99.32%) 0.012849 Save model 08:03:29 02:58:49 left. \n",
      "epoch: 147 train_loss: 0.012892 Test Acc.：247046/255000(96.88%),253211/255000(99.30%) 0.012861 Save model 08:06:48 02:55:30 left. \n",
      "epoch: 148 train_loss: 0.012878 Test Acc.：247115/255000(96.91%),253225/255000(99.30%) 0.012854 Save model 08:10:06 02:52:11 left. \n",
      "epoch: 149 train_loss: 0.012873 Test Acc.：247249/255000(96.96%),253172/255000(99.28%) 0.012817 Save model 08:13:23 02:48:52 left. \n",
      "epoch: 150 train_loss: 0.012862 Test Acc.：247123/255000(96.91%),253145/255000(99.27%) 0.012839 Save model 08:16:43 02:45:34 left. \n",
      "epoch: 151 train_loss: 0.01286 Test Acc.：247216/255000(96.95%),253226/255000(99.30%) 0.012828 Save model 08:19:59 02:42:14 left. \n",
      "epoch: 152 train_loss: 0.012849 Test Acc.：247260/255000(96.96%),253208/255000(99.30%) 0.012811 Save model 08:23:17 02:38:56 left. \n",
      "epoch: 153 train_loss: 0.012836 Test Acc.：247312/255000(96.99%),253249/255000(99.31%) 0.012812 Save model 08:26:40 02:35:38 left. \n",
      "epoch: 154 train_loss: 0.012829 Test Acc.：247254/255000(96.96%),253211/255000(99.30%) 0.012821 Save model 08:30:01 02:32:20 left. \n",
      "epoch: 155 train_loss: 0.012819 Test Acc.：247397/255000(97.02%),253205/255000(99.30%) 0.012781 Save model 08:33:24 02:29:03 left. \n",
      "epoch: 156 train_loss: 0.012811 Test Acc.：247373/255000(97.01%),253161/255000(99.28%) 0.012775 Save model 08:36:44 02:25:44 left. \n",
      "epoch: 157 train_loss: 0.012803 Test Acc.：247376/255000(97.01%),253199/255000(99.29%) 0.012797 Save model 08:40:13 02:22:28 left. \n",
      "epoch: 158 train_loss: 0.012795 Test Acc.：247493/255000(97.06%),253187/255000(99.29%) 0.012785 Save model 08:43:35 02:19:11 left. \n",
      "epoch: 159 train_loss: 0.012783 Test Acc.：247451/255000(97.04%),253256/255000(99.32%) 0.01279 Save model 08:46:50 02:15:51 left. \n",
      "epoch: 160 train_loss: 0.012776 Test Acc.：247549/255000(97.08%),253198/255000(99.29%) 0.012767 Save model 08:50:08 02:12:32 left. \n",
      "epoch: 161 train_loss: 0.012766 Test Acc.：247599/255000(97.10%),253295/255000(99.33%) 0.012751 Save model 08:53:32 02:09:14 left. \n",
      "epoch: 162 train_loss: 0.01276 Test Acc.：247656/255000(97.12%),253231/255000(99.31%) 0.012744 Save model 08:56:49 02:05:55 left. \n",
      "epoch: 163 train_loss: 0.012754 Test Acc.：247661/255000(97.12%),253217/255000(99.30%) 0.012756 Save model 09:00:09 02:02:36 left. \n",
      "epoch: 164 train_loss: 0.012741 Test Acc.：247661/255000(97.12%),253304/255000(99.33%) 0.012727 Save model 09:03:36 01:59:19 left. \n",
      "epoch: 165 train_loss: 0.012737 Test Acc.：247786/255000(97.17%),253240/255000(99.31%) 0.012719 Save model 09:06:57 01:56:01 left. \n",
      "epoch: 166 train_loss: 0.012725 Test Acc.：247790/255000(97.17%),253241/255000(99.31%) 0.012731 Save model 09:10:17 01:52:42 left. \n",
      "epoch: 167 train_loss: 0.012718 Test Acc.：247633/255000(97.11%),253290/255000(99.33%) 0.012717 Save model 09:13:35 01:49:23 left. \n",
      "epoch: 168 train_loss: 0.012708 Test Acc.：247764/255000(97.16%),253279/255000(99.33%) 0.01272 Save model 09:16:56 01:46:04 left. \n",
      "epoch: 169 train_loss: 0.012702 Test Acc.：247928/255000(97.23%),253288/255000(99.33%) 0.012719 Save model 09:20:19 01:42:46 left. \n",
      "epoch: 170 train_loss: 0.012698 Test Acc.：247813/255000(97.18%),253320/255000(99.34%) 0.012708 Save model 09:23:40 01:39:28 left. \n",
      "epoch: 171 train_loss: 0.012686 Test Acc.：247836/255000(97.19%),253308/255000(99.34%) 0.012708 Save model 09:27:00 01:36:09 left. \n",
      "epoch: 172 train_loss: 0.012677 Test Acc.：247900/255000(97.22%),253309/255000(99.34%) 0.012702 Save model 09:30:19 01:32:50 left. \n",
      "epoch: 173 train_loss: 0.012668 Test Acc.：247862/255000(97.20%),253281/255000(99.33%) 0.012683 Save model 09:33:40 01:29:31 left. \n",
      "epoch: 174 train_loss: 0.012666 Test Acc.：247931/255000(97.23%),253224/255000(99.30%) 0.012691 Save model 09:36:58 01:26:12 left. \n",
      "epoch: 175 train_loss: 0.012652 Test Acc.：247907/255000(97.22%),253284/255000(99.33%) 0.012689 Save model 09:40:23 01:22:54 left. \n",
      "epoch: 176 train_loss: 0.012647 Test Acc.：248005/255000(97.26%),253281/255000(99.33%) 0.012683 Save model 09:43:39 01:19:35 left. \n",
      "epoch: 177 train_loss: 0.012643 Test Acc.：247936/255000(97.23%),253244/255000(99.31%) 0.012663 Save model 09:46:58 01:16:16 left. \n",
      "epoch: 178 train_loss: 0.012635 Test Acc.：248046/255000(97.27%),253311/255000(99.34%) 0.012655 Save model 09:50:21 01:12:57 left. \n",
      "epoch: 179 train_loss: 0.012629 Test Acc.：248069/255000(97.28%),253333/255000(99.35%) 0.012654 Save model 09:53:47 01:09:39 left. \n",
      "epoch: 180 train_loss: 0.01262 Test Acc.：248067/255000(97.28%),253290/255000(99.33%) 0.012653 Save model 09:57:08 01:06:20 left. \n",
      "epoch: 181 train_loss: 0.012613 Test Acc.：248080/255000(97.29%),253283/255000(99.33%) 0.01265 Save model 10:00:33 01:03:02 left. \n",
      "epoch: 182 train_loss: 0.012607 Test Acc.：248132/255000(97.31%),253314/255000(99.34%) 0.012657 Save model 10:03:52 00:59:43 left. \n",
      "epoch: 183 train_loss: 0.012598 Test Acc.：248105/255000(97.30%),253270/255000(99.32%) 0.012638 Save model 10:07:15 00:56:24 left. \n",
      "epoch: 184 train_loss: 0.012597 Test Acc.：248228/255000(97.34%),253269/255000(99.32%) 0.012635 Save model 10:10:34 00:53:05 left. \n",
      "epoch: 185 train_loss: 0.012589 Test Acc.：248158/255000(97.32%),253301/255000(99.33%) 0.012657 Save model 10:13:58 00:49:46 left. \n",
      "epoch: 186 train_loss: 0.012583 Test Acc.：248138/255000(97.31%),253294/255000(99.33%) 0.012632 Save model 10:17:14 00:46:27 left. \n",
      "epoch: 187 train_loss: 0.012576 Test Acc.：248198/255000(97.33%),253269/255000(99.32%) 0.01263 Save model 10:20:32 00:43:08 left. \n",
      "epoch: 188 train_loss: 0.012571 Test Acc.：248169/255000(97.32%),253310/255000(99.34%) 0.012627 Save model 10:23:55 00:39:49 left. \n",
      "epoch: 189 train_loss: 0.012568 Test Acc.：248227/255000(97.34%),253326/255000(99.34%) 0.012641 Save model 10:27:13 00:36:30 left. \n",
      "epoch: 190 train_loss: 0.012564 Test Acc.：248263/255000(97.36%),253281/255000(99.33%) 0.012625 Save model 10:30:34 00:33:11 left. \n",
      "epoch: 191 train_loss: 0.012559 Test Acc.：248201/255000(97.33%),253299/255000(99.33%) 0.012634 Save model 10:33:58 00:29:52 left. \n",
      "epoch: 192 train_loss: 0.012557 Test Acc.：248232/255000(97.35%),253292/255000(99.33%) 0.012618 Save model 10:37:24 00:26:33 left. \n",
      "epoch: 193 train_loss: 0.012555 Test Acc.：248222/255000(97.34%),253289/255000(99.33%) 0.012621 Save model 10:40:46 00:23:14 left. \n",
      "epoch: 194 train_loss: 0.012553 Test Acc.：248247/255000(97.35%),253288/255000(99.33%) 0.012625 Save model 10:44:03 00:19:55 left. \n",
      "epoch: 195 train_loss: 0.012552 Test Acc.：248225/255000(97.34%),253278/255000(99.32%) 0.012627 Save model 10:47:21 00:16:35 left. \n",
      "epoch: 196 train_loss: 0.012549 Test Acc.：248245/255000(97.35%),253298/255000(99.33%) 0.012614 Save model 10:50:33 00:13:16 left. \n",
      "epoch: 197 train_loss: 0.012545 Test Acc.：248236/255000(97.35%),253288/255000(99.33%) 0.012615 Save model 10:53:52 00:09:57 left. \n",
      "epoch: 198 train_loss: 0.012546 Test Acc.：248281/255000(97.37%),253283/255000(99.33%) 0.012617 10:57:13 00:06:38 left. \n",
      "epoch: 199 train_loss: 0.012545 Test Acc.：248269/255000(97.36%),253287/255000(99.33%) 0.012627 Save model 11:00:30 00:03:19 left. \n",
      "epoch: 200 train_loss: 0.012544 Test Acc.：248259/255000(97.36%),253284/255000(99.33%) 0.012611 Save model 11:03:51 00:00:00 left. \n",
      "Test Acc.：175665/255000(68.89%),59909/255000(23.49%) 0.029299 \n",
      "01_1_VC2003_32bit_none 01_2_VC2017_32bit_none 02_1_VC2003_32bit_max 02_2_VC2017_32bit_max 03_1_gcc6.3.0_x86_none 03_2_x86-O0-gcc7.5.0 04_1_gcc6.3.0_x86_O3 04_2_x86-O3-gcc7.5.0 05_1_clang5.0.2_32_none 05_2_x86-O0-Clang10.0.0 06_1_clang5.0.2_32_O3 06_2_x86-O3-Clang10.0.0 07_intel_32_none 08_intel_32bit_max 11_VC2017_64bit_none 12_VC2017_64bit_max 13_1_gcc6.3.0_64bit_none 13_2_x86_64-O0-gcc7.5.0 14_1_gcc6.3.0_64bit_max 14_2_x86_64-O3-gcc7.5.0 15_1_clang5.0.2_64bit_none 15_2_x86_64-O0-Clang10.0.0 16_1_clang5.0.2_64bit_max 16_2_x86_64-O3-Clang10.0.0 17_intel_64_none 18_intel_64bit_max 21_arm-O0-gcc 22_arm-O3-gcc 23_arm-O0-Clang 24_arm-O3-Clang 31_arm64-O0-gcc 32_arm64-O3-gcc 33_arm64-O0-Clang 34_arm64-O3-Clang 41_mips-O0-gcc 42_mips-O3-gcc 43_mips-O0-Clang 44_mips-O3-Clang 51_mips64-O0-gcc 52_mips64-O3-gcc 53_mips64-O0-Clang 54_mips64-O3-Clang 61_powerpc-O0-gcc 62_powerpc-O3-gcc 63_powerpc-O0-Clang 64_powerpc-O3-Clang 71_powerpc64-O0-gcc 72_powerpc64-O3-gcc 73_powerpc64-O0-Clang 74_powerpc64-O3-Clang 80_document \n",
      "01_1_VC2003_32bit_none 4795 196 2 1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 \n",
      "01_2_VC2017_32bit_none 239 4752 1 3 0 0 0 0 0 0 0 0 0 0 0 2 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "02_1_VC2003_32bit_max 11 1 4937 32 0 0 2 0 0 0 3 1 0 8 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 2 \n",
      "02_2_VC2017_32bit_max 5 3 32 4928 0 1 5 1 0 0 6 1 0 15 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 \n",
      "03_1_gcc6.3.0_x86_none 0 0 1 0 4859 132 6 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "03_2_x86-O0-gcc7.5.0 2 0 0 0 166 4820 0 5 0 2 2 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "04_1_gcc6.3.0_x86_O3 0 0 6 3 3 0 4686 233 0 0 34 27 0 3 0 0 0 0 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "04_2_x86-O3-gcc7.5.0 0 0 1 3 2 3 326 4617 0 0 18 25 0 0 0 0 0 0 2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "05_1_clang5.0.2_32_none 0 1 0 0 1 0 1 1 4733 250 4 0 0 1 0 0 0 0 0 0 7 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "05_2_x86-O0-Clang10.0.0 0 0 0 0 1 0 0 1 309 4685 0 1 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "06_1_clang5.0.2_32_O3 0 1 2 10 0 0 36 16 1 0 4732 182 0 11 0 0 0 0 0 1 1 0 6 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "06_2_x86-O3-Clang10.0.0 0 0 4 0 0 0 24 17 0 3 218 4730 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 \n",
      "07_intel_32_none 1 0 1 0 1 2 0 0 0 1 0 1 4992 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "08_intel_32bit_max 2 0 6 16 0 0 4 1 1 0 9 3 1 4952 0 1 0 0 1 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "11_VC2017_64bit_none 0 1 0 1 0 0 0 0 0 0 0 1 0 1 4988 0 0 2 0 2 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 \n",
      "12_VC2017_64bit_max 0 0 0 0 0 0 0 0 0 0 3 1 0 0 6 4960 0 1 4 5 0 0 3 0 1 15 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 \n",
      "13_1_gcc6.3.0_64bit_none 0 0 0 0 3 1 0 0 0 0 0 0 0 0 0 0 4437 551 2 0 4 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "13_2_x86_64-O0-gcc7.5.0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 652 4335 3 2 0 1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 \n",
      "14_1_gcc6.3.0_64bit_max 0 0 0 0 0 0 6 2 0 0 1 1 0 0 0 1 1 1 4668 261 0 1 32 24 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "14_2_x86_64-O3-gcc7.5.0 0 0 0 0 0 0 4 2 0 0 4 0 0 2 0 0 0 0 481 4460 1 0 20 24 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "15_1_clang5.0.2_64bit_none 0 0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 1 0 3 0 4736 252 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "15_2_x86_64-O0-Clang10.0.0 0 0 0 0 0 0 1 0 2 2 0 0 0 0 0 0 1 1 1 1 282 4708 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "16_1_clang5.0.2_64bit_max 0 0 0 1 0 0 0 0 0 0 17 2 0 0 4 2 3 1 63 31 3 2 4666 204 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "16_2_x86_64-O3-Clang10.0.0 0 0 1 0 0 0 1 0 0 0 6 2 0 0 0 2 1 1 55 50 1 0 238 4641 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 \n",
      "17_intel_64_none 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5000 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "18_intel_64bit_max 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 2 4 0 0 3 1 1 4985 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "21_arm-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4988 5 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 \n",
      "22_arm-O3-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 3 4984 0 0 0 1 0 3 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 4 \n",
      "23_arm-O0-Clang 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 4988 6 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 \n",
      "24_arm-O3-Clang 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 2 13 4977 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 \n",
      "31_arm64-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4999 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "32_arm64-O3-gcc 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 4967 0 27 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "33_arm64-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 4997 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "34_arm64-O3-Clang 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 17 3 4975 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 \n",
      "41_mips-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4992 5 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "42_mips-O3-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 4984 0 6 0 8 0 0 0 0 0 0 0 0 0 0 1 \n",
      "43_mips-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 2 4993 3 0 0 0 1 0 0 0 0 0 0 0 0 0 \n",
      "44_mips-O3-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 22 2 4961 0 2 0 13 0 0 0 0 0 0 0 0 0 \n",
      "51_mips64-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 11 2 0 0 4985 2 0 0 0 0 0 0 0 0 0 0 0 \n",
      "52_mips64-O3-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 25 0 7 1 4956 0 10 0 0 0 0 0 0 0 0 0 \n",
      "53_mips64-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1 1 4995 1 0 0 0 0 0 0 0 0 0 \n",
      "54_mips64-O3-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 0 43 0 22 0 4928 0 1 0 0 0 0 0 0 0 \n",
      "61_powerpc-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4998 2 0 0 0 0 0 0 0 \n",
      "62_powerpc-O3-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 4979 0 14 0 3 0 2 0 \n",
      "63_powerpc-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4998 1 0 0 1 0 0 \n",
      "64_powerpc-O3-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 19 3 4963 0 2 0 13 0 \n",
      "71_powerpc64-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 1 0 0 4990 5 0 0 0 \n",
      "72_powerpc64-O3-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 22 0 1 2 4963 0 9 0 \n",
      "73_powerpc64-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 10 0 0 5 4984 1 0 \n",
      "74_powerpc64-O3-Clang 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 11 0 55 0 8 1 4923 0 \n",
      "80_document 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 3 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 4990 \n",
      "no label Num TP FP FN TN R P F1 Acc.\n",
      "0 01_1_VC2003_32bit_none 5000 4795 205 262 4795 0.9481906268538659 0.959 0.953564681316496 0.953564681316496\n",
      "1 01_2_VC2017_32bit_none 5000 4752 248 204 4752 0.9588377723970944 0.9504 0.954600241060667 0.954600241060667\n",
      "2 02_1_VC2003_32bit_max 5000 4937 63 57 4937 0.9885863035642771 0.9874 0.9879927956774065 0.9879927956774065\n",
      "3 02_2_VC2017_32bit_max 5000 4928 72 72 4928 0.9856 0.9856 0.9856 0.9856\n",
      "4 03_1_gcc6.3.0_x86_none 5000 4859 141 178 4859 0.9646615048640064 0.9718 0.9682175948988742 0.9682175948988742\n",
      "5 03_2_x86-O0-gcc7.5.0 5000 4820 180 140 4820 0.9717741935483871 0.964 0.9678714859437753 0.9678714859437751\n",
      "6 04_1_gcc6.3.0_x86_O3 5000 4686 314 418 4686 0.9181034482758621 0.9372 0.9275534441805225 0.9275534441805225\n",
      "7 04_2_x86-O3-gcc7.5.0 5000 4617 383 281 4617 0.9426296447529604 0.9234 0.9329157405536472 0.9329157405536472\n",
      "8 05_1_clang5.0.2_32_none 5000 4733 267 321 4733 0.9364859517214088 0.9466 0.9415158146011539 0.9415158146011537\n",
      "9 05_2_x86-O0-Clang10.0.0 5000 4685 315 260 4685 0.9474216380182002 0.937 0.9421820010055305 0.9421820010055304\n",
      "10 06_1_clang5.0.2_32_O3 5000 4732 268 326 4732 0.9355476472914195 0.9464 0.9409425333068204 0.9409425333068204\n",
      "11 06_2_x86-O3-Clang10.0.0 5000 4730 270 248 4730 0.9501807955002008 0.946 0.9480857887352175 0.9480857887352174\n",
      "12 07_intel_32_none 5000 4992 8 1 4992 0.9997997196074504 0.9984 0.999099369558691 0.999099369558691\n",
      "13 08_intel_32bit_max 5000 4952 48 47 4952 0.9905981196239247 0.9904 0.9904990499049905 0.9904990499049905\n",
      "14 11_VC2017_64bit_none 5000 4988 12 11 4988 0.9977995599119824 0.9976 0.9976997699769977 0.9976997699769977\n",
      "15 12_VC2017_64bit_max 5000 4960 40 11 4960 0.9977871655602495 0.992 0.9948851669842544 0.9948851669842543\n",
      "16 13_1_gcc6.3.0_64bit_none 5000 4437 563 663 4437 0.87 0.8874 0.8786138613861386 0.8786138613861386\n",
      "17 13_2_x86_64-O0-gcc7.5.0 5000 4335 665 563 4335 0.8850551245406288 0.867 0.8759345322287332 0.8759345322287331\n",
      "18 14_1_gcc6.3.0_64bit_max 5000 4668 332 623 4668 0.8822528822528822 0.9336 0.9072004664269749 0.907200466426975\n",
      "19 14_2_x86_64-O3-gcc7.5.0 5000 4460 540 362 4460 0.9249274160099544 0.892 0.9081653431073101 0.9081653431073101\n",
      "20 15_1_clang5.0.2_64bit_none 5000 4736 264 305 4736 0.9394961317198969 0.9472 0.9433323374165921 0.943332337416592\n",
      "21 15_2_x86_64-O0-Clang10.0.0 5000 4708 292 262 4708 0.9472837022132797 0.9416 0.944433299899699 0.9444332998996992\n",
      "22 16_1_clang5.0.2_64bit_max 5000 4666 334 312 4666 0.937324226597027 0.9332 0.9352575666466225 0.9352575666466225\n",
      "23 16_2_x86_64-O3-Clang10.0.0 5000 4641 359 255 4641 0.9479166666666666 0.9282 0.9379547291835085 0.9379547291835085\n",
      "24 17_intel_64_none 5000 5000 0 2 5000 0.9996001599360256 1.0 0.9998000399920016 0.9998000399920016\n",
      "25 18_intel_64bit_max 5000 4985 15 20 4985 0.996003996003996 0.997 0.9965017491254373 0.9965017491254373\n",
      "26 21_arm-O0-gcc 5000 4988 12 7 4988 0.9985985985985986 0.9976 0.9980990495247624 0.9980990495247624\n",
      "27 22_arm-O3-gcc 5000 4984 16 10 4984 0.9979975971165399 0.9968 0.9973984390634382 0.9973984390634381\n",
      "28 23_arm-O0-Clang 5000 4988 12 13 4988 0.9974005198960207 0.9976 0.9975002499750025 0.9975002499750025\n",
      "29 24_arm-O3-Clang 5000 4977 23 9 4977 0.9981949458483754 0.9954 0.9967955137192068 0.9967955137192069\n",
      "30 31_arm64-O0-gcc 5000 4999 1 8 4999 0.9984022368683843 0.9998 0.9991006295593086 0.9991006295593085\n",
      "31 32_arm64-O3-gcc 5000 4967 33 18 4967 0.9963891675025075 0.9934 0.9948923385077616 0.9948923385077616\n",
      "32 33_arm64-O0-Clang 5000 4997 3 3 4997 0.9994 0.9994 0.9994 0.9994\n",
      "33 34_arm64-O3-Clang 5000 4975 25 35 4975 0.9930139720558883 0.995 0.994005994005994 0.994005994005994\n",
      "34 41_mips-O0-gcc 5000 4992 8 12 4992 0.9976019184652278 0.9984 0.9980007996801279 0.998000799680128\n",
      "35 42_mips-O3-gcc 5000 4984 16 64 4984 0.9873217115689382 0.9968 0.9920382165605095 0.9920382165605095\n",
      "36 43_mips-O0-Clang 5000 4993 7 6 4993 0.9987997599519904 0.9986 0.9986998699869988 0.9986998699869987\n",
      "37 44_mips-O3-Clang 5000 4961 39 62 4961 0.9876567788174397 0.9922 0.9899231766936046 0.9899231766936047\n",
      "38 51_mips64-O0-gcc 5000 4985 15 6 4985 0.998797836104989 0.997 0.9978981082974678 0.9978981082974677\n",
      "39 52_mips64-O3-gcc 5000 4956 44 35 4956 0.9929873772791024 0.9912 0.9920928835952357 0.9920928835952357\n",
      "40 53_mips64-O0-Clang 5000 4995 5 1 4995 0.9997998398718975 0.999 0.9993997599039617 0.9993997599039616\n",
      "41 54_mips64-O3-Clang 5000 4928 72 26 4928 0.994751715785224 0.9856 0.990154711673699 0.9901547116736991\n",
      "42 61_powerpc-O0-gcc 5000 4998 2 6 4998 0.9988009592326139 0.9996 0.9992003198720513 0.9992003198720512\n",
      "43 62_powerpc-O3-gcc 5000 4979 21 56 4979 0.9888778550148958 0.9958 0.9923268560039861 0.9923268560039861\n",
      "44 63_powerpc-O0-Clang 5000 4998 2 13 4998 0.997405707443624 0.9996 0.9985016481869943 0.9985016481869943\n",
      "45 64_powerpc-O3-Clang 5000 4963 37 73 4963 0.9855043685464655 0.9926 0.9890394579513752 0.989039457951375\n",
      "46 71_powerpc64-O0-gcc 5000 4990 10 3 4990 0.9993991588223513 0.998 0.9986990893625537 0.9986990893625538\n",
      "47 72_powerpc64-O3-gcc 5000 4963 37 25 4963 0.9949879711307137 0.9926 0.9937925510612736 0.9937925510612735\n",
      "48 73_powerpc64-O0-Clang 5000 4984 16 2 4984 0.9995988768551946 0.9968 0.9981974764670539 0.9981974764670539\n",
      "49 74_powerpc64-O3-Clang 5000 4923 77 29 4923 0.9941437802907916 0.9846 0.9893488745980707 0.9893488745980707\n",
      "50 80_document 5000 4990 10 15 4990 0.997002997002997 0.998 0.9975012493753124 0.9975012493753124\n",
      "  255000 248259 6741 6741 248259 0.9735647058823529 0.9735647058823529 0.9735647058823529 0.9735647058823529\n",
      "All results\n",
      "01_1_VC2003_32bit_none 01_2_VC2017_32bit_none 02_1_VC2003_32bit_max 02_2_VC2017_32bit_max 03_1_gcc6.3.0_x86_none 03_2_x86-O0-gcc7.5.0 04_1_gcc6.3.0_x86_O3 04_2_x86-O3-gcc7.5.0 05_1_clang5.0.2_32_none 05_2_x86-O0-Clang10.0.0 06_1_clang5.0.2_32_O3 06_2_x86-O3-Clang10.0.0 07_intel_32_none 08_intel_32bit_max 11_VC2017_64bit_none 12_VC2017_64bit_max 13_1_gcc6.3.0_64bit_none 13_2_x86_64-O0-gcc7.5.0 14_1_gcc6.3.0_64bit_max 14_2_x86_64-O3-gcc7.5.0 15_1_clang5.0.2_64bit_none 15_2_x86_64-O0-Clang10.0.0 16_1_clang5.0.2_64bit_max 16_2_x86_64-O3-Clang10.0.0 17_intel_64_none 18_intel_64bit_max 21_arm-O0-gcc 22_arm-O3-gcc 23_arm-O0-Clang 24_arm-O3-Clang 31_arm64-O0-gcc 32_arm64-O3-gcc 33_arm64-O0-Clang 34_arm64-O3-Clang 41_mips-O0-gcc 42_mips-O3-gcc 43_mips-O0-Clang 44_mips-O3-Clang 51_mips64-O0-gcc 52_mips64-O3-gcc 53_mips64-O0-Clang 54_mips64-O3-Clang 61_powerpc-O0-gcc 62_powerpc-O3-gcc 63_powerpc-O0-Clang 64_powerpc-O3-Clang 71_powerpc64-O0-gcc 72_powerpc64-O3-gcc 73_powerpc64-O0-Clang 74_powerpc64-O3-Clang 80_document \n",
      "01_1_VC2003_32bit_none 19186 764 13 6 0 0 0 0 6 1 6 0 0 5 0 1 1 0 0 1 2 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 3 \n",
      "01_2_VC2017_32bit_none 929 19033 4 8 1 2 2 3 4 0 2 0 0 1 1 2 3 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 \n",
      "02_1_VC2003_32bit_max 54 1 19756 100 0 1 9 4 0 4 14 8 0 34 0 5 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 7 \n",
      "02_2_VC2017_32bit_max 19 18 160 19689 0 3 14 3 2 1 17 4 0 50 0 0 0 1 4 1 1 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 8 \n",
      "03_1_gcc6.3.0_x86_none 0 0 1 1 19482 469 21 7 1 5 4 2 0 1 0 0 1 2 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 \n",
      "03_2_x86-O0-gcc7.5.0 3 1 0 0 664 19291 2 17 4 3 4 0 0 2 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 \n",
      "04_1_gcc6.3.0_x86_O3 2 0 13 11 12 1 18684 1008 2 4 124 107 0 9 0 0 0 0 15 4 0 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "04_2_x86-O3-gcc7.5.0 2 0 5 7 4 14 1326 18454 6 3 78 85 0 4 0 0 0 0 6 4 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "05_1_clang5.0.2_32_none 8 5 0 0 4 1 2 4 18967 976 6 1 0 2 0 0 0 0 0 0 19 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "05_2_x86-O0-Clang10.0.0 2 1 0 1 3 1 2 8 1231 18733 3 3 0 0 0 0 0 0 0 0 3 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "06_1_clang5.0.2_32_O3 2 1 5 44 3 3 114 68 4 1 18944 727 0 24 1 1 0 0 8 2 2 1 32 12 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "06_2_x86-O3-Clang10.0.0 0 0 22 3 1 1 101 85 1 6 791 18959 0 7 1 0 0 0 1 2 0 1 6 7 0 0 1 0 0 0 0 0 0 0 0 1 0 2 0 0 0 0 0 0 0 0 0 0 0 1 0 \n",
      "07_intel_32_none 1 1 1 0 3 6 0 0 1 1 0 2 19982 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "08_intel_32bit_max 4 4 27 63 2 0 16 3 4 4 37 4 2 19816 0 4 0 0 1 2 3 0 1 2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "11_VC2017_64bit_none 1 1 2 2 0 0 0 0 0 0 1 1 0 2 19958 7 1 6 4 2 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 8 \n",
      "12_VC2017_64bit_max 0 1 5 7 1 0 0 0 0 0 4 1 0 1 13 19865 0 5 16 15 1 1 17 4 1 36 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 \n",
      "13_1_gcc6.3.0_64bit_none 2 0 0 1 7 4 1 0 1 0 0 0 0 1 0 0 17844 2107 8 3 6 6 7 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "13_2_x86_64-O0-gcc7.5.0 0 2 0 0 12 12 1 0 3 1 1 0 0 0 0 0 2693 17248 6 8 3 6 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 \n",
      "14_1_gcc6.3.0_64bit_max 0 0 0 2 0 0 32 14 0 0 9 1 0 0 1 2 11 3 18579 1049 0 6 151 127 1 8 0 1 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 \n",
      "14_2_x86_64-O3-gcc7.5.0 0 1 0 2 0 1 15 16 1 1 19 1 0 4 1 5 3 9 1938 17752 3 4 94 117 1 12 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "15_1_clang5.0.2_64bit_none 1 0 0 0 0 0 0 0 38 2 0 0 0 0 0 1 6 2 7 2 18824 1112 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "15_2_x86_64-O0-Clang10.0.0 2 0 0 0 0 0 1 0 8 18 1 1 0 0 0 0 8 10 3 2 1038 18906 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "16_1_clang5.0.2_64bit_max 1 0 0 12 0 1 2 2 1 1 76 6 0 0 6 9 5 2 243 124 10 6 18634 848 0 11 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "16_2_x86_64-O3-Clang10.0.0 0 0 5 1 0 1 6 1 0 0 20 10 0 1 1 4 5 3 204 165 1 0 1043 18523 1 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 \n",
      "17_intel_64_none 1 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 1 1 0 0 0 0 1 0 19993 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 \n",
      "18_intel_64bit_max 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 12 2 3 6 15 0 0 11 3 1 19944 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "21_arm-O0-gcc 0 2 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 19932 23 0 5 2 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 24 \n",
      "22_arm-O3-gcc 0 0 1 1 0 0 1 0 0 0 0 2 0 1 0 2 0 0 1 1 0 0 1 3 0 0 16 19933 4 1 0 1 1 5 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 20 \n",
      "23_arm-O0-Clang 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 2 0 1 1 0 0 3 1 19942 33 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 2 0 0 0 0 6 \n",
      "24_arm-O3-Clang 1 0 0 1 0 0 1 3 0 0 1 0 0 2 0 1 1 0 3 1 1 2 0 1 0 0 4 12 68 19875 2 2 0 1 0 1 2 2 1 0 1 0 0 0 1 4 0 2 0 2 1 \n",
      "31_arm64-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 19991 6 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "32_arm64-O3-gcc 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 11 19915 1 72 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "33_arm64-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 19982 13 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 2 \n",
      "34_arm64-O3-Clang 1 0 0 0 0 0 0 1 0 0 0 2 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 4 107 35 19838 0 2 0 0 0 1 0 1 0 0 0 2 1 2 0 1 1 \n",
      "41_mips-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 19973 7 1 0 12 3 0 2 0 0 0 0 0 0 0 0 1 \n",
      "42_mips-O3-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 5 19910 0 55 0 27 0 0 0 0 0 0 0 0 0 1 1 \n",
      "43_mips-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 3 3 19985 6 0 0 1 1 0 0 0 0 0 0 0 0 0 \n",
      "44_mips-O3-Clang 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 76 4 19841 0 6 0 70 0 0 0 0 0 0 0 1 0 \n",
      "51_mips64-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 26 2 1 0 19966 5 0 0 0 0 0 0 0 0 0 0 0 \n",
      "52_mips64-O3-gcc 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 108 0 20 5 19826 0 37 0 0 0 0 0 0 0 0 0 \n",
      "53_mips64-O0-Clang 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 5 0 6 4 19982 1 0 0 0 0 0 0 0 0 0 \n",
      "54_mips64-O3-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 28 0 140 0 78 3 19744 0 1 0 0 0 0 0 0 1 \n",
      "61_powerpc-O0-gcc 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 19988 7 2 0 2 0 0 0 0 \n",
      "62_powerpc-O3-gcc 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 2 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 5 19918 1 34 0 28 0 7 0 \n",
      "63_powerpc-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 19990 5 0 0 3 0 0 \n",
      "64_powerpc-O3-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 65 9 19872 0 6 0 42 1 \n",
      "71_powerpc64-O0-gcc 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 13 4 0 0 19962 11 0 1 3 \n",
      "72_powerpc64-O3-gcc 0 0 0 0 0 0 0 0 0 0 2 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 3 0 0 0 0 0 3 0 0 0 1 0 0 0 2 1 93 0 7 7 19841 3 30 4 \n",
      "73_powerpc64-O0-Clang 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 37 0 0 5 19952 6 0 \n",
      "74_powerpc64-O3-Clang 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 20 2 168 0 30 6 19771 0 \n",
      "80_document 1 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 14 8 0 0 2 2 0 4 0 1 0 0 0 2 0 2 0 0 0 0 0 2 0 0 19955 \n",
      "no label Num TP FP FN TN R P F1 Acc.\n",
      "0 01_1_VC2003_32bit_none 20000 19186 814 1037 19186 0.9487217524600702 0.9593 0.9539815528429008 0.9539815528429009\n",
      "1 01_2_VC2017_32bit_none 20000 19033 967 806 19033 0.9593729522657392 0.95165 0.9554958708802931 0.9554958708802932\n",
      "2 02_1_VC2003_32bit_max 20000 19756 244 266 19756 0.9867146139246828 0.9878 0.9872570086452452 0.9872570086452451\n",
      "3 02_2_VC2017_32bit_max 20000 19689 311 276 19689 0.986175807663411 0.98445 0.9853121481296134 0.9853121481296134\n",
      "4 03_1_gcc6.3.0_x86_none 20000 19482 518 718 19482 0.9644554455445544 0.9741 0.9692537313432835 0.9692537313432836\n",
      "5 03_2_x86-O0-gcc7.5.0 20000 19291 709 521 19291 0.9737028063799718 0.96455 0.9691047925248669 0.9691047925248669\n",
      "6 04_1_gcc6.3.0_x86_O3 20000 18684 1316 1671 18684 0.9179071481208548 0.9342 0.9259819105439226 0.9259819105439226\n",
      "7 04_2_x86-O3-gcc7.5.0 20000 18454 1546 1251 18454 0.936513575234712 0.9227 0.9295554716030727 0.9295554716030726\n",
      "8 05_1_clang5.0.2_32_none 20000 18967 1033 1321 18967 0.93488761829653 0.94835 0.9415706910246228 0.9415706910246228\n",
      "9 05_2_x86-O0-Clang10.0.0 20000 18733 1267 1033 18733 0.9477385409288678 0.93665 0.9421616456269174 0.9421616456269175\n",
      "10 06_1_clang5.0.2_32_O3 20000 18944 1056 1222 18944 0.9394029554696023 0.9472 0.9432853657322113 0.9432853657322113\n",
      "11 06_2_x86-O3-Clang10.0.0 20000 18959 1041 968 18959 0.9514226928288252 0.94795 0.949683171788514 0.949683171788514\n",
      "12 07_intel_32_none 20000 19982 18 4 19982 0.9997998599019313 0.9991 0.9994498074326015 0.9994498074326014\n",
      "13 08_intel_32bit_max 20000 19816 184 156 19816 0.9921890646905668 0.9908 0.9914940458320824 0.9914940458320824\n",
      "14 11_VC2017_64bit_none 20000 19958 42 27 19958 0.9986489867400551 0.9979 0.9982743528823308 0.9982743528823309\n",
      "15 12_VC2017_64bit_max 20000 19865 135 57 19865 0.9971388414817789 0.99325 0.9951906217123391 0.9951906217123391\n",
      "16 13_1_gcc6.3.0_64bit_none 20000 17844 2156 2741 17844 0.866844789895555 0.8922 0.8793396575089318 0.8793396575089318\n",
      "17 13_2_x86_64-O0-gcc7.5.0 20000 17248 2752 2165 17248 0.8884767939009942 0.8624 0.8752442087636061 0.8752442087636059\n",
      "18 14_1_gcc6.3.0_64bit_max 20000 18579 1421 2479 18579 0.8822775192325957 0.92895 0.9050124214525793 0.9050124214525793\n",
      "19 14_2_x86_64-O3-gcc7.5.0 20000 17752 2248 1409 17752 0.9264652158029331 0.8876 0.9066162763974361 0.9066162763974362\n",
      "20 15_1_clang5.0.2_64bit_none 20000 18824 1176 1097 18824 0.9449324833090709 0.9412 0.9430625485333535 0.9430625485333534\n",
      "21 15_2_x86_64-O0-Clang10.0.0 20000 18906 1094 1161 18906 0.9421438182089998 0.9453 0.9437192702223776 0.9437192702223776\n",
      "22 16_1_clang5.0.2_64bit_max 20000 18634 1366 1389 18634 0.9306297757578784 0.9317 0.9311645803662894 0.9311645803662894\n",
      "23 16_2_x86_64-O3-Clang10.0.0 20000 18523 1477 1138 18523 0.9421189156197548 0.92615 0.9340662111394066 0.9340662111394065\n",
      "24 17_intel_64_none 20000 19993 7 5 19993 0.9997499749974997 0.99965 0.9996999849992501 0.99969998499925\n",
      "25 18_intel_64bit_max 20000 19944 56 74 19944 0.9963033270056949 0.9972 0.996751461842171 0.996751461842171\n",
      "26 21_arm-O0-gcc 20000 19932 68 41 19932 0.9979472287588245 0.9966 0.9972731593825832 0.9972731593825832\n",
      "27 22_arm-O3-gcc 20000 19933 67 51 19933 0.9974479583666933 0.99665 0.9970488195278111 0.9970488195278111\n",
      "28 23_arm-O0-Clang 20000 19942 58 72 19942 0.9964025182372339 0.9971 0.9967511371020142 0.9967511371020142\n",
      "29 24_arm-O3-Clang 20000 19875 125 41 19875 0.997941353685479 0.99375 0.995841266659986 0.995841266659986\n",
      "30 31_arm64-O0-gcc 20000 19991 9 21 19991 0.9989506296222267 0.99955 0.9992502249325202 0.9992502249325202\n",
      "31 32_arm64-O3-gcc 20000 19915 85 120 19915 0.9940104816571 0.99575 0.9948794804546023 0.9948794804546022\n",
      "32 33_arm64-O0-Clang 20000 19982 18 38 19982 0.9981018981018981 0.9991 0.9986006996501748 0.998600699650175\n",
      "33 34_arm64-O3-Clang 20000 19838 162 106 19838 0.9946851183313277 0.9919 0.9932906068495894 0.9932906068495895\n",
      "34 41_mips-O0-gcc 20000 19973 27 35 19973 0.9982506997201119 0.99865 0.9984503099380123 0.9984503099380124\n",
      "35 42_mips-O3-gcc 20000 19910 90 234 19910 0.9883836378077839 0.9955 0.991929055400558 0.991929055400558\n",
      "36 43_mips-O0-Clang 20000 19985 15 14 19985 0.9992999649982499 0.99925 0.9992749818745469 0.9992749818745469\n",
      "37 44_mips-O3-Clang 20000 19841 159 231 19841 0.9884914308489438 0.99205 0.9902675184667598 0.9902675184667599\n",
      "38 51_mips64-O0-gcc 20000 19966 34 26 19966 0.9986994797919168 0.9983 0.998499699939988 0.998499699939988\n",
      "39 52_mips64-O3-gcc 20000 19826 174 126 19826 0.9936848436246993 0.9913 0.9924909891870244 0.9924909891870244\n",
      "40 53_mips64-O0-Clang 20000 19982 18 6 19982 0.9996998198919351 0.9991 0.9993998199459837 0.9993998199459838\n",
      "41 54_mips64-O3-Clang 20000 19744 256 119 19744 0.9940089613854907 0.9872 0.9905927802724331 0.9905927802724331\n",
      "42 61_powerpc-O0-gcc 20000 19988 12 20 19988 0.999000399840064 0.9994 0.9992001599680064 0.9992001599680064\n",
      "43 62_powerpc-O3-gcc 20000 19918 82 192 19918 0.9904525111884634 0.9959 0.9931687858389429 0.9931687858389429\n",
      "44 63_powerpc-O0-Clang 20000 19990 10 52 19990 0.9974054485580282 0.9995 0.9984516257929175 0.9984516257929175\n",
      "45 64_powerpc-O3-Clang 20000 19872 128 224 19872 0.9888535031847133 0.9936 0.9912210694333599 0.99122106943336\n",
      "46 71_powerpc64-O0-gcc 20000 19962 38 10 19962 0.9994992990186261 0.9981 0.9987991594115881 0.9987991594115881\n",
      "47 72_powerpc64-O3-gcc 20000 19841 159 88 19841 0.9955843243514476 0.99205 0.9938140198852964 0.9938140198852964\n",
      "48 73_powerpc64-O0-Clang 20000 19952 48 13 19952 0.9993488605058853 0.9976 0.9984736644563994 0.9984736644563994\n",
      "49 74_powerpc64-O3-Clang 20000 19771 229 98 19771 0.9950676933917157 0.98855 0.9917981389049136 0.9917981389049136\n",
      "50 80_document 20000 19955 45 100 19955 0.9950137122911992 0.99775 0.996379977530895 0.996379977530895\n",
      "  1020000 992930 27070 27070 992930 0.9734607843137255 0.9734607843137255 0.9734607843137255 0.9734607843137255\n"
     ]
    }
   ],
   "source": [
    "kfold_test(args_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (sub): o_glassesX(\n",
      "    (linear): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pe): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (att): Attention(\n",
      "      (l_q): Conv1d(32, 128, kernel_size=(1,), stride=(1,))\n",
      "      (l_k): Conv1d(32, 128, kernel_size=(1,), stride=(1,))\n",
      "      (l_v): Conv1d(32, 128, kernel_size=(1,), stride=(1,))\n",
      "      (l_q2): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
      "      (l_k2): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
      "      (l_v2): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (fc): Linear(in_features=7424, out_features=9, bias=True)\n",
      "  )\n",
      "  (main): o_glassesX(\n",
      "    (linear): Conv1d(41, 32, kernel_size=(1,), stride=(1,))\n",
      "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pe): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (att): Attention(\n",
      "      (l_q): Conv1d(32, 128, kernel_size=(1,), stride=(1,))\n",
      "      (l_k): Conv1d(32, 128, kernel_size=(1,), stride=(1,))\n",
      "      (l_v): Conv1d(32, 128, kernel_size=(1,), stride=(1,))\n",
      "      (l_q2): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
      "      (l_k2): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
      "      (l_v2): Conv1d(128, 32, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "    (fc): Linear(in_features=7424, out_features=51, bias=True)\n",
      "  )\n",
      ")\n",
      "0 torch.Size([32, 32, 1])\n",
      "1 torch.Size([32])\n",
      "2 torch.Size([32])\n",
      "3 torch.Size([32])\n",
      "4 torch.Size([128, 32, 1])\n",
      "5 torch.Size([128])\n",
      "6 torch.Size([128, 32, 1])\n",
      "7 torch.Size([128])\n",
      "8 torch.Size([128, 32, 1])\n",
      "9 torch.Size([128])\n",
      "10 torch.Size([32, 128, 1])\n",
      "11 torch.Size([32])\n",
      "12 torch.Size([32, 128, 1])\n",
      "13 torch.Size([32])\n",
      "14 torch.Size([32, 128, 1])\n",
      "15 torch.Size([32])\n",
      "16 torch.Size([9, 7424])\n",
      "17 torch.Size([9])\n",
      "18 torch.Size([32, 41, 1])\n",
      "19 torch.Size([32])\n",
      "20 torch.Size([32])\n",
      "21 torch.Size([32])\n",
      "22 torch.Size([128, 32, 1])\n",
      "23 torch.Size([128])\n",
      "24 torch.Size([128, 32, 1])\n",
      "25 torch.Size([128])\n",
      "26 torch.Size([128, 32, 1])\n",
      "27 torch.Size([128])\n",
      "28 torch.Size([32, 128, 1])\n",
      "29 torch.Size([32])\n",
      "30 torch.Size([32, 128, 1])\n",
      "31 torch.Size([32])\n",
      "32 torch.Size([32, 128, 1])\n",
      "33 torch.Size([32])\n",
      "34 torch.Size([51, 7424])\n",
      "35 torch.Size([51])\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "for i,p in enumerate(model.parameters()):\n",
    "    print(i,p.data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Yoys6TzULcwK",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if args_output_model:  \n",
    "#     # model = nn.Sequential()\n",
    "#     # model.add_module('l1', Model(op_num, num_of_types))\n",
    "#     model = Model(block_size, num_of_types)\n",
    "#     model.load_state_dict(torch.load(args_output_model + '.pth'))\n",
    "#     device = torch.device('cpu')\n",
    "    \n",
    "#     model.eval()\n",
    "        \n",
    "#     bb = master_dataset_X[0:1]\n",
    "\n",
    "#     print(bb)\n",
    "#     t = master_dataset_Y[0:10]\n",
    "#     print(t)\n",
    "\n",
    "    \n",
    "#     p = model(master_dataset_X[0:10])\n",
    "#     print(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 846
    },
    "id": "z5aXVHDeLTAb",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "87ff4951-3235-42b2-e873-729a41f2f2a7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if F_arch_check:\n",
    "#   from sklearn.manifold import TSNE\n",
    "#   from sklearn.model_selection import ShuffleSplit\n",
    "#   # loader_test = DataLoader(master_dataset, batch_size=args_batch_size, shuffle=False)\n",
    "#   test_size = min(0.5,1000*51/len(master_dataset_X))\n",
    "#   ss = ShuffleSplit(n_splits=1, test_size=test_size, random_state=0)\n",
    "#   for fold_idx, (train_idx, test_idx) in enumerate(ss.split(master_dataset_X, master_dataset_Y)):\n",
    "#     print(fold_idx+1)\n",
    "#     arch_num = args_arch_num\n",
    "\n",
    "#     # ds_train = Subset(master_dataset, train_idx)\n",
    "#     ds_test = Subset(master_dataset, test_idx)\n",
    "#     ds_test_y = master_dataset_Y[test_idx]\n",
    "#     loader_test = DataLoader(ds_test, batch_size=args_batch_size, shuffle=False)\n",
    "#   model.to(device)\n",
    "#   model.eval()\n",
    "#   arch = []\n",
    "#   arch_num = args_arch_num\n",
    "#   with torch.no_grad():\n",
    "#     for data, targets in loader_test:\n",
    "#       if torch.cuda.is_available():\n",
    "#         data, targets = data.to(device), targets.to(device)\n",
    "      \n",
    "#       x = torch.split(data[:,:-arch_num],8,dim=1)\n",
    "#       x = torch.stack(x,dim=2)\n",
    "#       x = torch.cat([x[:,:,0:-3],x[:,:,1:-2],x[:,:,2:-1],x[:,:,3:]],dim=1)\n",
    "\n",
    "\n",
    "#       outputs = model.get_arch(x).to('cpu').detach().numpy().copy()\n",
    "#       arch.append(outputs)\n",
    "#   arch = np.concatenate(arch)\n",
    "\n",
    "#   print(arch.shape)\n",
    "#   targets = ds_test_y.to('cpu').detach().numpy().copy()\n",
    "#   print(targets.shape)\n",
    "#   #Color Settings\n",
    "#   for i in range(len(targets)):\n",
    "#     t = targets[i]\n",
    "#     if t <= 13:\n",
    "#       #x86\n",
    "#       c = 0\n",
    "#     elif t <= 25:\n",
    "#       #x86-64\n",
    "#       c = 1\n",
    "#     elif t <= 29:\n",
    "#       #Arm\n",
    "#       c = 2\n",
    "#     elif t <= 33:\n",
    "#       #Arm64\n",
    "#       c = 3\n",
    "#     elif t <= 37:\n",
    "#       #mips\n",
    "#       c = 4\n",
    "#     elif t <= 41:\n",
    "#       #mips64\n",
    "#       c = 5\n",
    "#     elif t <= 45:\n",
    "#       #powerpc\n",
    "#       c = 6\n",
    "#     elif t <= 49:\n",
    "#       #powerpc64\n",
    "#       c = 7\n",
    "#     else:\n",
    "#       #others\n",
    "#       c = 8\n",
    "#     targets[i] = c\n",
    "#   X_reduced = TSNE(n_components=2, random_state=0).fit_transform(arch)\n",
    "#   print(X_reduced.shape)\n",
    "#   plt.figure(dpi=200)\n",
    "#   plt.scatter(X_reduced[:,0], X_reduced[:,1], s = 0.1, c = targets[:], cmap='jet')\n",
    "#   plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "7sPI9ZH8DrgH",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "2cd70155-24a6-4595-88e2-cb1aa47e8715",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from bokeh.plotting import figure, output_file, show, ColumnDataSource\n",
    "# from bokeh.palettes import Turbo256\n",
    "# from bokeh.transform import linear_cmap\n",
    "# from bokeh.io import output_notebook\n",
    "\n",
    "# # 出力設定\n",
    "# output_notebook()\n",
    "\n",
    "# mapper = linear_cmap(field_name=\"l\", palette=Turbo256, low=min(targets), high=max(targets))\n",
    "# label = [\"x86\",\"x86-64\",\"ARM\",\"ARM64\",\"MIPS\",\"MIPS64\",\"PowerPC\",\"PowerPC64\",\"Others\"]\n",
    "# # 辞書\n",
    "# source = ColumnDataSource(data=dict(\n",
    "#     x=X_reduced[:,0],\n",
    "#     y=X_reduced[:,1],\n",
    "#     c=[label[i] for i in targets],\n",
    "#     l=targets,\n",
    "# ))\n",
    "\n",
    "# # tooltips設定\n",
    "# TOOLTIPS = [\n",
    "#     (\"index\", \"$index\"),\n",
    "#     (\"x\", \"$x\"),\n",
    "#     (\"y\", \"$y\"), \n",
    "#     (\"label\", \"@c\"),\n",
    "# ]\n",
    "\n",
    "\n",
    "# # グラフ全体の設定\n",
    "# p = figure(\n",
    "#             title=\"test\",\n",
    "#             plot_width=800,\n",
    "#             plot_height=600,\n",
    "#             x_axis_label='x', \n",
    "#             y_axis_label='y',\n",
    "#             tooltips=TOOLTIPS\n",
    "#           )\n",
    "\n",
    "\n",
    "# # 描画\n",
    "# p.circle('x', 'y', color=mapper, fill_alpha=0.5, size=5, source=source)\n",
    "\n",
    "# # 出力\n",
    "# output_file(\"iris.html\")\n",
    "# show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "20220226_02_o_glasses_fine_tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
